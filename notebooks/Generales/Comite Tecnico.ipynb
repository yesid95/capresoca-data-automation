{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Exceles = r\"\\\\Servernas\\AYC2\\(Z)RSERVER(Z)\\01_Ing. YESID\\FRANK\\GRUPO1\"\n",
    "R_PDFs = r\"\\\\Servernas\\Server2\\12_Servicios de Salud\\2025\\CTC\\ACTAS  COMITES\"\n",
    "R_Salida = r\"\\\\Servernas\\Server2\\12_Servicios de Salud\\2025\\CTC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Get all XLS files from the directory\n",
    "excel_files = glob.glob(os.path.join(R_Exceles, \"*.XLS\"))\n",
    "\n",
    "print(f\"Total de archivos encontrados: {len(excel_files)}\")\n",
    "\n",
    "data_list = []\n",
    "error_count = 0\n",
    "\n",
    "for file_path in excel_files:\n",
    "    try:\n",
    "        # Leer con xlrd para obtener datos\n",
    "        df_full = pd.read_excel(file_path, sheet_name=0, header=None, engine='xlrd')\n",
    "        \n",
    "        # Buscar la fecha en todo el archivo (puede estar en cualquier lugar)\n",
    "        fecha_reporte = None\n",
    "        for idx, row in df_full.iterrows():\n",
    "            for col_idx, cell_value in enumerate(row):\n",
    "                if isinstance(cell_value, (pd.Timestamp, datetime)):\n",
    "                    fecha_reporte = cell_value\n",
    "                    break\n",
    "            if fecha_reporte:\n",
    "                break\n",
    "        \n",
    "        # Si no encontró como datetime, buscar fechas en formato texto\n",
    "        if not fecha_reporte:\n",
    "            patron_fecha = r'\\d{1,2}/\\d{1,2}/\\d{4}|\\d{4}-\\d{1,2}-\\d{1,2}'\n",
    "            for idx, row in df_full.iterrows():\n",
    "                for cell_value in row:\n",
    "                    if isinstance(cell_value, str) and re.search(patron_fecha, cell_value):\n",
    "                        try:\n",
    "                            fecha_reporte = pd.to_datetime(cell_value)\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                if fecha_reporte:\n",
    "                    break\n",
    "        \n",
    "        # Leer los datos (columnas A a I, header en fila 3)\n",
    "        df_temp = pd.read_excel(file_path, sheet_name=0, header=2, usecols='A:I', engine='xlrd')\n",
    "        \n",
    "        # PASO 1: Eliminar columnas completamente vacías (Unnamed)\n",
    "        df_temp = df_temp.dropna(axis=1, how='all')\n",
    "        \n",
    "        # PASO 2: Eliminar filas completamente vacías\n",
    "        df_temp = df_temp.dropna(axis=0, how='all')\n",
    "        \n",
    "        # PASO 3: Eliminar filas donde TODAS las columnas de datos están vacías\n",
    "        df_temp = df_temp.loc[df_temp.notna().any(axis=1)]\n",
    "        \n",
    "        # PASO 4: Limpiar columnas específicas que suelen tener problemas\n",
    "        if 'CANT.' in df_temp.columns:\n",
    "            df_temp['CANT.'] = pd.to_numeric(df_temp['CANT.'], errors='coerce')\n",
    "            df_temp['CANT.'] = df_temp['CANT.'].fillna(method='ffill')\n",
    "        \n",
    "        if 'ESTADO' in df_temp.columns:\n",
    "            df_temp['ESTADO'] = df_temp['ESTADO'].str.upper().str.strip()\n",
    "            df_temp.loc[~df_temp['ESTADO'].isin(['AUTORIZADO', 'NEGADO']), 'ESTADO'] = np.nan\n",
    "            df_temp['ESTADO'] = df_temp['ESTADO'].fillna(method='ffill')\n",
    "        \n",
    "        # PASO 5: Eliminar filas duplicadas completamente\n",
    "        df_temp = df_temp.drop_duplicates(subset=None, keep='first')\n",
    "        \n",
    "        # PASO 6: Eliminar filas donde ESTADO sea NaN después de limpieza\n",
    "        if 'ESTADO' in df_temp.columns:\n",
    "            df_temp = df_temp[df_temp['ESTADO'].notna()]\n",
    "        \n",
    "        # PASO 7: Llenar ACTA si está vacía (forward fill desde arriba)\n",
    "        if 'ACTA' in df_temp.columns:\n",
    "            df_temp['ACTA'] = df_temp['ACTA'].fillna(method='ffill')\n",
    "        \n",
    "        # PASO 8: Eliminar filas donde ACTA sea NaN o vacía después de llenar\n",
    "        if 'ACTA' in df_temp.columns:\n",
    "            df_temp = df_temp[df_temp['ACTA'].notna()]\n",
    "            df_temp = df_temp[df_temp['ACTA'].astype(str).str.strip() != '']\n",
    "        \n",
    "        # PASO 9: Eliminar filas donde CASO sea NaN o vacía\n",
    "        if 'CASO' in df_temp.columns:\n",
    "            df_temp = df_temp[df_temp['CASO'].notna()]\n",
    "            df_temp = df_temp[df_temp['CASO'].astype(str).str.strip() != '']\n",
    "        \n",
    "        # PASO 10: Agregar columnas de fecha y archivo\n",
    "        df_temp['Fecha-Reporte'] = fecha_reporte\n",
    "        df_temp['Nombre-Archivo'] = os.path.basename(file_path)\n",
    "        \n",
    "        # Solo agregar si tiene registros válidos\n",
    "        if len(df_temp) > 0:\n",
    "            data_list.append(df_temp)\n",
    "            print(f\"✓ Procesado: {os.path.basename(file_path)} | Fecha: {fecha_reporte} | Filas: {len(df_temp)}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Sin datos válidos: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"✗ Error en {os.path.basename(file_path)}: {str(e)[:100]}\")\n",
    "\n",
    "print(f\"\\n--- Resumen ---\")\n",
    "print(f\"Archivos procesados: {len(data_list)}\")\n",
    "print(f\"Archivos con error: {error_count}\")\n",
    "\n",
    "if len(data_list) > 0:\n",
    "    # Concatenar de forma inteligente\n",
    "    df_unificado = pd.concat(data_list, axis=0, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Limpiar columnas Unnamed vacías\n",
    "    unnamed_cols = [col for col in df_unificado.columns if 'Unnamed' in str(col)]\n",
    "    for col in unnamed_cols:\n",
    "        if df_unificado[col].isna().sum() == len(df_unificado):\n",
    "            df_unificado = df_unificado.drop(columns=[col])\n",
    "    \n",
    "    # Eliminar filas completamente vacías\n",
    "    df_unificado = df_unificado.dropna(axis=0, how='all')\n",
    "    \n",
    "    # Validación final: eliminar filas sin ESTADO válido\n",
    "    if 'ESTADO' in df_unificado.columns:\n",
    "        df_unificado = df_unificado[df_unificado['ESTADO'].isin(['AUTORIZADO', 'NEGADO'])]\n",
    "    \n",
    "    # Validación final: eliminar filas sin ACTA válida\n",
    "    if 'ACTA' in df_unificado.columns:\n",
    "        df_unificado = df_unificado[df_unificado['ACTA'].notna()]\n",
    "        df_unificado = df_unificado[df_unificado['ACTA'].astype(str).str.strip() != '']\n",
    "    \n",
    "    # Validación final: eliminar filas sin CASO válido\n",
    "    if 'CASO' in df_unificado.columns:\n",
    "        df_unificado = df_unificado[df_unificado['CASO'].notna()]\n",
    "        df_unificado = df_unificado[df_unificado['CASO'].astype(str).str.strip() != '']\n",
    "    \n",
    "    # Reorganizar columnas (Fecha-Reporte y Nombre-Archivo al final)\n",
    "    cols = [col for col in df_unificado.columns if col not in ['Fecha-Reporte', 'Nombre-Archivo']]\n",
    "    cols.extend(['Fecha-Reporte', 'Nombre-Archivo'])\n",
    "    df_unificado = df_unificado[cols]\n",
    "    \n",
    "    # Guardar\n",
    "    output_path = r\"\\\\Servernas\\Server2\\12_Servicios de Salud\\2025\\CTC\\Reporte_unificado_CTC.xlsx\"\n",
    "    df_unificado.to_excel(output_path, index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"\\n✓ Reporte unificado guardado en: {output_path}\")\n",
    "    print(f\"  Total de filas: {len(df_unificado)}\")\n",
    "    print(f\"  Total de columnas: {len(df_unificado.columns)}\")\n",
    "    print(f\"  Columnas: {list(df_unificado.columns)}\")\n",
    "    print(f\"\\nValidación de ESTADO:\")\n",
    "    print(df_unificado['ESTADO'].value_counts())\n",
    "    print(f\"\\nACTAs únicas: {df_unificado['ACTA'].nunique()}\")\n",
    "    print(f\"CASOs únicos: {df_unificado['CASO'].nunique()}\")\n",
    "else:\n",
    "    print(\"⚠️ No se procesó ningún archivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Filtrar registros con estado \"NEGADO\"\n",
    "df_negados = df_unificado[df_unificado['ESTADO'].str.upper().str.contains('NEGADO', na=False)].copy()\n",
    "\n",
    "print(f\"Total de registros NEGADOS: {len(df_negados)}\")\n",
    "\n",
    "# Obtener ACTAs únicas de los registros negados\n",
    "actas_negadas = df_negados['ACTA'].dropna().unique()\n",
    "print(f\"ACTAs únicas con estado NEGADO: {len(actas_negadas)}\")\n",
    "\n",
    "# Preparar rutas\n",
    "carpeta_origen_pdfs = R_PDFs\n",
    "carpeta_destino_pdfs = r\"\\\\Servernas\\AYC2\\(Z)RSERVER(Z)\\01_Ing. YESID\\FRANK\\PDF Negados\"\n",
    "\n",
    "# Crear carpeta de destino si no existe\n",
    "os.makedirs(carpeta_destino_pdfs, exist_ok=True)\n",
    "\n",
    "# Buscar y copiar PDFs\n",
    "\n",
    "pdfs_copiados = []\n",
    "pdfs_no_encontrados = []\n",
    "\n",
    "for acta in actas_negadas:\n",
    "    # Convertir ACTA a entero para formato sin decimales\n",
    "    try:\n",
    "        acta_num = int(acta)\n",
    "        nombre_pdf = f\"ACTAS COMITE {acta_num}.pdf\"\n",
    "        ruta_origen = os.path.join(carpeta_origen_pdfs, nombre_pdf)\n",
    "        ruta_destino = os.path.join(carpeta_destino_pdfs, nombre_pdf)\n",
    "        \n",
    "        # Verificar si el PDF existe y copiarlo\n",
    "        if os.path.exists(ruta_origen):\n",
    "            shutil.copy2(ruta_origen, ruta_destino)\n",
    "            pdfs_copiados.append(nombre_pdf)\n",
    "            print(f\"✓ Copiado: {nombre_pdf}\")\n",
    "        else:\n",
    "            pdfs_no_encontrados.append(nombre_pdf)\n",
    "            print(f\"✗ No encontrado: {nombre_pdf}\")\n",
    "    except:\n",
    "        print(f\"⚠️ Error procesando ACTA: {acta}\")\n",
    "\n",
    "print(f\"\\n--- Resumen de Copia de PDFs ---\")\n",
    "print(f\"PDFs copiados exitosamente: {len(pdfs_copiados)}\")\n",
    "print(f\"PDFs no encontrados: {len(pdfs_no_encontrados)}\")\n",
    "print(f\"Destino: {carpeta_destino_pdfs}\")\n",
    "\n",
    "# Mostrar algunos casos negados para verificación\n",
    "if len(df_negados) > 0:\n",
    "    print(f\"\\nPrimeros registros NEGADOS:\")\n",
    "    print(df_negados[['ACTA', 'CASO', 'NOMBRES Y APELLIDOS', 'ESTADO', 'Nombre-Archivo']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
