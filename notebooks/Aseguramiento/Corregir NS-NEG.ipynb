{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52061c12",
   "metadata": {},
   "source": [
    "# 1. Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e80f0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ece43",
   "metadata": {},
   "source": [
    "# 2. Rutas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257bdc",
   "metadata": {},
   "source": [
    "1. Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "db2d5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada \n",
    "\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0020052025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0020052025.TXT\"\n",
    "R_NS_NEG = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02516052025.NEG\"\n",
    "R_NS_Anterior = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\16\\NSEPS02516052025.txt\"\n",
    "R_NS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\\SIE_NSEPS02522052025.txt\"\n",
    "R_NS_No_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\\No enviar_Preliminar NS.txt\"\n",
    "R_NS_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\\Enviar otras NS.txt\"\n",
    "\n",
    "# Ruta Salida\n",
    "\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e898105",
   "metadata": {},
   "source": [
    "2. Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9ca22530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada\n",
    "\n",
    "#R_Ms_ADRES_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0020052025.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0020052025.TXT\"\n",
    "#R_NS_NEG = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02516052025.NEG\"\n",
    "#R_NS_Anterior = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\16\\NSEPS02516052025.txt\"\n",
    "\n",
    "# Ruta Salida\n",
    "\n",
    "#R_Salida = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d2401c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"23/05/2025\"\n",
    "F_Envio = \"23052025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d22915",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "249155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID_ADRES\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a5851b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros en Df_S3: 1891\n",
      "N√∫mero de registros en Df_S3: 597\n"
     ]
    }
   ],
   "source": [
    "# Dataframe NS anterior\n",
    "new_columns = [\"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNS_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"]\n",
    "Df_NS_Anterior = pd.read_csv(R_NS_Anterior, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NS_Anterior.columns = new_columns\n",
    "print(\"N√∫mero de registros en Df_S3:\", Df_NS_Anterior.shape[0])\n",
    "\n",
    "# Dataframe NS Glosado\n",
    "new_columns.append(\"Glosa\")\n",
    "Df_NS_NEG = pd.read_csv(R_NS_NEG, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NS_NEG.columns = new_columns\n",
    "print(\"N√∫mero de registros en Df_S3:\", Df_NS_NEG.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1275f",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0167e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0‚Ä¶n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coiNSide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# 1) Tu merge original (trae ENT_ID_ADRES y TPS_EST_AFL_ID_from_adres de ADRES)\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer  = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "\n",
    "# 2) Preparamos un mini-dataframe s√≥lo con las filas N01 que s√≠ s√≠ cruzaron con ADRES\n",
    "mapa_n01 = (\n",
    "    Df_NS_NEG.loc[Df_NS_NEG[\"NOVEDAD\"] == \"N01\", \n",
    "                  [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .copy()  # Asegurarse de trabajar con una copia del DataFrame\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\": \"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\": \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\": \"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\": \"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# 3) Merge ‚Äúsecundario‚Äù contra esas N01 para rellenar vac√≠os\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4) Para los que quedaron con ENT_ID_ADRES NaN, y son evoluciones (p.ej. N02, N03, ‚Ä¶),\n",
    "#    rellenamos con el valor tra√≠do de la fila N01\n",
    "mask = Df_NS_NEG[\"ENT_ID_ADRES\"].isna() & Df_NS_NEG[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    "Df_NS_NEG.loc[mask, \"ENT_ID_ADRES\"]                   = Df_NS_NEG.loc[mask, \"ENT_ID_from_self\"]\n",
    "Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "# 5) Limpiamos las columnas auxiliares\n",
    "Df_NS_NEG.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bf23a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros en Df_NS_NEG: 597\n",
      "N√∫mero de registros en DF_NS_EPSC25: 3\n",
      "N√∫mero de registros en DF_No_Enviar: 0\n",
      "N√∫mero de registros en Df_NS_NEG: 594\n",
      "N√∫mero de registros en Df_NS_Envio: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"N√∫mero de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "\n",
    "# 1. Definimos la m√°scara con la l√≥gica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_NS_NEG[\"ENT_ID_ADRES\"] == \"EPSC25\")\n",
    ")\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_NS_EPSC25 = Df_NS_NEG.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_NS_NEG = Df_NS_NEG.loc[~mask].copy()\n",
    "\n",
    "# 4. Definimos la m√°scara con la l√≥gica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    Df_NS_NEG[\"ENT_ID_ADRES\"].isna()                 # NaN\n",
    "    | (Df_NS_NEG[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")  # \"\" o \"   \"\n",
    ")\n",
    "\n",
    "# Asignar un DataFrame vac√≠o con las mismas columnas que el original\n",
    "DF_059_169 = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "Df_NS_Envio = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "DF_No_Enviar = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "\n",
    "print(\"N√∫mero de registros en DF_NS_EPSC25:\", DF_NS_EPSC25.shape[0])\n",
    "print(\"N√∫mero de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"N√∫mero de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "print(\"N√∫mero de registros en Df_NS_Envio:\", Df_NS_Envio.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0d474368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros √∫nicos en Code_Glosa: 19\n",
      "Code_Glosa\n",
      "GN0079    500\n",
      "GN0030     19\n",
      "GN0032     17\n",
      "GN0059     12\n",
      "GN0034     11\n",
      "GN0258      8\n",
      "GN0031      4\n",
      "GN0404      3\n",
      "GN0084      3\n",
      "GN0390      3\n",
      "GN0018      3\n",
      "GN0169      2\n",
      "GN0009      2\n",
      "GN0035      2\n",
      "GN0014      1\n",
      "GN0036      1\n",
      "GN0049      1\n",
      "GN0130      1\n",
      "GN0501      1\n",
      "Name: count, dtype: int64\n",
      "Valores √∫nicos en No_Glosa: [2 3 5 4]\n"
     ]
    }
   ],
   "source": [
    "# Crear la columna \"Code_Glosa\" con los primeros seis caracteres de la columna \"Glosa\"\n",
    "Df_NS_NEG[\"Code_Glosa\"] = Df_NS_NEG[\"Glosa\"].str[:6]\n",
    "\n",
    "# Crear la columna \"No_Glosa\" contando el car√°cter ';' en \"Glosa\" y sumando 1 si no est√° vac√≠a\n",
    "Df_NS_NEG[\"No_Glosa\"] = Df_NS_NEG[\"Glosa\"].apply(lambda x: x.count(';') + 1 if pd.notnull(x) and x != \"\" else 0)\n",
    "\n",
    "# Imprimir el n√∫mero de registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(\"Registros √∫nicos en Code_Glosa:\", Df_NS_NEG[\"Code_Glosa\"].nunique())\n",
    "# Imprimir los registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())\n",
    "\n",
    "# Imprimir los valores √∫nicos de la columna \"No_Glosa\"\n",
    "print(\"Valores √∫nicos en No_Glosa:\", Df_NS_NEG[\"No_Glosa\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f59425d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de columnas antes: 25\n",
      "N√∫mero de columnas despu√©s: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"N√∫mero de columnas antes:\", Df_NS_NEG.shape[1])\n",
    "Df_NS_NEG[\"Glosa_2\"] = Df_NS_NEG[\"Glosa\"]\n",
    "print(\"N√∫mero de columnas despu√©s:\", Df_NS_NEG.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bfb05",
   "metadata": {},
   "source": [
    "# 5. Limpiar Glosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c7682",
   "metadata": {},
   "source": [
    "## üö© GN0079 \n",
    "\n",
    "1. Afiliado Cotizante, Cabeza de familia¬† o Adicional, diligeNSia la condici√≥n de estudiante o discapacitado.\n",
    "   1. Se suma un dia a la fecha de la glosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3226460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros iniciales en Df_NS_NEG: 594\n",
      "Registros iniciales en Df_NS_Envio: 0\n",
      "Registros en Df_NS_NEG despu√©s de limpieza: 94\n",
      "Registros en Df_NS_Envio: 500\n"
     ]
    }
   ],
   "source": [
    "def clean_gn0079(df, df_envio):\n",
    "    # Imprimir conteo inicial\n",
    "    initial_count = len(df)\n",
    "    print(f\"Registros iniciales en Df_NS_NEG: {initial_count}\")\n",
    "    print(f\"Registros iniciales en Df_NS_Envio: {len(df_envio)}\")\n",
    "    \n",
    "    # M√°scara para registros con GN0079 en Glosa_2\n",
    "    mask = df['Glosa_2'].str.contains('GN0079', na=False)\n",
    "    \n",
    "    # FuNSi√≥n para extraer fecha, sumar un d√≠a y formatear\n",
    "    def extract_and_iNSrement(glosa):\n",
    "        import re\n",
    "        match = re.search(r'GN0079\\([^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);', glosa)\n",
    "        if match:\n",
    "            fecha = datetime.strptime(match.group(1), '%d/%m/%Y')\n",
    "            nueva_fecha = fecha + timedelta(days=1)\n",
    "            return nueva_fecha.strftime('%d/%m/%Y')\n",
    "        return None\n",
    "\n",
    "    # Actualizar FECHA_NOVEDAD para los registros con GN0079\n",
    "    df.loc[mask, 'FECHA_NOVEDAD'] = df.loc[mask, 'Glosa_2'].apply(extract_and_iNSrement)\n",
    "\n",
    "    # Eliminar la glosa GN0079(...) de Glosa_2\n",
    "    df['Glosa_2'] = df['Glosa_2'].str.replace(r'GN0079\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # Recalcular No_Glosa contando ';'\n",
    "    df['No_Glosa'] = df['Glosa_2'].str.count(';')\n",
    "\n",
    "    # Actualizar Code_Glosa con los primeros 6 caracteres o '0' si est√° vac√≠o\n",
    "    df['Code_Glosa'] = df['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # Separar registros con No_Glosa == 0 a un nuevo DataFrame\n",
    "    mask_zero = df['No_Glosa'] == 0\n",
    "    df_envio = df[mask_zero].copy()\n",
    "    df_envio = df_envio.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'])\n",
    "\n",
    "    # Mantener solo en Df_NS_NEG los registros restantes\n",
    "    df_neg_clean = df[~mask_zero].copy()\n",
    "\n",
    "    # Imprimir conteos finales\n",
    "    print(f\"Registros en Df_NS_NEG despu√©s de limpieza: {len(df_neg_clean)}\")\n",
    "    print(f\"Registros en Df_NS_Envio: {len(df_envio)}\")\n",
    "\n",
    "    return df_neg_clean, df_envio\n",
    "\n",
    "# Aplicar la fuNSi√≥n al DataFrame existente\n",
    "Df_NS_NEG, Df_NS_Envio = clean_gn0079(Df_NS_NEG, Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a365be",
   "metadata": {},
   "source": [
    "## üö© GN0032\n",
    "\n",
    "1. Afiliado existe en BDUA en estado Fallecido.\n",
    "   1. GN0032(Regimen|Entidad|FechaInicioAfil|Dpto|Mpio|TipoAfiliado|EstadoBDUA|FechaInicioCond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4583f581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG tiene 94 registros\n",
      "Antes: DF_No_Enviar tiene 0 registros\n",
      "Despu√©s: Df_NS_NEG tiene 77 registros\n",
      "Despu√©s: DF_No_Enviar tiene 17 registros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def move_gn0032(Df_NS_NEG, DF_No_Enviar):\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # M√°scara para glosa GN0032\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0032', na=False)\n",
    "    \n",
    "    # Extraer los registros a mover\n",
    "    to_move = Df_NS_NEG[mask].copy()\n",
    "    # Asignar motivo\n",
    "    to_move['Motivo'] = \"Fallecido en actualmente ADRES\"\n",
    "    # Eliminar columnas auxiliares\n",
    "    to_move = to_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    \n",
    "    # Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "    \n",
    "    # Quedarse en Df_NS_NEG con los que NO tienen GN0032\n",
    "    Df_NS_NEG = Df_NS_NEG[~mask].copy()\n",
    "    \n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG, DF_No_Enviar\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = move_gn0032(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110229e",
   "metadata": {},
   "source": [
    "## üö© GN0030    \n",
    "\n",
    "1.  Afiliado no pertenece a la entidad / R√©gimen.\n",
    "2.  GN0030(Regimen|EntidadAfil|FechaInicioAfil|Dpto|Mpio|TipoAfil|EstadoAfil|FechaInicioCond);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f89220ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 77 registros\n",
      "Antes: DF_NS_EPSC25 = 3 registros\n",
      "Antes: DF_No_Enviar = 17 registros\n",
      "Despu√©s: Df_NS_NEG = 58 registros\n",
      "Despu√©s: DF_NS_EPSC25 = 11 registros\n",
      "Despu√©s: DF_No_Enviar = 28 registros\n"
     ]
    }
   ],
   "source": [
    "def process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0030 (‚ÄúAfiliado no pertenece a la entidad / R√©gimen‚Äù) en un DataFrame.\n",
    "\n",
    "    Para cada registro en `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0030':\n",
    "      1. Extrae la fecha de condici√≥n (`FechaInicioCond`) de la glosa.\n",
    "      2. Convierte ambas fechas (`FECHA_NOVEDAD` y `FechaInicioCond`) a datetime.\n",
    "      3. Si `cod_ent == 'EPSC25'` y `FechaInicioCond >= FECHA_NOVEDAD`:\n",
    "         - Actualiza `FECHA_NOVEDAD` al d√≠a siguiente de `FechaInicioCond`.\n",
    "         - Mueve el registro a `DF_NS_EPSC25`.\n",
    "      4. En caso contrario:\n",
    "         - Mueve el registro a `DF_No_Enviar`.\n",
    "         - Asigna motivo \"Afiliado en proceso de traslado a otra EPS\".\n",
    "      5. En ambos DataFrames de destino elimina las columnas de validaci√≥n:\n",
    "         ['Glosa_2', 'No_Glosa', 'Code_Glosa'].\n",
    "      6. Imprime conteos de registros en los DataFrames antes y despu√©s de la operaci√≥n.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        DF_NS_EPSC25 (pd.DataFrame): DataFrame destino para registros EPSC25.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros originales sin GN0030.\n",
    "            - DF_NS_EPSC25_updated (pd.DataFrame): Registros EPSC25 procesados.\n",
    "            - DF_No_Enviar_updated (pd.DataFrame): Registros restantes con motivo de traslado.\n",
    "    \"\"\"\n",
    "    # Imprimir conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_NS_EPSC25 = {len(DF_NS_EPSC25)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # M√°scara para registros con GN0030\n",
    "    mask_gn0030 = Df_NS_NEG['Glosa_2'].str.contains('GN0030', na=False)\n",
    "    df_gn0030 = Df_NS_NEG[mask_gn0030].copy()\n",
    "\n",
    "    # 1) Extraer cod_ent (grupo 2)\n",
    "    df_gn0030['cod_ent'] = df_gn0030['Glosa_2'].str.extract(r'GN0030\\([^|]*\\|([^|]*)\\|')\n",
    "\n",
    "    # 2) Extraer FechaInicioCond (grupo √∫ltimo)\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "        \n",
    "    # Extraer FechaInicioCond de la glosa GN0030\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "    # Parsear fechas\n",
    "    df_gn0030['FechaInicioCond_dt'] = pd.to_datetime(df_gn0030['FechaInicioCond'], dayfirst=True, format='%d/%m/%Y')\n",
    "    df_gn0030['FECHA_NOVEDAD_dt'] = pd.to_datetime(df_gn0030['FECHA_NOVEDAD'], dayfirst=True, format='%d/%m/%Y')\n",
    "    \n",
    "    # Caso 1: EPSC25 y FechaInicioCond >= FECHA_NOVEDAD\n",
    "    mask1 = (df_gn0030['cod_ent'] == 'EPSC25') # & (df_gn0030['FechaInicioCond_dt'] >= df_gn0030['FECHA_NOVEDAD_dt'])\n",
    "    df_case1 = df_gn0030[mask1].copy()\n",
    "    # Actualizar FECHA_NOVEDAD a FechaInicioCond + 1 d√≠a\n",
    "    df_case1['FECHA_NOVEDAD'] = (df_case1['FechaInicioCond_dt'] + timedelta(days=1)).dt.strftime('%d/%m/%Y')\n",
    "    # Preparar para mover\n",
    "    df_case1 = df_case1.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Caso 2: resto de GN0030\n",
    "    df_case2 = df_gn0030[~mask1].copy()\n",
    "    df_case2['Motivo'] = \"Afiliado en proceso de traslado a otra EPS\"\n",
    "    df_case2 = df_case2.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Mover registros\n",
    "    DF_NS_EPSC25_updated = pd.concat([DF_NS_EPSC25, df_case1], ignore_index=True)\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_case2], ignore_index=True)\n",
    "    \n",
    "    # Quedar en Df_NS_NEG solo los que no ten√≠an GN0030\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask_gn0030].copy()\n",
    "    \n",
    "    # Imprimir conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_NS_EPSC25 = {len(DF_NS_EPSC25_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG_updated, DF_NS_EPSC25_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Uso de ejemplo:\n",
    "Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar = process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501a9a9",
   "metadata": {},
   "source": [
    "## üö© GN0059\n",
    "1. Afiliado con datos certificados RNEC, no se puede aplicar esta novedad.\n",
    "2. GN0059;\n",
    "3. Los datos b√°sicos del afiliado reportados en el archivo no corresponden con los certificados por la Registraduria Nacional del Estado Civil, se hace necesario revisar el documento del usuario para corroborar la informaci√≥n en caso de coincidir notificar inconsistencia ante ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "00fd5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 58 registros\n",
      "Antes: Df_NS_Envio = 500 registros\n",
      "Antes: DF_059_169 = 0 registros\n",
      "Despu√©s: Df_NS_NEG = 55 registros\n",
      "Despu√©s: Df_NS_Envio = 503 registros\n",
      "Despu√©s: DF_059_169 = 12 registros\n"
     ]
    }
   ],
   "source": [
    "def process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0059 (‚ÄúAfiliado con datos certificados RNEC, no se puede aplicar esta novedad‚Äù).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0059;'.\n",
    "      2. Copia todos esos registros a `DF_059_169` (para verificaci√≥n manual en auditor√≠a).\n",
    "      3. Elimina la subcadena 'GN0059;' de `Glosa_2`.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vac√≠o.\n",
    "      5. De los registros filtrados, mueve a `Df_NS_Envio` aquellos que pasaron a `No_Glosa == 0`.\n",
    "      6. Elimina las columnas auxiliares ['Glosa_2','No_Glosa','Code_Glosa'] \n",
    "         en los DataFrames `Df_NS_Envio` y `DF_059_169`.\n",
    "      7. Imprime conteos de registros antes y despu√©s en cada DataFrame.\n",
    "\n",
    "    Nota:\n",
    "      - La validaci√≥n en Registradur√≠a o Migraci√≥n (web scraping con captcha) \n",
    "        queda comentada para futura implementaci√≥n.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "        DF_059_169 (pd.DataFrame): DataFrame destino para auditor√≠a GN0059.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated)\n",
    "    \"\"\"\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0059;\n",
    "    mask_gn = Df_NS_NEG['Glosa_2'].str.contains('GN0059;', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn].copy()\n",
    "\n",
    "    # 2) Copiar todos a DF_059_169 para verificaci√≥n manual\n",
    "    df_059 = df_gn.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    DF_059_169_updated = pd.concat([DF_059_169, df_059], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0059;' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace('GN0059;', '', regex=False)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que pasaron a No_Glosa == 0 y ten√≠an GN0059\n",
    "    mask_move = mask_gn & (Df_NS_NEG_updated['No_Glosa'] == 0)\n",
    "    df_move = Df_NS_NEG_updated[mask_move].copy()\n",
    "    df_move = df_move.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # 6) Quedarse en Df_NS_NEG los registros restantes\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_move].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_059_169 = {len(DF_059_169_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650a7d4",
   "metadata": {},
   "source": [
    "## üö© GN0169\n",
    "1. Los datos del afiliado enviados no coinciden con los datos certificados por la RNEC.\n",
    "2. GN0169(|SEGUNDO APELLIDO|URDANETA|USDANETA||SEGUNDO NOMBRE|MANUEL||);\n",
    "   1. GN0169(|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad||SEGUNDO NOMBRE|DatoTablaReferencia|DatoEnviadoEntidad|);\n",
    "3. GN0169(PRIMER APELLIDO|GONZALEZ|AMAYA|SEGUNDO APELLIDO|PUSHAINA|GUERRA|||);\n",
    "   1. GN0169(PRIMER APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|||);\n",
    "4. GN0169(||||||);\n",
    "   1. GN0169(DocumentoNoExisteTablaReferencia); \n",
    "5. GN0169(||||FECHA NACIMIENTO|19/07/2016|01/07/2016);\n",
    "6. GN0169(||||SEXO|F|M);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f2b65839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 55\n",
      "Antes: Df_NS_Envio = 503\n",
      "Antes: DF_059_169 = 12\n",
      "Despu√©s: Df_NS_NEG = 44\n",
      "Despu√©s: Df_NS_Envio = 514\n",
      "Despu√©s: DF_059_169 = 14\n"
     ]
    }
   ],
   "source": [
    "def process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0169 (‚ÄúDatos del afiliado no coinciden con RNEC‚Äù) en un DataFrame.\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0169('.\n",
    "      2. Copia a `DF_059_169` s√≥lo aquellos registros **nuevos**, usando como clave √∫nica\n",
    "         ['TPS_IDN_ID','HST_IDN_NUMERO_IDENTIFICACION','NOVEDAD'] para evitar duplicados.\n",
    "      3. Recalcula en todo `Df_NS_NEG`:\n",
    "         - `No_Glosa` = conteo de ‚Äò;‚Äô en `Glosa_2`\n",
    "         - `Code_Glosa` = primeros 6 caracteres de `Glosa_2` o '0' si est√° vac√≠o\n",
    "      4. Mueve a `Df_NS_Envio` todos los registros de `Df_NS_NEG` con `No_Glosa == 0`\n",
    "         (ya no tienen glosas) y elim√≠nales las columnas de validaci√≥n\n",
    "         ['Glosa_2','No_Glosa','Code_Glosa'].\n",
    "      5. Imprime los conteos de registros **antes** y **despu√©s** en los tres DataFrames.\n",
    "\n",
    "    Nota de futuro:\n",
    "      ‚Ä¢ Aqu√≠ deber√≠a implementarse la validaci√≥n t√©cnica de cada bloque GN0169\n",
    "        (comprobaci√≥n contra tablas de referencia o RNEC) para decidir si\n",
    "        corregir, rechazar o no reportar la novedad. Queda marcado para pr√≥xima versi√≥n.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o sin glosas.\n",
    "        DF_059_169 (pd.DataFrame): DataFrame destino para auditor√≠a GN0059/GN0169.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros sin mover (glosas a√∫n pendientes).\n",
    "            - Df_NS_Envio_updated (pd.DataFrame): Registros ya sin glosas listos para env√≠o.\n",
    "            - DF_059_169_updated (pd.DataFrame): Auditor√≠a acumulada de GN0059/GN0169.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)}\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0169\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0169\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Copiar s√≥lo nuevos a DF_059_169 para auditor√≠a\n",
    "    key_cols = ['TPS_IDN_ID','HST_IDN_NUMERO_IDENTIFICACION','NOVEDAD']\n",
    "    # Determinar cu√°les ya existen en DF_059_169\n",
    "    existing_keys = DF_059_169[key_cols].drop_duplicates()\n",
    "    df_gn_keys = df_gn[key_cols]\n",
    "    new_mask = ~df_gn_keys.apply(tuple,1).isin(existing_keys.apply(tuple,1))\n",
    "    df_new = df_gn[new_mask].copy()\n",
    "    DF_059_169_updated = pd.concat([DF_059_169, df_new], ignore_index=True)\n",
    "\n",
    "    # 3) Recalcular No_Glosa y Code_Glosa en todo Df_NS_NEG\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    # Eliminar la(s) glosa(s) GN0169(...) de Glosa_2\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'] \\\n",
    "        .str.replace(r'GN0169\\([^)]*\\);', '', regex=True)\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = (\n",
    "        Df_NS_NEG_updated['Glosa_2'].str[:6]\n",
    "        .replace('', '0')\n",
    "    )\n",
    "\n",
    "    # 4) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = Df_NS_NEG_updated['No_Glosa'] == 0\n",
    "    df_move = Df_NS_NEG_updated[mask_zero].copy()\n",
    "    df_move = df_move.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # Quedarse en Df_NS_NEG s√≥lo los que a√∫n tienen glosas\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_zero].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Despu√©s: DF_059_169 = {len(DF_059_169_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad7593",
   "metadata": {},
   "source": [
    "# 6. Procesar Envio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bc688",
   "metadata": {},
   "source": [
    "## 6.1. Cargue NS SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ea916557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de registros en Df_NS_Envio: 514\n",
      "N√∫mero total de registros en Df_NS_Envio: 956\n"
     ]
    }
   ],
   "source": [
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_SIE\n",
    "new_data = pd.read_csv(R_NS_SIE, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"], errors='ignore')\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Marcar los registros existentes con \"Glosas\"\n",
    "Df_NS_Envio[\"Where\"] = \"Glosas\"\n",
    "\n",
    "# Marcar los nuevos registros con \"SIE\"\n",
    "new_data[\"Where\"] = \"SIE\"\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10cb3a",
   "metadata": {},
   "source": [
    "## 6.2. Cargue novedades planos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "716c606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de registros en Df_NS_Envio: 956\n",
      "N√∫mero total de registros en Df_NS_Envio: 26971\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_Enviar\n",
    "new_data = pd.read_csv(R_NS_Enviar, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio, excepto las columnas faltantes\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"], errors='ignore')\n",
    "\n",
    "# La √∫ltima columna del archivo cargado corresponde a la columna \"Where\"\n",
    "new_data[\"Where\"] = new_data.iloc[:, -1]\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae014e4",
   "metadata": {},
   "source": [
    "## 6.3 Validar ADRES EPS y Regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "06da815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT_ID_ADRES\n",
      "None      26457\n",
      "EPS025      514\n",
      "Name: count, dtype: int64\n",
      "ENT_ID_ADRES\n",
      "EPS025    26881\n",
      "EPSC25       52\n",
      "NaN          38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 0) Ver conteos previos (opcional)\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))\n",
    "\n",
    "# 1) Merge global para traer datos desde ADRES:\n",
    "df_adres_mini = (\n",
    "    DF_ADRES[[\n",
    "        \"TPS_IDN_ID\", \n",
    "        \"HST_IDN_NUMERO_IDENTIFICACION\", \n",
    "        \"ENT_ID_ADRES\", \n",
    "        \"TPS_EST_AFL_ID\"\n",
    "    ]]\n",
    "    .rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    df_adres_mini,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\",\"_new\")\n",
    ")\n",
    "\n",
    "# 2) Rellenar solo donde faltaba:\n",
    "Df_NS_Envio[\"ENT_ID_ADRES\"] = Df_NS_Envio[\"ENT_ID_ADRES\"].fillna(Df_NS_Envio[\"ENT_ID_ADRES_new\"])\n",
    "Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"].fillna(\n",
    "    Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres_new\"]\n",
    ")\n",
    "\n",
    "# 3) Eliminar columnas auxiliares del merge\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_ADRES_new\",\"TPS_EST_AFL_ID_from_adres_new\"], inplace=True)\n",
    "\n",
    "# 4) Continuar con tu l√≥gica de evoluciones N01‚Ä¶\n",
    "mapa_n01 = (\n",
    "    Df_NS_Envio.loc[Df_NS_Envio[\"NOVEDAD\"] == \"N01\", \n",
    "                    [\"COD_1_NOVEDAD\",\"COD_2_NOVEDAD\",\"ENT_ID_ADRES\",\"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\":\"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\":\"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\":\"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\":\"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "mask_evol = (\n",
    "    Df_NS_Envio[\"ENT_ID_ADRES\"].isna() &\n",
    "    Df_NS_Envio[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    ")\n",
    "Df_NS_Envio.loc[mask_evol, \"ENT_ID_ADRES\"] = Df_NS_Envio.loc[mask_evol, \"ENT_ID_from_self\"]\n",
    "Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n",
    "\n",
    "# 5) Verificar conteos finales\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed89d7",
   "metadata": {},
   "source": [
    "### 6.3.1 Mover novedades de contributivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "99d4098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 26971\n",
      "N√∫mero de registros en DF_NS_EPSC25: 11\n",
      "Despu√©s del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 26919\n",
      "N√∫mero de registros en DF_NS_EPSC25: 63\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")\n",
    "\n",
    "# Asegurarse de que DF_NS_EPSC25 tenga la columna 'Where'\n",
    "if 'Where' not in DF_NS_EPSC25.columns:\n",
    "    DF_NS_EPSC25['Where'] = None\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES sea igual a EPSC25\n",
    "mask = Df_NS_Envio[\"ENT_ID_ADRES\"] == \"EPSC25\"\n",
    "to_move = Df_NS_Envio[mask].copy()\n",
    "\n",
    "# Mover los registros a DF_NS_EPSC25\n",
    "DF_NS_EPSC25 = pd.concat([DF_NS_EPSC25, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio[~mask].copy()\n",
    "\n",
    "# Imprimir el n√∫mero de registros despu√©s del proceso\n",
    "print(f\"Despu√©s del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478ba24",
   "metadata": {},
   "source": [
    "### 6.3.2 Mover a no enviar las que no tienen EPS capresoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9e12207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 26919\n",
      "N√∫mero de registros en DF_No_Enviar: 28\n",
      "Despu√©s del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 26881\n",
      "N√∫mero de registros en DF_No_Enviar: 66\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES es nulo o vac√≠o\n",
    "mask_empty = Df_NS_Envio[\"ENT_ID_ADRES\"].isna() | (Df_NS_Envio[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")\n",
    "to_move = Df_NS_Envio.loc[mask_empty].copy()\n",
    "\n",
    "# Asignar el motivo a los registros movidos\n",
    "to_move[\"Motivo\"] = \"No existen MS Adres actualmente\"\n",
    "\n",
    "# Concatenar los registros movidos al DataFrame DF_No_Enviar\n",
    "DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio.loc[~mask_empty].copy()\n",
    "\n",
    "# Imprimir el n√∫mero de registros despu√©s del proceso\n",
    "print(f\"Despu√©s del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "46826b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
       "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
       "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
       "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
       "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
       "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
       "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Motivo', 'cod_ent',\n",
       "       'Where'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_No_Enviar[\"Where\"] = \"Glosas\"\n",
    "DF_No_Enviar.columns\n",
    "#print(DF_No_Enviar['Where'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "de5c8649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_SOLICITUD_NOVEDAD</th>\n",
       "      <th>ENT_ID</th>\n",
       "      <th>TPS_IDN_ID</th>\n",
       "      <th>HST_IDN_NUMERO_IDENTIFICACION</th>\n",
       "      <th>AFL_PRIMER_APELLIDO</th>\n",
       "      <th>AFL_SEGUNDO_APELLIDO</th>\n",
       "      <th>AFL_PRIMER_NOMBRE</th>\n",
       "      <th>AFL_SEGUNDO_NOMBRE</th>\n",
       "      <th>AFL_FECHA_NACIMIENTO</th>\n",
       "      <th>DPR_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>COD_4_NOVEDAD</th>\n",
       "      <th>COD_5_NOVEDAD</th>\n",
       "      <th>COD_6_NOVEDAD</th>\n",
       "      <th>COD_7_NOVEDAD</th>\n",
       "      <th>Glosa</th>\n",
       "      <th>ENT_ID_ADRES</th>\n",
       "      <th>TPS_EST_AFL_ID_from_adres</th>\n",
       "      <th>Motivo</th>\n",
       "      <th>cod_ent</th>\n",
       "      <th>Where</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>144</td>\n",
       "      <td>EPS025</td>\n",
       "      <td>CC</td>\n",
       "      <td>1006414900</td>\n",
       "      <td>DUARTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARILENY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/12/1996</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GN0030(C|EPS010|05/04/2025|05|001|B|AC|05/04/2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afiliado en proceso de traslado a otra EPS</td>\n",
       "      <td>EPS010</td>\n",
       "      <td>Glosas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>923</td>\n",
       "      <td>EPS025</td>\n",
       "      <td>CC</td>\n",
       "      <td>1006414900</td>\n",
       "      <td>DUARTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARILENY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/12/1996</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GN0030(C|EPS010|05/04/2025|05|001|B|AC|05/04/2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afiliado en proceso de traslado a otra EPS</td>\n",
       "      <td>EPS010</td>\n",
       "      <td>Glosas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
       "17                   144  EPS025         CC                    1006414900   \n",
       "24                   923  EPS025         CC                    1006414900   \n",
       "\n",
       "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
       "17              DUARTE                  NaN          MARILENY   \n",
       "24              DUARTE                  NaN          MARILENY   \n",
       "\n",
       "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_4_NOVEDAD  \\\n",
       "17                NaN           05/12/1996     85  ...           NaN   \n",
       "24                NaN           05/12/1996     85  ...           NaN   \n",
       "\n",
       "   COD_5_NOVEDAD COD_6_NOVEDAD COD_7_NOVEDAD  \\\n",
       "17           NaN           NaN           NaN   \n",
       "24           NaN           NaN           NaN   \n",
       "\n",
       "                                                Glosa ENT_ID_ADRES  \\\n",
       "17  GN0030(C|EPS010|05/04/2025|05|001|B|AC|05/04/2...          NaN   \n",
       "24  GN0030(C|EPS010|05/04/2025|05|001|B|AC|05/04/2...          NaN   \n",
       "\n",
       "   TPS_EST_AFL_ID_from_adres                                      Motivo  \\\n",
       "17                       NaN  Afiliado en proceso de traslado a otra EPS   \n",
       "24                       NaN  Afiliado en proceso de traslado a otra EPS   \n",
       "\n",
       "   cod_ent   Where  \n",
       "17  EPS010  Glosas  \n",
       "24  EPS010  Glosas  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_NS_EPSC25\n",
    "DF_No_Enviar\n",
    "Df_NS_Envio\n",
    "\n",
    "# Filtrar el registro donde \"HST_IDN_NUMERO_IDENTIFICACION\" es igual a \"1006414900\"\n",
    "DF_No_Enviar[DF_No_Enviar[\"HST_IDN_NUMERO_IDENTIFICACION\"] == \"1006414900\"]\n",
    "#Df_NS_NEG[Df_NS_NEG['No_Glosa'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5df940ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code_Glosa\n",
      "GN0034    11\n",
      "GN0258     8\n",
      "GN0031     4\n",
      "GN0018     3\n",
      "GN0084     3\n",
      "GN0404     3\n",
      "GN0390     3\n",
      "GN0009     2\n",
      "GN0035     2\n",
      "GN0049     1\n",
      "GN0036     1\n",
      "GN0014     1\n",
      "GN0130     1\n",
      "GN0501     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bb895175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N01' 'N02' 'N03' 'N04' 'N14' 'N21' 'N25' 'N32' 'N36' 'N39' 'N43' 'N46']\n",
      "No_Glosa\n",
      "1    33\n",
      "2    10\n",
      "4     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los valores √∫nicos\n",
    "print(Df_NS_NEG['NOVEDAD'].unique())\n",
    "print(Df_NS_NEG['No_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "28f312e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glosa_2\n",
      "GN0034(ROJAS);GN0035(CATA√ëO);                                                                           4\n",
      "GN0390;                                                                                                 3\n",
      "GN0084(cnd_afl|01/04/2025);                                                                             2\n",
      "GN0404(85001);                                                                                          2\n",
      "GN0258(2|A04|A03);                                                                                      2\n",
      "GN0031(|||||||||);                                                                                      2\n",
      "GN0031(RC|1117327379|CAVIEDES|YABIMAY|NOAH|JESUS|S|EPS025|85|230);                                      1\n",
      "GN0014(CC|1119887423|BOHORQUEZ|GAHONA|CINDY||14/09/1988|C|EPS037|01/08/2008|50|226|B|RE|01/02/2019);    1\n",
      "GN0049(01/02/2005);                                                                                     1\n",
      "GN0036(YEDIXS√íN);                                                                                       1\n",
      "GN0018(CC|1118121026|CANCELADO RNEC RNEC);                                                              1\n",
      "GN0035(CATA√ëO);                                                                                         1\n",
      "GN0034(CAVIEDES);GN0035(YABIMAY);                                                                       1\n",
      "GN0035(MONTA√ëEZ);                                                                                       1\n",
      "GN0034(PINTO);GN0035(LOMBANA);                                                                          1\n",
      "GN0034(NU√ëEZ);                                                                                          1\n",
      "GN0034(MENA);GN0035(CASTELBLANCO);                                                                      1\n",
      "GN0018(CC|1118542994|CANCELADO RNEC RNEC);                                                              1\n",
      "GN0034(CAVIEDES);GN0035(YABIMAY);GN0036(NOAH);GN0037(JESUS);                                            1\n",
      "GN0258(2|A03|B01);                                                                                      1\n",
      "GN0034(TORRES);GN0035(PREGONERO);                                                                       1\n",
      "GN0258(2|B04|C11);                                                                                      1\n",
      "GN0018(CC|23726083|FALLECIDO RNEC RUAF_ND);GN0018(CC|23726083|FALLECIDO RNEC RUAF_ND);                  1\n",
      "GN0258(2|A02|A04);                                                                                      1\n",
      "GN0258(2|A03|A02);                                                                                      1\n",
      "GN0258(2|B03|C06);                                                                                      1\n",
      "GN0258(2|C01|A05);                                                                                      1\n",
      "GN0009(C|EPS037|01/09/2022|50|573|C|AC|03/01/2025);                                                     1\n",
      "GN0009(S|EPS025|01/05/2023|85|250|B|AC|01/06/2023);                                                     1\n",
      "GN0130(F|RE|04/03/2025);                                                                                1\n",
      "GN0084(cnd_afl_sbs|15/05/2025);                                                                         1\n",
      "GN0031(TI|1115867506|ACEVEDO|GUATIBONZA|MAUREN|GISSELL|S|EPS025|85|250);                                1\n",
      "GN0034(GUANAY);GN0035(ATILUA);                                                                          1\n",
      "GN0404(85410);                                                                                          1\n",
      "GN0501(A);                                                                                              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Df_NS_NEG['Glosa_2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f1ec2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df_NS_NEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54bdef",
   "metadata": {},
   "source": [
    "# Guardar Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "137391f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado en: C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\\DataFrames_Activos 23052025.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Guardar todos los DataFrames activos en un archivo Excel, una hoja por cada uno\n",
    "\n",
    "# Lista de DataFrames y nombres de hoja\n",
    "dfs_to_save = {\n",
    "    \"Df_NS_Anterior\": Df_NS_Anterior,\n",
    "    \"Df_NS_Envio\": Df_NS_Envio,\n",
    "    \"Df_NS_NEG\": Df_NS_NEG,\n",
    "    \"DF_NS_EPSC25\": DF_NS_EPSC25,\n",
    "    \"DF_No_Enviar\": DF_No_Enviar,\n",
    "    \"DF_ADRES\": DF_ADRES,\n",
    "    \"DF_059_169\": DF_059_169\n",
    "}\n",
    "\n",
    "# Ruta de salida y nombre de archivo\n",
    "output_path = Path(R_Salida) / f\"DataFrames_Activos {F_Envio}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name, df in dfs_to_save.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # Excel limita el nombre de hoja a 31 caracteres\n",
    "\n",
    "print(f\"Archivo guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
