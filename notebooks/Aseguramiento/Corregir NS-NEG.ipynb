{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52061c12",
   "metadata": {},
   "source": [
    "# 1. Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ece43",
   "metadata": {},
   "source": [
    "# 2. Rutas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257bdc",
   "metadata": {},
   "source": [
    "1. Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2401c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"19/09/2025\"\n",
    "F_Envio = \"19092025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2d5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada \n",
    "\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0016092025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-02\\EPS025MS0016092025.TXT\"\n",
    "R_NS_NEG = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02512092025.NEG\"\n",
    "R_NS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\09_Septiempre\\19\\SIE_NSEPS02519092025.txt\"\n",
    "R_NS_No_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\09_Septiempre\\19\\No enviar 19-09-2025.txt\"\n",
    "R_NS_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\09_Septiempre\\19\\Pendiente 19-09-2025.txt\"\n",
    "\n",
    "R_Ips = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Constantes\\IPS_CODIGO.txt\"\n",
    "\n",
    "# Ruta Salida\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\09_Septiempre\\19\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e898105",
   "metadata": {},
   "source": [
    "2. Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca22530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada\n",
    "\n",
    "#R_Ms_ADRES_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0020052025.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0020052025.TXT\"\n",
    "#R_NS_NEG = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02516052025.NEG\"\n",
    "\n",
    "# Ruta Salida\n",
    "\n",
    "#R_Salida = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d22915",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID_ADRES\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5851b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros en Df_NS_Anterior: 179\n"
     ]
    }
   ],
   "source": [
    "# Dataframe NS anterior\n",
    "new_columns = [\"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNS_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"]\n",
    "\n",
    "\n",
    "# Dataframe NS Glosado\n",
    "new_columns.append(\"Glosa\")\n",
    "Df_NS_NEG = pd.read_csv(R_NS_NEG, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NS_NEG.columns = new_columns\n",
    "print(\"Número de registros en Df_NS_Anterior:\", Df_NS_NEG.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1275f",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0167e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coiNSide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# 1) Tu merge original (trae ENT_ID_ADRES y TPS_EST_AFL_ID_from_adres de ADRES)\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer  = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "\n",
    "# 2) Preparamos un mini-dataframe sólo con las filas N01 que sí sí cruzaron con ADRES\n",
    "mapa_n01 = (\n",
    "    Df_NS_NEG.loc[Df_NS_NEG[\"NOVEDAD\"] == \"N01\", \n",
    "                  [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .copy()  # Asegurarse de trabajar con una copia del DataFrame\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\": \"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\": \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\": \"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\": \"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# 3) Merge “secundario” contra esas N01 para rellenar vacíos\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4) Para los que quedaron con ENT_ID_ADRES NaN, y son evoluciones (p.ej. N02, N03, …),\n",
    "#    rellenamos con el valor traído de la fila N01\n",
    "mask = Df_NS_NEG[\"ENT_ID_ADRES\"].isna() & Df_NS_NEG[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    "Df_NS_NEG.loc[mask, \"ENT_ID_ADRES\"]                   = Df_NS_NEG.loc[mask, \"ENT_ID_from_self\"]\n",
    "Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "# 5) Limpiamos las columnas auxiliares\n",
    "Df_NS_NEG.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf23a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros en Df_NS_NEG: 179\n",
      "Número de registros en DF_NS_EPSC25: 2\n",
      "Número de registros en DF_No_Enviar: 0\n",
      "Número de registros en Df_NS_NEG: 177\n",
      "Número de registros en Df_NS_Envio: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "\n",
    "# 1. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_NS_NEG[\"ENT_ID_ADRES\"] == \"EPSC25\")\n",
    ")\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_NS_EPSC25 = Df_NS_NEG.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_NS_NEG = Df_NS_NEG.loc[~mask].copy()\n",
    "\n",
    "# 4. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    Df_NS_NEG[\"ENT_ID_ADRES\"].isna()                 # NaN\n",
    "    | (Df_NS_NEG[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")  # \"\" o \"   \"\n",
    ")\n",
    "\n",
    "# Asignar un DataFrame vacío con las mismas columnas que el original\n",
    "DF_059_169 = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "Df_NS_Envio = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "DF_No_Enviar = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "\n",
    "print(\"Número de registros en DF_NS_EPSC25:\", DF_NS_EPSC25.shape[0])\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "print(\"Número de registros en Df_NS_Envio:\", Df_NS_Envio.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d474368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros únicos en Code_Glosa: 22\n",
      "Code_Glosa\n",
      "GN0084    34\n",
      "GN0169    23\n",
      "GN0390    21\n",
      "GN0032    16\n",
      "GN0030    15\n",
      "GN0059    15\n",
      "GN0031     8\n",
      "GN0009     7\n",
      "GN0036     6\n",
      "GN0501     6\n",
      "GN0361     5\n",
      "GN0258     4\n",
      "GN0130     3\n",
      "GN0014     3\n",
      "GN0018     2\n",
      "GN0122     2\n",
      "GN0404     2\n",
      "GN0079     1\n",
      "GN0037     1\n",
      "GN0113     1\n",
      "GN0011     1\n",
      "GN0035     1\n",
      "Name: count, dtype: int64\n",
      "Valores únicos en No_Glosa: [2 3 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Crear la columna \"Code_Glosa\" con los primeros seis caracteres de la columna \"Glosa\"\n",
    "Df_NS_NEG[\"Code_Glosa\"] = Df_NS_NEG[\"Glosa\"].str[:6]\n",
    "\n",
    "# Crear la columna \"No_Glosa\" contando el carácter ';' en \"Glosa\" y sumando 1 si no está vacía\n",
    "Df_NS_NEG[\"No_Glosa\"] = Df_NS_NEG[\"Glosa\"].apply(lambda x: x.count(';') + 1 if pd.notnull(x) and x != \"\" else 0)\n",
    "\n",
    "# Imprimir el número de registros únicos de la columna \"Code_Glosa\"\n",
    "print(\"Registros únicos en Code_Glosa:\", Df_NS_NEG[\"Code_Glosa\"].nunique())\n",
    "# Imprimir los registros únicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())\n",
    "\n",
    "# Imprimir los valores únicos de la columna \"No_Glosa\"\n",
    "print(\"Valores únicos en No_Glosa:\", Df_NS_NEG[\"No_Glosa\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59425d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de columnas antes: 25\n",
      "Número de columnas después: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de columnas antes:\", Df_NS_NEG.shape[1])\n",
    "Df_NS_NEG[\"Glosa_2\"] = Df_NS_NEG[\"Glosa\"]\n",
    "print(\"Número de columnas después:\", Df_NS_NEG.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44196c3d",
   "metadata": {},
   "source": [
    "## Calcular edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a3622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def agregar_edad(df, col_fecha_nac, fecha_hoy, nuevo_nombre='EDAD'):\n",
    "    \"\"\"\n",
    "    Devuelve una copia de df con una columna nuevo_nombre que contiene\n",
    "    la edad (en años cumplidos) calculada al día fecha_hoy,\n",
    "    partiendo de col_fecha_nac en formato 'dd/mm/yyyy'.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    \n",
    "    # 1) Parsear las fechas de nacimiento\n",
    "    nac_dt = pd.to_datetime(df2[col_fecha_nac], format='%d/%m/%Y', dayfirst=True)\n",
    "    \n",
    "    # 2) Convertir la fecha fija a datetime\n",
    "    hoy = datetime.strptime(fecha_hoy, '%d/%m/%Y')\n",
    "    \n",
    "    # 3) Determinar si ya cumplió años este año:\n",
    "    ha_cumplido = (\n",
    "        (nac_dt.dt.month < hoy.month) |\n",
    "        ((nac_dt.dt.month == hoy.month) & (nac_dt.dt.day <= hoy.day))\n",
    "    )\n",
    "    \n",
    "    # 4) Calcular edad: diferencia de años menos 0/1 según ha cumplido o no\n",
    "    df2[nuevo_nombre] = (\n",
    "        hoy.year - nac_dt.dt.year\n",
    "        - (~ha_cumplido).astype(int)\n",
    "    )\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "\n",
    "DF_ADRES = agregar_edad(\n",
    "    DF_ADRES,\n",
    "    col_fecha_nac='AFL_FECHA_NACIMIENTO',\n",
    "    fecha_hoy= Fecha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bfb05",
   "metadata": {},
   "source": [
    "# 5. Limpiar Glosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe5087",
   "metadata": {},
   "source": [
    "## 🚩 GN0009    \n",
    "\n",
    "1.  Cotizante principal (CNT) / Cabeza de familia (SBS), existe en la BDUA o en el Maestro de Ingresos, pero no pertenece a la misma entidad / régimen o no esta en calidad de Cotizante principal / Cabeza de familia.\n",
    "2. GN0009(BDUA|C|EPS002|22/11/1997|B|RE|29/10/2007)\n",
    "3. GN0009(Origen|RegimenCot/Cab|EntidadCot/Cab|FechaAfilEntidadCot/Cab|TipoAfiliado|EstadoAfilCot/Cab|FechaInicioCondicion)\n",
    "4. El Cotizante principal /cabeza de familia que pretenden relacionar al beneficiario, existe en la BDUA o en el Maestro de Ingresos, pero no pertenece a la misma entidad / régimen o no esta en calidad de Cotizante principal / Cabeza de familia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58de3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando process_gn0009 ---\n",
      "Antes: Df_NS_NEG = 177\n",
      "Antes: Df_NS_Envio = 0\n",
      "\n",
      "Después: Df_NS_NEG = 170\n",
      "Después: DF_NS_EPSC25 = 4\n",
      "Después: DF_No_Enviar = 5\n",
      "Después: Df_NS_Envio = 0\n",
      "--- Finalizando process_gn0009 ---\n",
      "Empty DataFrame\n",
      "Columns: [NUM_SOLICITUD_NOVEDAD, ENT_ID, TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, AFL_PRIMER_APELLIDO, AFL_SEGUNDO_APELLIDO, AFL_PRIMER_NOMBRE, AFL_SEGUNDO_NOMBRE, AFL_FECHA_NACIMIENTO, DPR_ID, MNS_ID, NOVEDAD, FECHA_NOVEDAD, COD_1_NOVEDAD, COD_2_NOVEDAD, COD_3_NOVEDAD, COD_4_NOVEDAD, COD_5_NOVEDAD, COD_6_NOVEDAD, COD_7_NOVEDAD, Glosa, ENT_ID_ADRES, TPS_EST_AFL_ID_from_adres]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_gn0009(\n",
    "    Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio, DF_ADRES, Fecha\n",
    "):\n",
    "    \"\"\"\n",
    "    Clasifica y procesa la glosa GN0009. Versión final con corrección para\n",
    "    prevenir la contaminación de columnas y el error InvalidIndexError.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando process_gn0009 ---\")\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "\n",
    "    # --- 1. Inicialización ---\n",
    "    DF_No_Enviar_updated = DF_No_Enviar.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "    DF_NS_EPSC25_updated = DF_NS_EPSC25.copy()\n",
    "\n",
    "    mask_gn0009 = Df_NS_NEG['Glosa_2'].str.contains('GN0009', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0009].copy()\n",
    "    \n",
    "    # CORRECCIÓN 1: Eliminar columnas duplicadas del DataFrame de origen.\n",
    "    df_gn = df_gn.loc[:, ~df_gn.columns.duplicated()]\n",
    "\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0009.\")\n",
    "        return Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio\n",
    "\n",
    "    # --- 2. Extracción y Clasificación ---\n",
    "    regex_entidad = r'GN0009\\([^|]*\\|([^|]*)\\|'\n",
    "    df_gn['EntidadCotCab'] = df_gn['Glosa_2'].str.extract(regex_entidad, expand=False).str.strip()\n",
    "\n",
    "    def classify_row(row):\n",
    "        if pd.isna(row['EntidadCotCab']) or row['EntidadCotCab'] == '': return 'Validar - Extraccion Fallida'\n",
    "        if row['NOVEDAD'] != 'N32': return 'Validar - Novedad Incorrecta'\n",
    "        if row['EntidadCotCab'] == 'EPS025': return 'Procesar Subsidiado'\n",
    "        if row['EntidadCotCab'] == 'EPSC25': return 'Procesar Contributivo'\n",
    "        return 'No Enviar'\n",
    "    df_gn['classification'] = df_gn.apply(classify_row, axis=1)\n",
    "    \n",
    "    # --- 3. Procesamiento por Categoría ---\n",
    "    df_para_validar = pd.DataFrame()\n",
    "\n",
    "    # CASO: No Enviar (Otra EPS)\n",
    "    df_no_enviar_rows = df_gn[df_gn['classification'] == 'No Enviar'].copy()\n",
    "    if not df_no_enviar_rows.empty:\n",
    "        df_aligned = df_no_enviar_rows.reindex(columns=DF_No_Enviar_updated.columns)\n",
    "        df_aligned['Motivo'] = \"Cabeza de familia en otra EPS\"\n",
    "        DF_No_Enviar_updated = pd.concat([DF_No_Enviar_updated, df_aligned], ignore_index=True)\n",
    "\n",
    "    # CASO: Procesar Contributivo (EPSC25)\n",
    "    df_contributivo_rows = df_gn[df_gn['classification'] == 'Procesar Contributivo'].copy()\n",
    "    if not df_contributivo_rows.empty:\n",
    "        adres_epsc25 = DF_ADRES[DF_ADRES['ENT_ID_ADRES'] == 'EPSC25']\n",
    "        df_merged = pd.merge(df_contributivo_rows, adres_epsc25, how='left', \n",
    "                             left_on=['COD_1_NOVEDAD', 'COD_2_NOVEDAD'], \n",
    "                             right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], \n",
    "                             indicator=True, suffixes=('_original', '_ADRES'))\n",
    "        \n",
    "        df_encontrados = df_merged[df_merged['_merge'] == 'both'].copy()\n",
    "        if not df_encontrados.empty:\n",
    "            reconstructed_data = {}\n",
    "            for col_name in DF_NS_EPSC25_updated.columns:\n",
    "                col_adres, col_original = f\"{col_name}_ADRES\", f\"{col_name}_original\"\n",
    "                if col_adres in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_adres].values\n",
    "                elif col_original in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_original].values\n",
    "                elif col_name in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_name].values\n",
    "            \n",
    "            df_clean = pd.DataFrame(reconstructed_data, index=df_encontrados.index)\n",
    "            df_clean['Where'] = \"Reportar en R1, unificación grupo familiar\"\n",
    "            DF_NS_EPSC25_updated = pd.concat([DF_NS_EPSC25_updated, df_clean], ignore_index=True)\n",
    "        \n",
    "        df_no_encontrados = df_merged[df_merged['_merge'] == 'left_only']\n",
    "        if not df_no_encontrados.empty:\n",
    "            # CORRECCIÓN 2: Limpiar df_no_encontrados antes de añadirlo a la pila de validación.\n",
    "            df_no_encontrados_clean = df_no_encontrados[df_gn.columns].copy()\n",
    "            df_para_validar = pd.concat([df_para_validar, df_no_encontrados_clean], ignore_index=True)\n",
    "\n",
    "    # CASO: Procesar Subsidiado (EPS025)\n",
    "    df_subsidiado_rows = df_gn[df_gn['classification'] == 'Procesar Subsidiado'].copy()\n",
    "    if not df_subsidiado_rows.empty:\n",
    "        df_subsidiado_rows['FECHA_NOVEDAD'] = Fecha\n",
    "        df_subsidiado_rows['Glosa_2'] = df_subsidiado_rows['Glosa_2'].str.replace(r'\\s*GN0009\\([^)]*\\);?', '', regex=True)\n",
    "        df_subsidiado_rows['No_Glosa'] = df_subsidiado_rows['Glosa_2'].str.count(';')\n",
    "        \n",
    "        df_envio_move = df_subsidiado_rows[df_subsidiado_rows['No_Glosa'] == 0].copy()\n",
    "        df_remain = df_subsidiado_rows[df_subsidiado_rows['No_Glosa'] > 0].copy()\n",
    "\n",
    "        if not df_envio_move.empty:\n",
    "            df_aligned = df_envio_move.reindex(columns=Df_NS_Envio_updated.columns)\n",
    "            Df_NS_Envio_updated = pd.concat([Df_NS_Envio_updated, df_aligned], ignore_index=True)\n",
    "        \n",
    "        if not df_remain.empty:\n",
    "            df_para_validar = pd.concat([df_para_validar, df_remain], ignore_index=True)\n",
    "\n",
    "    # --- 4. Reconstrucción Final ---\n",
    "    df_para_validar = pd.concat([\n",
    "        df_para_validar,\n",
    "        df_gn[df_gn['classification'].str.startswith('Validar')]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    if not df_para_validar.empty:\n",
    "        if 'NUM_SOLICITUD_NOVEDAD' in df_para_validar.columns:\n",
    "             df_para_validar = df_para_validar.drop_duplicates(subset=['NUM_SOLICITUD_NOVEDAD'])\n",
    "        df_para_validar['Observacion_Validacion'] = 'Validar Novedad/Glosa GN0009'\n",
    "        df_para_validar['Code_Glosa'] = 'GN0009'\n",
    "        df_aligned = df_para_validar.reindex(columns=Df_NS_NEG.columns)\n",
    "    else:\n",
    "        df_aligned = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "        \n",
    "    Df_NS_NEG_final = pd.concat([Df_NS_NEG[~mask_gn0009], df_aligned], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nDespués: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Después: DF_NS_EPSC25 = {len(DF_NS_EPSC25_updated)}\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"--- Finalizando process_gn0009 ---\")\n",
    "\n",
    "    return Df_NS_NEG_final, DF_NS_EPSC25_updated, DF_No_Enviar_updated, Df_NS_Envio_updated\n",
    "\n",
    "\n",
    "# --- Ejemplo de uso ---\n",
    "# Asegúrate de que este nombre coincida con el de la definición\n",
    "Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio = process_gn0009(\n",
    "     Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio, DF_ADRES, Fecha\n",
    ")\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110229e",
   "metadata": {},
   "source": [
    "## 🚩 GN0030    \n",
    "\n",
    "1.  Afiliado no pertenece a la entidad / Régimen.\n",
    "2.  GN0030(Regimen|EntidadAfil|FechaInicioAfil|Dpto|Mpio|TipoAfil|EstadoAfil|FechaInicioCond);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f89220ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 170 registros\n",
      "Antes: DF_NS_EPSC25 = 4 registros\n",
      "Antes: DF_No_Enviar = 5 registros\n",
      "Después: Df_NS_NEG = 155 registros\n",
      "Después: DF_NS_EPSC25 = 14 registros\n",
      "Después: DF_No_Enviar = 10 registros\n"
     ]
    }
   ],
   "source": [
    "def process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0030 (“Afiliado no pertenece a la entidad / Régimen”) en un DataFrame.\n",
    "\n",
    "    Para cada registro en `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0030':\n",
    "      1. Extrae la fecha de condición (`FechaInicioCond`) de la glosa.\n",
    "      2. Convierte ambas fechas (`FECHA_NOVEDAD` y `FechaInicioCond`) a datetime.\n",
    "      3. Si `cod_ent == 'EPSC25'` y `FechaInicioCond >= FECHA_NOVEDAD`:\n",
    "         - Actualiza `FECHA_NOVEDAD` al día siguiente de `FechaInicioCond`.\n",
    "         - Mueve el registro a `DF_NS_EPSC25`.\n",
    "      4. En caso contrario:\n",
    "         - Mueve el registro a `DF_No_Enviar`.\n",
    "         - Asigna motivo \"Afiliado en proceso de traslado a otra EPS\".\n",
    "      5. En ambos DataFrames de destino elimina las columnas de validación:\n",
    "         ['Glosa_2', 'No_Glosa', 'Code_Glosa'].\n",
    "      6. Imprime conteos de registros en los DataFrames antes y después de la operación.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        DF_NS_EPSC25 (pd.DataFrame): DataFrame destino para registros EPSC25.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para envío general.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros originales sin GN0030.\n",
    "            - DF_NS_EPSC25_updated (pd.DataFrame): Registros EPSC25 procesados.\n",
    "            - DF_No_Enviar_updated (pd.DataFrame): Registros restantes con motivo de traslado.\n",
    "    \"\"\"\n",
    "    # Imprimir conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_NS_EPSC25 = {len(DF_NS_EPSC25)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # Máscara para registros con GN0030\n",
    "    mask_gn0030 = Df_NS_NEG['Glosa_2'].str.contains('GN0030', na=False)\n",
    "    df_gn0030 = Df_NS_NEG[mask_gn0030].copy()\n",
    "\n",
    "    # 1) Extraer cod_ent (grupo 2)\n",
    "    df_gn0030['cod_ent'] = df_gn0030['Glosa_2'].str.extract(r'GN0030\\([^|]*\\|([^|]*)\\|')\n",
    "\n",
    "    # 2) Extraer FechaInicioCond (grupo último)\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "        \n",
    "    # Extraer FechaInicioCond de la glosa GN0030\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "    # Parsear fechas\n",
    "    df_gn0030['FechaInicioCond_dt'] = pd.to_datetime(df_gn0030['FechaInicioCond'], dayfirst=True, format='%d/%m/%Y')\n",
    "    df_gn0030['FECHA_NOVEDAD_dt'] = pd.to_datetime(df_gn0030['FECHA_NOVEDAD'], dayfirst=True, format='%d/%m/%Y')\n",
    "    \n",
    "    # Caso 1: EPSC25 y FechaInicioCond >= FECHA_NOVEDAD\n",
    "    mask1 = (df_gn0030['cod_ent'] == 'EPSC25') # & (df_gn0030['FechaInicioCond_dt'] >= df_gn0030['FECHA_NOVEDAD_dt'])\n",
    "    df_case1 = df_gn0030[mask1].copy()\n",
    "    # Actualizar FECHA_NOVEDAD a FechaInicioCond + 1 día\n",
    "    df_case1['FECHA_NOVEDAD'] = (df_case1['FechaInicioCond_dt'] + timedelta(days=1)).dt.strftime('%d/%m/%Y')\n",
    "    # Preparar para mover\n",
    "    df_case1 = df_case1.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Caso 2: resto de GN0030\n",
    "    df_case2 = df_gn0030[~mask1].copy()\n",
    "    df_case2['Motivo'] = \"Afiliado en proceso de traslado a otra EPS\"\n",
    "    df_case2 = df_case2.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Mover registros\n",
    "    DF_NS_EPSC25_updated = pd.concat([DF_NS_EPSC25, df_case1], ignore_index=True)\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_case2], ignore_index=True)\n",
    "    \n",
    "    # Quedar en Df_NS_NEG solo los que no tenían GN0030\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask_gn0030].copy()\n",
    "    \n",
    "    # Imprimir conteos finales\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: DF_NS_EPSC25 = {len(DF_NS_EPSC25_updated)} registros\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG_updated, DF_NS_EPSC25_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Uso de ejemplo:\n",
    "Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar = process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbbd93",
   "metadata": {},
   "source": [
    "## 🚩 GN0031\n",
    "\n",
    "1. Afiliado no existe en la BDUA.\n",
    "2. GN0031(TI|1124822660|GARZON|MORALES|DUVAN|FELIPE|S|EPSS03|25|245);\n",
    "3. GN0031(|||||||||||||||||||);\n",
    "4. GN0031(TipoDocBDUA|DocumentoBDUA|PrimerApeBDUA|SegundoApeBDUA|PrimerNomBDUA|SegundoNomBDUA|Regimen|EntidadBDUA|Dpto|Mpio);\n",
    "5. GN0031(SinInformacionBDUA);\n",
    "6. Es muy frecuente que se solicite con un tipo de documento diferente al cargado en la BDUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80399309",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 155\n",
      "Antes: Df_NS_Envio = 0\n",
      "Antes: DF_No_Enviar = 10\n",
      "\n",
      "Después: Df_NS_NEG = 149\n",
      "Después: Df_NS_Envio = 3\n",
      "Después: DF_No_Enviar = 13\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    55  EPS025         RC                    1222147870   \n",
      "1                    69  EPS025         RC                    1118583296   \n",
      "2                    80  EPS025         RC                    1222147870   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0              GALVIZ                 RIOS           HIJO DE   \n",
      "1               NAOMI              FAJARDO          MADISSON   \n",
      "2              GALVIZ                 RIOS             EMILY   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                NaN           28/08/2025     85  ...         EMILY   \n",
      "1              NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2            JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0       JULIANA           NaN           NaN           NaN           NaN   \n",
      "1       HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2       GALVIZ            NaN           NaN           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD               Glosa ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "1           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "2           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "\n",
      "[3 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_gn0031(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0031 de forma robusta, preservando la estructura de los DataFrames.\n",
    "    - Caso 1: N01 y ADRES vacío -> No Enviar.\n",
    "    - Caso 2: ≠ N01 y ADRES lleno -> Limpiar glosa y enviar si aplica.\n",
    "    - Caso 3: ≠ N01 y ADRES vacío -> Validación manual.\n",
    "    - Caso 4: N01 y ADRES lleno -> Validación manual (PQR).\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1. INICIAMOS COPIAS PARA ACUMULAR RESULTADOS\n",
    "    # Esto evita errores si un caso no encuentra registros.\n",
    "    DF_No_Enviar_updated = DF_No_Enviar.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "\n",
    "    # Validar y filtrar registros con GN0031\n",
    "    required_columns = ['Glosa_2', 'NOVEDAD', 'TPS_EST_AFL_ID_from_adres']\n",
    "    for col in required_columns:\n",
    "        if col not in Df_NS_NEG.columns:\n",
    "            raise ValueError(f\"Columna requerida no encontrada: {col}\")\n",
    "    \n",
    "    mask_gn0031 = Df_NS_NEG['Glosa_2'].str.contains('GN0031', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0031].copy()\n",
    "\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0031.\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "    # --- DataFrames temporales para cada caso (solo filas) ---\n",
    "    is_n01 = df_gn['NOVEDAD'] == 'N01'\n",
    "    is_adres_empty = df_gn['TPS_EST_AFL_ID_from_adres'].isna() | (df_gn['TPS_EST_AFL_ID_from_adres'].astype(str).str.strip() == \"\")\n",
    "\n",
    "    df_case1 = df_gn[is_n01 & is_adres_empty].copy()\n",
    "    df_case2 = df_gn[~is_n01 & ~is_adres_empty].copy()\n",
    "    df_case3 = df_gn[~is_n01 & is_adres_empty].copy()\n",
    "    df_case4 = df_gn[is_n01 & ~is_adres_empty].copy()\n",
    "\n",
    "    # --- Procesar cada caso ---\n",
    "\n",
    "    # CASO 1: Mover a 'No Enviar'\n",
    "    if not df_case1.empty:\n",
    "        df_case1['Motivo'] = \"Evolución/corrección ya efectiva en ADRES\"\n",
    "        # 2. ALINEAMOS COLUMNAS ANTES DE UNIR\n",
    "        df_aligned = df_case1.reindex(columns=DF_No_Enviar_updated.columns)\n",
    "        DF_No_Enviar_updated = pd.concat([DF_No_Enviar_updated, df_aligned], ignore_index=True)\n",
    "\n",
    "    # CASO 2: Limpiar glosa y mover a 'Envio' si aplica\n",
    "    df_case2_remain = pd.DataFrame() # Para los que se quedan en NEG\n",
    "    if not df_case2.empty:\n",
    "        df_case2['Glosa_2'] = df_case2['Glosa_2'].str.replace(r'GN0031\\([^)]*\\);', '', regex=True)\n",
    "        df_case2['No_Glosa'] = df_case2['Glosa_2'].str.count(';')\n",
    "        df_case2['Code_Glosa'] = df_case2['Glosa_2'].str.extract(r'(GN\\d{4})', expand=False).fillna('0')\n",
    "        \n",
    "        # Mover los que ya no tienen glosas\n",
    "        df_envio_move = df_case2[df_case2['No_Glosa'] == 0]\n",
    "        if not df_envio_move.empty:\n",
    "            df_aligned = df_envio_move.reindex(columns=Df_NS_Envio_updated.columns)\n",
    "            Df_NS_Envio_updated = pd.concat([Df_NS_Envio_updated, df_aligned], ignore_index=True)\n",
    "        \n",
    "        # Guardar los que aún tienen otras glosas\n",
    "        df_case2_remain = df_case2[df_case2['No_Glosa'] > 0]\n",
    "\n",
    "    # CASO 3: Añadir observación para validación manual\n",
    "    if not df_case3.empty:\n",
    "        df_case3['Observacion_Validacion'] = \"Validar GN0031: posible error digitación o falta de N01 previo.\"\n",
    "\n",
    "    # CASO 4: Añadir observación para PQR\n",
    "    if not df_case4.empty:\n",
    "        df_case4['Observacion_Validacion'] = \"Validar GN0031: ADRES lo tiene pero lo glosa. Requiere PQR.\"\n",
    "\n",
    "    # --- Reconstruir Df_NS_NEG final ---\n",
    "    \n",
    "    # 3. JUNTAMOS TODAS LAS PIEZAS QUE SE QUEDAN EN NEG\n",
    "    piezas_para_neg = [df_case2_remain, df_case3, df_case4]\n",
    "    df_para_neg_final = pd.concat(piezas_para_neg, ignore_index=True)\n",
    "    \n",
    "    # ALINEAMOS EL LOTE COMPLETO a la estructura original de Df_NS_NEG\n",
    "    df_aligned_neg = df_para_neg_final.reindex(columns=Df_NS_NEG.columns)\n",
    "    \n",
    "    Df_NS_NEG_final = pd.concat([\n",
    "        Df_NS_NEG[~mask_gn0031], # Los que no tenían la glosa\n",
    "        df_aligned_neg           # Los que tenían la glosa y se quedan\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nDespués: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_No_Enviar_updated\n",
    "\n",
    "# --- 1. Llamado de la función corregida ---\n",
    "# Asegúrate de que las variables estén inicializadas previamente.\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0031(\n",
    "    Df_NS_NEG=Df_NS_NEG,\n",
    "    Df_NS_Envio=Df_NS_Envio,\n",
    "    DF_No_Enviar=DF_No_Enviar\n",
    ")\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e8ff2",
   "metadata": {},
   "source": [
    "## 🚩 GN0032\n",
    "\n",
    "1. Afiliado existe en BDUA en estado Fallecido.\n",
    "   1. GN0032(Regimen|Entidad|FechaInicioAfil|Dpto|Mpio|TipoAfiliado|EstadoBDUA|FechaInicioCond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac71001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG tiene 149 registros\n",
      "Antes: DF_No_Enviar tiene 13 registros\n",
      "Después: Df_NS_NEG tiene 133 registros\n",
      "Después: DF_No_Enviar tiene 29 registros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def move_gn0032(Df_NS_NEG, DF_No_Enviar):\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # Máscara para glosa GN0032\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0032', na=False)\n",
    "    \n",
    "    # Extraer los registros a mover\n",
    "    to_move = Df_NS_NEG[mask].copy()\n",
    "    # Asignar motivo\n",
    "    to_move['Motivo'] = \"Fallecido en actualmente ADRES\"\n",
    "    # Eliminar columnas auxiliares\n",
    "    to_move = to_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    \n",
    "    # Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "    \n",
    "    # Quedarse en Df_NS_NEG con los que NO tienen GN0032\n",
    "    Df_NS_NEG = Df_NS_NEG[~mask].copy()\n",
    "    \n",
    "    # Conteos finales\n",
    "    print(f\"Después: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Después: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG, DF_No_Enviar\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = move_gn0032(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8d17c",
   "metadata": {},
   "source": [
    "## 🚩 GN0034\n",
    "\n",
    "1. Primer Apellido diferente al registrado en la BDUA.\n",
    "2. GN0034(MARTINEZ);\n",
    "3. GN0034(PrimerApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5db1795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 133 registros\n",
      "Antes: Df_NS_Envio = 3 registros\n",
      "Después: Df_NS_NEG = 133 registros\n",
      "Después: Df_NS_Envio = 3 registros\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    55  EPS025         RC                    1222147870   \n",
      "1                    69  EPS025         RC                    1118583296   \n",
      "2                    80  EPS025         RC                    1222147870   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0              GALVIZ                 RIOS           HIJO DE   \n",
      "1               NAOMI              FAJARDO          MADISSON   \n",
      "2              GALVIZ                 RIOS             EMILY   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                NaN           28/08/2025     85  ...         EMILY   \n",
      "1              NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2            JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0       JULIANA           NaN           NaN           NaN           NaN   \n",
      "1       HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2       GALVIZ            NaN           NaN           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD               Glosa ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "1           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "2           NaN  GN0031(|||||||||);       EPS025                        AC  \n",
      "\n",
      "[3 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0034(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0034 (Primer Apellido diferente al registrado en la BDUA).\n",
    "    - Extrae el apellido correcto de la glosa GN0034(PrimerApeBDUA);\n",
    "    - Actualiza AFL_PRIMER_APELLIDO con ese valor;\n",
    "    - Elimina la glosa GN0034(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0034\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0034\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer apellido y actualizar AFL_PRIMER_APELLIDO\n",
    "    def extract_apellido(glosa):\n",
    "        match = re.search(r'GN0034\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_PRIMER_APELLIDO'] = df_gn['Glosa_2'].apply(extract_apellido)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0034(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0034\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0034(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c31f41",
   "metadata": {},
   "source": [
    "## 🚩 GN0035\n",
    "\n",
    "1. Segundo Apellido diferente al registrado en la BDUA.\n",
    "2. GN0035(SUAREZ);\n",
    "3. GN0035(SegundoApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42136af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 133 registros\n",
      "Antes: Df_NS_Envio = 3 registros\n",
      "Después: Df_NS_NEG = 132 registros\n",
      "Después: Df_NS_Envio = 4 registros\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    55  EPS025         RC                    1222147870   \n",
      "1                    69  EPS025         RC                    1118583296   \n",
      "2                    80  EPS025         RC                    1222147870   \n",
      "3                  1937  EPS025         CC                       4270683   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0              GALVIZ                 RIOS           HIJO DE   \n",
      "1               NAOMI              FAJARDO          MADISSON   \n",
      "2              GALVIZ                 RIOS             EMILY   \n",
      "3              ESTEPA          MENDILVELSO          NORBERTO   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                NaN           28/08/2025     85  ...         EMILY   \n",
      "1              NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2            JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                NaN           06/06/1947     85  ...         85400   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0       JULIANA           NaN           NaN           NaN           NaN   \n",
      "1       HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2       GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD                 Glosa ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "1           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "2           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "3           NaN  GN0035(MENDILVELSO);       EPS025                        AC  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0035(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0035 (Segundo Apellido diferente al registrado en la BDUA).\n",
    "    - Extrae el segundo apellido correcto de la glosa GN0035(SegundoApeBDUA);\n",
    "    - Actualiza AFL_SEGUNDO_APELLIDO con ese valor;\n",
    "    - Elimina la glosa GN0035(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0035\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0035\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer segundo apellido y actualizar AFL_SEGUNDO_APELLIDO\n",
    "    def extract_segundo_apellido(glosa):\n",
    "        match = re.search(r'GN0035\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_SEGUNDO_APELLIDO'] = df_gn['Glosa_2'].apply(extract_segundo_apellido)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0035(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0035\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0035(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417554e6",
   "metadata": {},
   "source": [
    "## 🚩 GN0036\n",
    "\n",
    "1. Primer Nombre diferente al registrado en la BDUA.\n",
    "2. GN0036(MARCELA);\n",
    "3. GN0036(PrimerNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9635ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 132 registros\n",
      "Antes: Df_NS_Envio = 4 registros\n",
      "Después: Df_NS_NEG = 131 registros\n",
      "Después: Df_NS_Envio = 5 registros\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    55  EPS025         RC                    1222147870   \n",
      "1                    69  EPS025         RC                    1118583296   \n",
      "2                    80  EPS025         RC                    1222147870   \n",
      "3                  1937  EPS025         CC                       4270683   \n",
      "4                    91  EPS025         CC                       6671964   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0              GALVIZ                 RIOS           HIJO DE   \n",
      "1               NAOMI              FAJARDO          MADISSON   \n",
      "2              GALVIZ                 RIOS             EMILY   \n",
      "3              ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4           RODRIGUEZ                BRITO             DIMAR   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                NaN           28/08/2025     85  ...         EMILY   \n",
      "1              NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2            JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                NaN           06/06/1947     85  ...         85400   \n",
      "4                NaN           10/03/1952     85  ...        MORENO   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0       JULIANA           NaN           NaN           NaN           NaN   \n",
      "1       HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2       GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3           NaN           NaN           NaN           NaN           NaN   \n",
      "4         BRITO           NaN           NaN           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD                 Glosa ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "1           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "2           NaN    GN0031(|||||||||);       EPS025                        AC  \n",
      "3           NaN  GN0035(MENDILVELSO);       EPS025                        AC  \n",
      "4           NaN        GN0036(DIMAR);       EPS025                        AC  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0036(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0036 (Primer Nombre diferente al registrado en la BDUA).\n",
    "    - Extrae el primer nombre correcto de la glosa GN0036(PrimerNomBDUA);\n",
    "    - Actualiza AFL_PRIMER_NOMBRE con ese valor;\n",
    "    - Elimina la glosa GN0036(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0036\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0036\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer primer nombre y actualizar AFL_PRIMER_NOMBRE\n",
    "    def extract_primer_nombre(glosa):\n",
    "        match = re.search(r'GN0036\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_PRIMER_NOMBRE'] = df_gn['Glosa_2'].apply(extract_primer_nombre)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0036(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0036\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0036(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e136c",
   "metadata": {},
   "source": [
    "## 🚩 GN0037\n",
    "\n",
    "1. Segundo nombre diferente al registrado en la BDUA.\n",
    "2. GN0037(CARLOS);\n",
    "3. GN0037(SegundoNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a49704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 131 registros\n",
      "Antes: Df_NS_Envio = 5 registros\n",
      "Después: Df_NS_NEG = 125 registros\n",
      "Después: Df_NS_Envio = 11 registros\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...         EMILY   \n",
      "1               NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2             JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                 NaN           06/06/1947     85  ...         85400   \n",
      "4                 NaN           10/03/1952     85  ...        MORENO   \n",
      "5                None           03/12/1986     85  ...     VELASQUEZ   \n",
      "6                JOEL           18/08/2025     85  ...          JUAN   \n",
      "7               ALAIA           22/08/2025     85  ...         AILYN   \n",
      "8              ZAMIRA           25/08/2025     85  ...      LUISSANA   \n",
      "9           ANTONELLA           25/09/2023     85  ...       MORALES   \n",
      "10           VICTORIA           10/05/2024     85  ...       CARREÑO   \n",
      "\n",
      "   COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0        JULIANA           NaN           NaN           NaN           NaN   \n",
      "1        HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2        GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4          BRITO           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6           JOEL           NaN           NaN           NaN           NaN   \n",
      "7          ALAIA           NaN           NaN           NaN           NaN   \n",
      "8         ZAMIRA           NaN           NaN           NaN           NaN   \n",
      "9         TINOCO           NaN           NaN           NaN           NaN   \n",
      "10         GOMEZ           NaN           NaN           NaN           NaN   \n",
      "\n",
      "   COD_7_NOVEDAD                              Glosa ENT_ID_ADRES  \\\n",
      "0            NaN                 GN0031(|||||||||);       EPS025   \n",
      "1            NaN                 GN0031(|||||||||);       EPS025   \n",
      "2            NaN                 GN0031(|||||||||);       EPS025   \n",
      "3            NaN               GN0035(MENDILVELSO);       EPS025   \n",
      "4            NaN                     GN0036(DIMAR);       EPS025   \n",
      "5            NaN                          GN0037();       EPS025   \n",
      "6            NaN         GN0036(JUAN);GN0037(JOEL);       EPS025   \n",
      "7            NaN       GN0036(AILYN);GN0037(ALAIA);       EPS025   \n",
      "8            NaN   GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025   \n",
      "9            NaN     GN0036(ZOE);GN0037(ANTONELLA);       EPS025   \n",
      "10           NaN  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres  \n",
      "0                         AC  \n",
      "1                         AC  \n",
      "2                         AC  \n",
      "3                         AC  \n",
      "4                         AC  \n",
      "5                         AC  \n",
      "6                         AC  \n",
      "7                         AC  \n",
      "8                         AC  \n",
      "9                         AC  \n",
      "10                        AC  \n",
      "\n",
      "[11 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0037(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0037 (Segundo Nombre diferente al registrado en la BDUA).\n",
    "    - Extrae el Segundo nombre correcto de la glosa GN0037(SegundoNomBDUA);\n",
    "    - Actualiza AFL_SEGUNDO_NOMBRE con ese valor;\n",
    "    - Elimina la glosa GN0037(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0037\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0037\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer Segundo nombre y actualizar AFL_SEGUNDO_NOMBRE\n",
    "    def extract_segundo_nombre(glosa):\n",
    "        match = re.search(r'GN0037\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_SEGUNDO_NOMBRE'] = df_gn['Glosa_2'].apply(extract_segundo_nombre)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0037(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0037\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0037(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200a06a",
   "metadata": {},
   "source": [
    "## 🚩 GN0049\n",
    "1. Fecha de nacimiento diferente al registrado en la BDUA.\n",
    "2. GN0049(13/12/1993);\n",
    "3. GN0049(FechNacBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "052a635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 125 registros\n",
      "Antes: Df_NS_Envio = 11 registros\n",
      "Después: Df_NS_NEG = 125 registros\n",
      "Después: Df_NS_Envio = 11 registros\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...         EMILY   \n",
      "1               NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2             JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                 NaN           06/06/1947     85  ...         85400   \n",
      "4                 NaN           10/03/1952     85  ...        MORENO   \n",
      "5                None           03/12/1986     85  ...     VELASQUEZ   \n",
      "6                JOEL           18/08/2025     85  ...          JUAN   \n",
      "7               ALAIA           22/08/2025     85  ...         AILYN   \n",
      "8              ZAMIRA           25/08/2025     85  ...      LUISSANA   \n",
      "9           ANTONELLA           25/09/2023     85  ...       MORALES   \n",
      "10           VICTORIA           10/05/2024     85  ...       CARREÑO   \n",
      "\n",
      "   COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0        JULIANA           NaN           NaN           NaN           NaN   \n",
      "1        HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2        GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4          BRITO           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6           JOEL           NaN           NaN           NaN           NaN   \n",
      "7          ALAIA           NaN           NaN           NaN           NaN   \n",
      "8         ZAMIRA           NaN           NaN           NaN           NaN   \n",
      "9         TINOCO           NaN           NaN           NaN           NaN   \n",
      "10         GOMEZ           NaN           NaN           NaN           NaN   \n",
      "\n",
      "   COD_7_NOVEDAD                              Glosa ENT_ID_ADRES  \\\n",
      "0            NaN                 GN0031(|||||||||);       EPS025   \n",
      "1            NaN                 GN0031(|||||||||);       EPS025   \n",
      "2            NaN                 GN0031(|||||||||);       EPS025   \n",
      "3            NaN               GN0035(MENDILVELSO);       EPS025   \n",
      "4            NaN                     GN0036(DIMAR);       EPS025   \n",
      "5            NaN                          GN0037();       EPS025   \n",
      "6            NaN         GN0036(JUAN);GN0037(JOEL);       EPS025   \n",
      "7            NaN       GN0036(AILYN);GN0037(ALAIA);       EPS025   \n",
      "8            NaN   GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025   \n",
      "9            NaN     GN0036(ZOE);GN0037(ANTONELLA);       EPS025   \n",
      "10           NaN  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres  \n",
      "0                         AC  \n",
      "1                         AC  \n",
      "2                         AC  \n",
      "3                         AC  \n",
      "4                         AC  \n",
      "5                         AC  \n",
      "6                         AC  \n",
      "7                         AC  \n",
      "8                         AC  \n",
      "9                         AC  \n",
      "10                        AC  \n",
      "\n",
      "[11 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0049(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0049 (Fecha de nacimiento diferente al registrado en la BDUA).\n",
    "    - Extrae la fecha correcta de la glosa GN0049(FechNacBDUA);\n",
    "    - Actualiza AFL_FECHA_NACIMIENTO con ese valor;\n",
    "    - Elimina la glosa GN0049(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0049\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0049\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer fecha y actualizar AFL_FECHA_NACIMIENTO\n",
    "    def extract_fecha(glosa):\n",
    "        match = re.search(r'GN0049\\((\\d{2}/\\d{2}/\\d{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_FECHA_NACIMIENTO'] = df_gn['Glosa_2'].apply(extract_fecha)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0049(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0049\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0049(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501a9a9",
   "metadata": {},
   "source": [
    "## 🚩 GN0059\n",
    "1. Afiliado con datos certificados RNEC, no se puede aplicar esta novedad.\n",
    "2. GN0059;\n",
    "3. Los datos básicos del afiliado reportados en el archivo no corresponden con los certificados por la Registraduria Nacional del Estado Civil, se hace necesario revisar el documento del usuario para corroborar la información en caso de coincidir notificar inconsistencia ante ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00fd5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 125 registros\n",
      "Antes: Df_NS_Envio = 11 registros\n",
      "Antes: DF_059_169 = 0 registros\n",
      "Después: Df_NS_NEG = 122 registros\n",
      "Después: Df_NS_Envio = 14 registros\n",
      "Después: DF_059_169 = 15 registros\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "11                     1  EPS025         RC                    1222147416   \n",
      "12                     2  EPS025         RC                    1115921009   \n",
      "13                    11  EPS025         CN                25088610286171   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "11             HOLGUIN              FAJARDO          MADISSON   \n",
      "12              CASTRO                 DIAZ            EITHAN   \n",
      "13              GALVIZ                 RIOS           HIJO DE   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...         EMILY   \n",
      "1               NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2             JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                 NaN           06/06/1947     85  ...         85400   \n",
      "4                 NaN           10/03/1952     85  ...        MORENO   \n",
      "5                None           03/12/1986     85  ...     VELASQUEZ   \n",
      "6                JOEL           18/08/2025     85  ...          JUAN   \n",
      "7               ALAIA           22/08/2025     85  ...         AILYN   \n",
      "8              ZAMIRA           25/08/2025     85  ...      LUISSANA   \n",
      "9           ANTONELLA           25/09/2023     85  ...       MORALES   \n",
      "10           VICTORIA           10/05/2024     85  ...       CARREÑO   \n",
      "11              NAOMI           07/05/2025     85  ...            RC   \n",
      "12             JAVIER           27/01/2025     85  ...            RC   \n",
      "13                NaN           28/08/2025     85  ...            RC   \n",
      "\n",
      "   COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0        JULIANA           NaN           NaN           NaN           NaN   \n",
      "1        HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2        GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4          BRITO           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6           JOEL           NaN           NaN           NaN           NaN   \n",
      "7          ALAIA           NaN           NaN           NaN           NaN   \n",
      "8         ZAMIRA           NaN           NaN           NaN           NaN   \n",
      "9         TINOCO           NaN           NaN           NaN           NaN   \n",
      "10         GOMEZ           NaN           NaN           NaN           NaN   \n",
      "11    1118583296    07/05/2025             1           NaN           NaN   \n",
      "12    1115921012    27/01/2025             1           NaN           NaN   \n",
      "13    1222147870    28/08/2025             1           NaN           NaN   \n",
      "\n",
      "   COD_7_NOVEDAD                              Glosa ENT_ID_ADRES  \\\n",
      "0            NaN                 GN0031(|||||||||);       EPS025   \n",
      "1            NaN                 GN0031(|||||||||);       EPS025   \n",
      "2            NaN                 GN0031(|||||||||);       EPS025   \n",
      "3            NaN               GN0035(MENDILVELSO);       EPS025   \n",
      "4            NaN                     GN0036(DIMAR);       EPS025   \n",
      "5            NaN                          GN0037();       EPS025   \n",
      "6            NaN         GN0036(JUAN);GN0037(JOEL);       EPS025   \n",
      "7            NaN       GN0036(AILYN);GN0037(ALAIA);       EPS025   \n",
      "8            NaN   GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025   \n",
      "9            NaN     GN0036(ZOE);GN0037(ANTONELLA);       EPS025   \n",
      "10           NaN  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025   \n",
      "11           NaN                            GN0059;       EPS025   \n",
      "12           NaN                            GN0059;       EPS025   \n",
      "13           NaN                            GN0059;       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres  \n",
      "0                         AC  \n",
      "1                         AC  \n",
      "2                         AC  \n",
      "3                         AC  \n",
      "4                         AC  \n",
      "5                         AC  \n",
      "6                         AC  \n",
      "7                         AC  \n",
      "8                         AC  \n",
      "9                         AC  \n",
      "10                        AC  \n",
      "11                        AC  \n",
      "12                        AC  \n",
      "13                        AC  \n",
      "\n",
      "[14 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0059 (“Afiliado con datos certificados RNEC, no se puede aplicar esta novedad”).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0059;'.\n",
    "      2. Copia todos esos registros a `DF_059_169` (para verificación manual en auditoría).\n",
    "      3. Elimina la subcadena 'GN0059;' de `Glosa_2`.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vacío.\n",
    "      5. De los registros filtrados, mueve a `Df_NS_Envio` aquellos que pasaron a `No_Glosa == 0`.\n",
    "      6. Elimina las columnas auxiliares ['Glosa_2','No_Glosa','Code_Glosa'] \n",
    "         en los DataFrames `Df_NS_Envio` y `DF_059_169`.\n",
    "      7. Imprime conteos de registros antes y después en cada DataFrame.\n",
    "\n",
    "    Nota:\n",
    "      - La validación en Registraduría o Migración (web scraping con captcha) \n",
    "        queda comentada para futura implementación.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "        DF_059_169 (pd.DataFrame): DataFrame destino para auditoría GN0059.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated)\n",
    "    \"\"\"\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0059;\n",
    "    mask_gn = Df_NS_NEG['Glosa_2'].str.contains('GN0059;', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn].copy()\n",
    "\n",
    "    # 2) Copiar todos a DF_059_169 para verificación manual\n",
    "    df_059 = df_gn.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    DF_059_169_updated = pd.concat([DF_059_169, df_059], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0059;' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace('GN0059;', '', regex=False)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que pasaron a No_Glosa == 0 y tenían GN0059\n",
    "    mask_move = mask_gn & (Df_NS_NEG_updated['No_Glosa'] == 0)\n",
    "    df_move = Df_NS_NEG_updated[mask_move].copy()\n",
    "    df_move = df_move.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # 6) Quedarse en Df_NS_NEG los registros restantes\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_move].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_final)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "    print(f\"Después: DF_059_169 = {len(DF_059_169_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169)\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58464ac",
   "metadata": {},
   "source": [
    "## 🚩 GN0079 \n",
    "\n",
    "1. Afiliado Cotizante, Cabeza de familia  o Adicional, diligensia la condición de estudiante o discapacitado.\n",
    "\n",
    "   **1. Se suma un dia a la fecha de la glosa**\n",
    "   \n",
    "   2. GN0079(C|AC|01/05/2016);\n",
    "   3. GN0079(TipoAfil|EstadoAfil|FechaInicioCondicion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60eaf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando clean_gn0079 ---\n",
      "Registros iniciales en Df_NS_NEG: 122\n",
      "Registros iniciales en Df_NS_Envio: 14\n",
      "\n",
      "Registros en Df_NS_NEG después de limpieza: 121\n",
      "Registros en Df_NS_Envio después de añadir: 15\n",
      "--- Finalizando clean_gn0079 ---\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "11                     1  EPS025         RC                    1222147416   \n",
      "12                     2  EPS025         RC                    1115921009   \n",
      "13                    11  EPS025         CN                25088610286171   \n",
      "14                   145  EPS025         CC                      46379657   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "11             HOLGUIN              FAJARDO          MADISSON   \n",
      "12              CASTRO                 DIAZ            EITHAN   \n",
      "13              GALVIZ                 RIOS           HIJO DE   \n",
      "14                LARA              CAMARGO              AURA   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...         EMILY   \n",
      "1               NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2             JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                 NaN           06/06/1947     85  ...         85400   \n",
      "4                 NaN           10/03/1952     85  ...        MORENO   \n",
      "5                None           03/12/1986     85  ...     VELASQUEZ   \n",
      "6                JOEL           18/08/2025     85  ...          JUAN   \n",
      "7               ALAIA           22/08/2025     85  ...         AILYN   \n",
      "8              ZAMIRA           25/08/2025     85  ...      LUISSANA   \n",
      "9           ANTONELLA           25/09/2023     85  ...       MORALES   \n",
      "10           VICTORIA           10/05/2024     85  ...       CARREÑO   \n",
      "11              NAOMI           07/05/2025     85  ...            RC   \n",
      "12             JAVIER           27/01/2025     85  ...            RC   \n",
      "13                NaN           28/08/2025     85  ...            RC   \n",
      "14            CENAIDA           21/01/1970     85  ...             D   \n",
      "\n",
      "   COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0        JULIANA           NaN           NaN           NaN           NaN   \n",
      "1        HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2        GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4          BRITO           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6           JOEL           NaN           NaN           NaN           NaN   \n",
      "7          ALAIA           NaN           NaN           NaN           NaN   \n",
      "8         ZAMIRA           NaN           NaN           NaN           NaN   \n",
      "9         TINOCO           NaN           NaN           NaN           NaN   \n",
      "10         GOMEZ           NaN           NaN           NaN           NaN   \n",
      "11    1118583296    07/05/2025             1           NaN           NaN   \n",
      "12    1115921012    27/01/2025             1           NaN           NaN   \n",
      "13    1222147870    28/08/2025             1           NaN           NaN   \n",
      "14           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "   COD_7_NOVEDAD                              Glosa ENT_ID_ADRES  \\\n",
      "0            NaN                 GN0031(|||||||||);       EPS025   \n",
      "1            NaN                 GN0031(|||||||||);       EPS025   \n",
      "2            NaN                 GN0031(|||||||||);       EPS025   \n",
      "3            NaN               GN0035(MENDILVELSO);       EPS025   \n",
      "4            NaN                     GN0036(DIMAR);       EPS025   \n",
      "5            NaN                          GN0037();       EPS025   \n",
      "6            NaN         GN0036(JUAN);GN0037(JOEL);       EPS025   \n",
      "7            NaN       GN0036(AILYN);GN0037(ALAIA);       EPS025   \n",
      "8            NaN   GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025   \n",
      "9            NaN     GN0036(ZOE);GN0037(ANTONELLA);       EPS025   \n",
      "10           NaN  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025   \n",
      "11           NaN                            GN0059;       EPS025   \n",
      "12           NaN                            GN0059;       EPS025   \n",
      "13           NaN                            GN0059;       EPS025   \n",
      "14           NaN           GN0079(F|AC|01/02/2018);       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres  \n",
      "0                         AC  \n",
      "1                         AC  \n",
      "2                         AC  \n",
      "3                         AC  \n",
      "4                         AC  \n",
      "5                         AC  \n",
      "6                         AC  \n",
      "7                         AC  \n",
      "8                         AC  \n",
      "9                         AC  \n",
      "10                        AC  \n",
      "11                        AC  \n",
      "12                        AC  \n",
      "13                        AC  \n",
      "14                        AC  \n",
      "\n",
      "[15 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_gn0079(df_neg, df_envio):\n",
    "    \"\"\"\n",
    "    Procesa y limpia la glosa GN0079 de forma aislada y segura,\n",
    "    añadiendo los registros limpios a df_envio sin eliminar los existentes.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando clean_gn0079 ---\")\n",
    "    print(f\"Registros iniciales en Df_NS_NEG: {len(df_neg)}\")\n",
    "    print(f\"Registros iniciales en Df_NS_Envio: {len(df_envio)}\")\n",
    "    \n",
    "    # --- 1. Inicialización ---\n",
    "    # Crear copias para trabajar de forma segura sin modificar los DFs originales.\n",
    "    df_neg_updated = df_neg.copy()\n",
    "    df_envio_updated = df_envio.copy()\n",
    "\n",
    "    # --- 2. Aislar y Procesar Registros con GN0079 ---\n",
    "    # Se crea una máscara para operar SOLO sobre los registros relevantes.\n",
    "    mask = df_neg_updated['Glosa_2'].str.contains('GN0079', na=False)\n",
    "    \n",
    "    # Si no hay nada que procesar, retornar.\n",
    "    if not mask.any():\n",
    "        print(\"No se encontraron registros con la glosa GN0079.\")\n",
    "        return df_neg_updated, df_envio_updated\n",
    "\n",
    "    # Función para extraer fecha (la lógica de la fecha se mantiene como la tenías).\n",
    "    def extract_and_increment(glosa):\n",
    "        import re\n",
    "        match = re.search(r'GN0079\\([^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            fecha = datetime.strptime(match.group(1), '%d/%m/%Y')\n",
    "            nueva_fecha = fecha # + timedelta(days=1)\n",
    "            return nueva_fecha.strftime('%d/%m/%Y')\n",
    "        return None\n",
    "\n",
    "    # Se aplican los cambios SÓLO a las filas con la glosa GN0079.\n",
    "    df_neg_updated.loc[mask, 'FECHA_NOVEDAD'] = df_neg_updated.loc[mask, 'Glosa_2'].apply(extract_and_increment)\n",
    "    df_neg_updated.loc[mask, 'Glosa_2'] = df_neg_updated.loc[mask, 'Glosa_2'].str.replace(r'\\s*GN0079\\([^)]*\\);?', '', regex=True)\n",
    "    df_neg_updated.loc[mask, 'No_Glosa'] = df_neg_updated.loc[mask, 'Glosa_2'].str.count(';')\n",
    "    df_neg_updated.loc[mask, 'Code_Glosa'] = df_neg_updated.loc[mask, 'Glosa_2'].str.split(';').str[0].str[:6].replace('', '0')\n",
    "\n",
    "    # --- 3. Mover Registros Limpios ---\n",
    "    \n",
    "    # Se identifican los registros que, tras la limpieza, quedaron sin glosas.\n",
    "    mask_move = mask & (df_neg_updated['No_Glosa'] == 0)\n",
    "    \n",
    "    if mask_move.any():\n",
    "        df_to_move = df_neg_updated[mask_move].copy()\n",
    "        \n",
    "        # Se eliminan las columnas de glosas antes de mover.\n",
    "        df_to_move_clean = df_to_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "        \n",
    "        # **LA CORRECCIÓN CLAVE:** Se usa pd.concat para AÑADIR los registros.\n",
    "        df_envio_updated = pd.concat([df_envio_updated, df_to_move_clean], ignore_index=True)\n",
    "        \n",
    "        # Se eliminan de df_neg_updated los registros que ya se movieron.\n",
    "        df_neg_updated = df_neg_updated.drop(df_neg_updated[mask_move].index)\n",
    "\n",
    "    # Imprimir conteos finales\n",
    "    print(f\"\\nRegistros en Df_NS_NEG después de limpieza: {len(df_neg_updated)}\")\n",
    "    print(f\"Registros en Df_NS_Envio después de añadir: {len(df_envio_updated)}\")\n",
    "    print(f\"--- Finalizando clean_gn0079 ---\")\n",
    "\n",
    "    return df_neg_updated, df_envio_updated\n",
    "\n",
    "# Aplicar la fuNSión al DataFrame existente\n",
    "Df_NS_NEG, Df_NS_Envio = clean_gn0079(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdaed3",
   "metadata": {},
   "source": [
    "## 🚩 GN0084\n",
    "1. Fecha de novedad inferior a la fecha de afiliación a la entidad actual.\n",
    "2. GN0084(grp_fml_p|20/11/2016);  GN0084(ctz_apr|01/12/2016); GN0084(cnd_afl|01/11/2016);\n",
    "3. GN0084(FuenteValidacion|FechaInicioCondicion);\n",
    "4. La entidad debe reportar la novedad un dìa después de la fecha mostrada en la información de la glosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb101b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 121 registros\n",
      "Antes: Df_NS_Envio = 15 registros\n",
      "Después: Df_NS_NEG = 88 registros\n",
      "Después: Df_NS_Envio = 48 registros\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "11                     1  EPS025         RC                    1222147416   \n",
      "12                     2  EPS025         RC                    1115921009   \n",
      "13                    11  EPS025         CN                25088610286171   \n",
      "14                   145  EPS025         CC                      46379657   \n",
      "15                  1426  EPS025         RC                    1115869879   \n",
      "16                  1436  EPS025         TI                    1116666932   \n",
      "17                  1438  EPS025         RC                    1115868183   \n",
      "18                  1445  EPS025         RC                    1117327098   \n",
      "19                  1446  EPS025         RC                    1115867839   \n",
      "20                  1458  EPS025         TI                    1116556063   \n",
      "21                  1468  EPS025         RC                    1028703391   \n",
      "22                  1472  EPS025         RC                    1222132697   \n",
      "23                  1480  EPS025         TI                    1118650718   \n",
      "24                  1501  EPS025         TI                    1116616771   \n",
      "25                  1529  EPS025         TI                    1057919409   \n",
      "26                  1533  EPS025         TI                    1053704017   \n",
      "27                  1541  EPS025         RC                    1118651785   \n",
      "28                  1546  EPS025         RC                    1115868237   \n",
      "29                  1568  EPS025         TI                    1116552271   \n",
      "30                  1580  EPS025         TI                    1118649558   \n",
      "31                  1602  EPS025         TI                    1222127775   \n",
      "32                  1609  EPS025         TI                    1029651250   \n",
      "33                  1613  EPS025         TI                    1115865748   \n",
      "34                  1645  EPS025         TI                    1029662894   \n",
      "35                  1646  EPS025         RC                    1222145348   \n",
      "36                  1668  EPS025         RC                    1222131879   \n",
      "37                  1671  EPS025         RC                    1115870166   \n",
      "38                  1674  EPS025         RC                    1028703392   \n",
      "39                  1690  EPS025         TI                    1115859749   \n",
      "40                  1693  EPS025         TI                    1222129308   \n",
      "41                  1697  EPS025         RC                    1074535123   \n",
      "42                  1712  EPS025         TI                    1116667709   \n",
      "43                  1781  EPS025         RC                    1116044132   \n",
      "44                  1782  EPS025         CC                      23466172   \n",
      "45                  1793  EPS025         RC                    1115868164   \n",
      "46                  1819  EPS025         CC                    1118774470   \n",
      "47                  1900  EPS025         CC                    1006424081   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "11             HOLGUIN              FAJARDO          MADISSON   \n",
      "12              CASTRO                 DIAZ            EITHAN   \n",
      "13              GALVIZ                 RIOS           HIJO DE   \n",
      "14                LARA              CAMARGO              AURA   \n",
      "15            LIZARAZO              PARALES           CRISTAL   \n",
      "16               SIGUA                TAPON              YEIS   \n",
      "17           REQUINIVA              DELGADO            JUNIOR   \n",
      "18         GUACARAPARE               PONARE         YORDAIRON   \n",
      "19              GARCIA                NAVAS             ANGEL   \n",
      "20           RODRIGUEZ              GALINDO             DASLY   \n",
      "21             GUEVARA              MONTAÑA           ZHARICK   \n",
      "22            MARTINEZ               QUIVAI            YICELL   \n",
      "23           GUALTEROS              CORDOBA            YOSHUA   \n",
      "24             CACERES              SERRANO            MAUREN   \n",
      "25            MARTINEZ                  ROA             MAURY   \n",
      "26            MARTINEZ                  ROA              JUAN   \n",
      "27               PINTO            GUALTEROS             KELLY   \n",
      "28           ATEHORTUA             GUALDRON             DILAN   \n",
      "29           RODRIGUEZ              GALINDO            MIGUEL   \n",
      "30            BASTILLA                  NaN             LEDIS   \n",
      "31              GARCIA                NAVAS              JUAN   \n",
      "32              CHACON                SILVA            ANDRES   \n",
      "33           ATEHORTUA             GUALDRON             WENDY   \n",
      "34            GARAVITO               MARIÑO             CAREN   \n",
      "35             MORALES             QUIÑONEZ              LIAM   \n",
      "36               SIGUA                TAPON              AXEL   \n",
      "37             NARANJO              HIGUERA             ERICK   \n",
      "38             GUEVARA              MONTAÑA             LUISA   \n",
      "39             DELGADO               CUEVAS             KEVIN   \n",
      "40             BARRERO               MORENO            ELIANA   \n",
      "41             NARANJO              HIGUERA             WENDY   \n",
      "42               SIGUA                TAPON             DANNA   \n",
      "43               CONDE                LOPEZ             MARIA   \n",
      "44             RIVEROS           DE SABOGAL             MARIA   \n",
      "45             JIMENEZ            RODRIGUEZ            RAFAEL   \n",
      "46              GARCIA                  NaN            SHAROL   \n",
      "47               LOPEZ                 ADAN             JEIMY   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...         EMILY   \n",
      "1               NAOMI           07/05/2025     85  ...         PESCA   \n",
      "2             JULIANA           28/08/2025     85  ...       MARQUEZ   \n",
      "3                 NaN           06/06/1947     85  ...         85400   \n",
      "4                 NaN           10/03/1952     85  ...        MORENO   \n",
      "5                None           03/12/1986     85  ...     VELASQUEZ   \n",
      "6                JOEL           18/08/2025     85  ...          JUAN   \n",
      "7               ALAIA           22/08/2025     85  ...         AILYN   \n",
      "8              ZAMIRA           25/08/2025     85  ...      LUISSANA   \n",
      "9           ANTONELLA           25/09/2023     85  ...       MORALES   \n",
      "10           VICTORIA           10/05/2024     85  ...       CARREÑO   \n",
      "11              NAOMI           07/05/2025     85  ...            RC   \n",
      "12             JAVIER           27/01/2025     85  ...            RC   \n",
      "13                NaN           28/08/2025     85  ...            RC   \n",
      "14            CENAIDA           21/01/1970     85  ...             D   \n",
      "15            SHADDAY           18/12/2021     85  ...  852500042203   \n",
      "16          ALEJANDRO           06/05/2013     85  ...  854300042206   \n",
      "17          SEBASTIAN           31/03/2019     85  ...  852500042203   \n",
      "18                NaN           14/03/2022     85  ...  852300042209   \n",
      "19              URIEL           25/07/2018     85  ...  851250042210   \n",
      "20             CAMILA           14/02/2018     85  ...  850100019001   \n",
      "21            ANTONIA           10/08/2022     85  ...  852500042203   \n",
      "22              NAOMI           02/11/2018     85  ...  850010014401   \n",
      "23          ALEJANDRO           18/04/2016     85  ...  851250042210   \n",
      "24            MILDREY           22/06/2017     85  ...  851390042204   \n",
      "25             NORELY           05/04/2013     85  ...  854400042202   \n",
      "26              DAVID           26/11/2010     85  ...  854400042202   \n",
      "27            DALIANA           03/09/2023     85  ...  851250042210   \n",
      "28              DAVID           02/05/2019     85  ...  852500042203   \n",
      "29              ANGEL           04/10/2014     85  ...  850100019001   \n",
      "30            YHURANI           22/02/2011     85  ...  851250042210   \n",
      "31            ESTEBAN           20/12/2016     85  ...  851250042210   \n",
      "32             FELIPE           19/02/2009     85  ...  850100019001   \n",
      "33              NICOL           05/08/2016     85  ...  852500042203   \n",
      "34             DURLEY           19/08/2007     85  ...  851390042204   \n",
      "35             CAMILO           15/11/2023     85  ...  853150042214   \n",
      "36             MATIAS           07/08/2018     85  ...  854300042206   \n",
      "37              MATEO           18/07/2022     85  ...  852500042203   \n",
      "38          ALEXANDRA           10/08/2022     85  ...  852500042203   \n",
      "39             ANDRES           15/12/2011     85  ...  852500042203   \n",
      "40           ISABELLA           26/05/2017     85  ...  854400042202   \n",
      "41            XIOMARA           31/08/2019     85  ...  852500042203   \n",
      "42             YARITH           07/01/2015     85  ...  854300042206   \n",
      "43               JOSE           20/02/2020     85  ...             0   \n",
      "44           DE JESUS           16/03/1926     85  ...             0   \n",
      "45           GILDARDO           09/03/2019     85  ...             0   \n",
      "46             JINETH           23/01/2007     85  ...             2   \n",
      "47              ZULAY           12/01/2001     85  ...             2   \n",
      "\n",
      "   COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0        JULIANA           NaN           NaN           NaN           NaN   \n",
      "1        HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "2        GALVIZ            NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4          BRITO           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6           JOEL           NaN           NaN           NaN           NaN   \n",
      "7          ALAIA           NaN           NaN           NaN           NaN   \n",
      "8         ZAMIRA           NaN           NaN           NaN           NaN   \n",
      "9         TINOCO           NaN           NaN           NaN           NaN   \n",
      "10         GOMEZ           NaN           NaN           NaN           NaN   \n",
      "11    1118583296    07/05/2025             1           NaN           NaN   \n",
      "12    1115921012    27/01/2025             1           NaN           NaN   \n",
      "13    1222147870    28/08/2025             1           NaN           NaN   \n",
      "14           NaN           NaN           NaN           NaN           NaN   \n",
      "15  852500042203           NaN           NaN           NaN           NaN   \n",
      "16  854300042206           NaN           NaN           NaN           NaN   \n",
      "17  852500042203           NaN           NaN           NaN           NaN   \n",
      "18  852300042209           NaN           NaN           NaN           NaN   \n",
      "19  851250042210           NaN           NaN           NaN           NaN   \n",
      "20  850100019001           NaN           NaN           NaN           NaN   \n",
      "21  852500042203           NaN           NaN           NaN           NaN   \n",
      "22  850010014401           NaN           NaN           NaN           NaN   \n",
      "23  851250042210           NaN           NaN           NaN           NaN   \n",
      "24  851390042204           NaN           NaN           NaN           NaN   \n",
      "25  854400042202           NaN           NaN           NaN           NaN   \n",
      "26  854400042202           NaN           NaN           NaN           NaN   \n",
      "27  851250042210           NaN           NaN           NaN           NaN   \n",
      "28  852500042203           NaN           NaN           NaN           NaN   \n",
      "29  850100019001           NaN           NaN           NaN           NaN   \n",
      "30  851250042210           NaN           NaN           NaN           NaN   \n",
      "31  851250042210           NaN           NaN           NaN           NaN   \n",
      "32  850100019001           NaN           NaN           NaN           NaN   \n",
      "33  852500042203           NaN           NaN           NaN           NaN   \n",
      "34  851390042204           NaN           NaN           NaN           NaN   \n",
      "35  853150042214           NaN           NaN           NaN           NaN   \n",
      "36  854300042206           NaN           NaN           NaN           NaN   \n",
      "37  852500042203           NaN           NaN           NaN           NaN   \n",
      "38  852500042203           NaN           NaN           NaN           NaN   \n",
      "39  852500042203           NaN           NaN           NaN           NaN   \n",
      "40  854400042202           NaN           NaN           NaN           NaN   \n",
      "41  852500042203           NaN           NaN           NaN           NaN   \n",
      "42  854300042206           NaN           NaN           NaN           NaN   \n",
      "43            11           001           NaN           NaN           NaN   \n",
      "44            85           001           NaN           NaN           NaN   \n",
      "45            85           263           NaN           NaN           NaN   \n",
      "46            85           001           NaN           NaN           NaN   \n",
      "47            73           268           NaN           NaN           NaN   \n",
      "\n",
      "   COD_7_NOVEDAD                              Glosa ENT_ID_ADRES  \\\n",
      "0            NaN                 GN0031(|||||||||);       EPS025   \n",
      "1            NaN                 GN0031(|||||||||);       EPS025   \n",
      "2            NaN                 GN0031(|||||||||);       EPS025   \n",
      "3            NaN               GN0035(MENDILVELSO);       EPS025   \n",
      "4            NaN                     GN0036(DIMAR);       EPS025   \n",
      "5            NaN                          GN0037();       EPS025   \n",
      "6            NaN         GN0036(JUAN);GN0037(JOEL);       EPS025   \n",
      "7            NaN       GN0036(AILYN);GN0037(ALAIA);       EPS025   \n",
      "8            NaN   GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025   \n",
      "9            NaN     GN0036(ZOE);GN0037(ANTONELLA);       EPS025   \n",
      "10           NaN  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025   \n",
      "11           NaN                            GN0059;       EPS025   \n",
      "12           NaN                            GN0059;       EPS025   \n",
      "13           NaN                            GN0059;       EPS025   \n",
      "14           NaN           GN0079(F|AC|01/02/2018);       EPS025   \n",
      "15           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "16           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "17           NaN        GN0084(cnd_afl|18/09/2025);       EPS025   \n",
      "18           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "19           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "20           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "21           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "22           NaN        GN0084(cnd_afl|15/09/2025);       EPS025   \n",
      "23           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "24           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "25           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "26           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "27           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "28           NaN        GN0084(cnd_afl|01/11/2025);       EPS025   \n",
      "29           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "30           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "31           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "32           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "33           NaN        GN0084(cnd_afl|01/11/2025);       EPS025   \n",
      "34           NaN        GN0084(cnd_afl|30/09/2025);       EPS025   \n",
      "35           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "36           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "37           NaN        GN0084(cnd_afl|20/09/2025);       EPS025   \n",
      "38           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "39           NaN        GN0084(cnd_afl|18/09/2025);       EPS025   \n",
      "40           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "41           NaN        GN0084(cnd_afl|20/09/2025);       EPS025   \n",
      "42           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "43           NaN        GN0084(cnd_afl|01/10/2025);       EPS025   \n",
      "44           NaN        GN0084(cnd_afl|12/09/2025);       EPS025   \n",
      "45           NaN        GN0084(cnd_afl|12/09/2025);       EPS025   \n",
      "46           NaN        GN0084(cnd_afl|12/09/2025);       EPS025   \n",
      "47           NaN        GN0084(cnd_afl|12/09/2025);       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres  \n",
      "0                         AC  \n",
      "1                         AC  \n",
      "2                         AC  \n",
      "3                         AC  \n",
      "4                         AC  \n",
      "5                         AC  \n",
      "6                         AC  \n",
      "7                         AC  \n",
      "8                         AC  \n",
      "9                         AC  \n",
      "10                        AC  \n",
      "11                        AC  \n",
      "12                        AC  \n",
      "13                        AC  \n",
      "14                        AC  \n",
      "15                        AC  \n",
      "16                        AC  \n",
      "17                        AC  \n",
      "18                        AC  \n",
      "19                        AC  \n",
      "20                        AC  \n",
      "21                        AC  \n",
      "22                        AC  \n",
      "23                        AC  \n",
      "24                        AC  \n",
      "25                        AC  \n",
      "26                        AC  \n",
      "27                        AC  \n",
      "28                        AC  \n",
      "29                        AC  \n",
      "30                        AC  \n",
      "31                        AC  \n",
      "32                        AC  \n",
      "33                        AC  \n",
      "34                        AC  \n",
      "35                        AC  \n",
      "36                        AC  \n",
      "37                        AC  \n",
      "38                        AC  \n",
      "39                        AC  \n",
      "40                        AC  \n",
      "41                        AC  \n",
      "42                        AC  \n",
      "43                        AC  \n",
      "44                        AC  \n",
      "45                        AC  \n",
      "46                        AC  \n",
      "47                        AC  \n",
      "\n",
      "[48 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0084(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0084 en Df_NS_NEG:\n",
    "      - Extrae la fecha de la glosa GN0084(FuenteValidacion|FechaInicioCondicion);\n",
    "      - Actualiza FECHA_NOVEDAD a FechaInicioCondicion + 1 día;\n",
    "      - Elimina la glosa GN0084(...) de Glosa_2;\n",
    "      - Recalcula Code_Glosa y No_Glosa;\n",
    "      - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "      - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0084\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0084\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer fecha y actualizar FECHA_NOVEDAD\n",
    "    def extract_and_increment(glosa):\n",
    "        match = re.search(r'GN0084\\([^\\|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            fecha = datetime.strptime(match.group(1), '%d/%m/%Y')\n",
    "            nueva_fecha = fecha + timedelta(days=1)\n",
    "            return nueva_fecha.strftime('%d/%m/%Y')\n",
    "        return None\n",
    "\n",
    "    df_gn['FECHA_NOVEDAD'] = df_gn['Glosa_2'].apply(extract_and_increment)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0084(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0084\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0084(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321422",
   "metadata": {},
   "source": [
    "## 🚩 GN0088\n",
    "1. Estado actual no valido para la novedad o igual al registrado en BDUA.\n",
    "2. GN0088(F|RE|01/12/2015)\n",
    "3. GN0088(TipoAfiliado|Estadp|FechaInicioCondicion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70c7848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 88\n",
      "Antes: Df_NS_Envio = 48\n",
      "Antes: DF_No_Enviar = 29\n",
      "Después: Df_NS_NEG = 88\n",
      "Después: Df_NS_Envio = 48\n",
      "Después: DF_No_Enviar = 29\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "5                     89  EPS025         CC                    1118533451   \n",
      "6                     50  EPS025         RC                    1206231438   \n",
      "7                     53  EPS025         RC                    1118583499   \n",
      "8                     54  EPS025         RC                    1222147836   \n",
      "9                     83  EPS025         RC                    1242693158   \n",
      "10                    92  EPS025         RC                    1118582358   \n",
      "11                     1  EPS025         RC                    1222147416   \n",
      "12                     2  EPS025         RC                    1115921009   \n",
      "13                    11  EPS025         CN                25088610286171   \n",
      "14                   145  EPS025         CC                      46379657   \n",
      "15                  1426  EPS025         RC                    1115869879   \n",
      "16                  1436  EPS025         TI                    1116666932   \n",
      "17                  1438  EPS025         RC                    1115868183   \n",
      "18                  1445  EPS025         RC                    1117327098   \n",
      "19                  1446  EPS025         RC                    1115867839   \n",
      "20                  1458  EPS025         TI                    1116556063   \n",
      "21                  1468  EPS025         RC                    1028703391   \n",
      "22                  1472  EPS025         RC                    1222132697   \n",
      "23                  1480  EPS025         TI                    1118650718   \n",
      "24                  1501  EPS025         TI                    1116616771   \n",
      "25                  1529  EPS025         TI                    1057919409   \n",
      "26                  1533  EPS025         TI                    1053704017   \n",
      "27                  1541  EPS025         RC                    1118651785   \n",
      "28                  1546  EPS025         RC                    1115868237   \n",
      "29                  1568  EPS025         TI                    1116552271   \n",
      "30                  1580  EPS025         TI                    1118649558   \n",
      "31                  1602  EPS025         TI                    1222127775   \n",
      "32                  1609  EPS025         TI                    1029651250   \n",
      "33                  1613  EPS025         TI                    1115865748   \n",
      "34                  1645  EPS025         TI                    1029662894   \n",
      "35                  1646  EPS025         RC                    1222145348   \n",
      "36                  1668  EPS025         RC                    1222131879   \n",
      "37                  1671  EPS025         RC                    1115870166   \n",
      "38                  1674  EPS025         RC                    1028703392   \n",
      "39                  1690  EPS025         TI                    1115859749   \n",
      "40                  1693  EPS025         TI                    1222129308   \n",
      "41                  1697  EPS025         RC                    1074535123   \n",
      "42                  1712  EPS025         TI                    1116667709   \n",
      "43                  1781  EPS025         RC                    1116044132   \n",
      "44                  1782  EPS025         CC                      23466172   \n",
      "45                  1793  EPS025         RC                    1115868164   \n",
      "46                  1819  EPS025         CC                    1118774470   \n",
      "47                  1900  EPS025         CC                    1006424081   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "5              TARACHE            VELASQUEZ            JHOANA   \n",
      "6               CACHAY                 CRUZ              JUAN   \n",
      "7            DOMINGUEZ               OSPINA             AILYN   \n",
      "8                 LEON              HURTADO          LUISSANA   \n",
      "9               TINOCO               MORENO               ZOE   \n",
      "10                CRUZ                ABRIL           CARLOTA   \n",
      "11             HOLGUIN              FAJARDO          MADISSON   \n",
      "12              CASTRO                 DIAZ            EITHAN   \n",
      "13              GALVIZ                 RIOS           HIJO DE   \n",
      "14                LARA              CAMARGO              AURA   \n",
      "15            LIZARAZO              PARALES           CRISTAL   \n",
      "16               SIGUA                TAPON              YEIS   \n",
      "17           REQUINIVA              DELGADO            JUNIOR   \n",
      "18         GUACARAPARE               PONARE         YORDAIRON   \n",
      "19              GARCIA                NAVAS             ANGEL   \n",
      "20           RODRIGUEZ              GALINDO             DASLY   \n",
      "21             GUEVARA              MONTAÑA           ZHARICK   \n",
      "22            MARTINEZ               QUIVAI            YICELL   \n",
      "23           GUALTEROS              CORDOBA            YOSHUA   \n",
      "24             CACERES              SERRANO            MAUREN   \n",
      "25            MARTINEZ                  ROA             MAURY   \n",
      "26            MARTINEZ                  ROA              JUAN   \n",
      "27               PINTO            GUALTEROS             KELLY   \n",
      "28           ATEHORTUA             GUALDRON             DILAN   \n",
      "29           RODRIGUEZ              GALINDO            MIGUEL   \n",
      "30            BASTILLA                  NaN             LEDIS   \n",
      "31              GARCIA                NAVAS              JUAN   \n",
      "32              CHACON                SILVA            ANDRES   \n",
      "33           ATEHORTUA             GUALDRON             WENDY   \n",
      "34            GARAVITO               MARIÑO             CAREN   \n",
      "35             MORALES             QUIÑONEZ              LIAM   \n",
      "36               SIGUA                TAPON              AXEL   \n",
      "37             NARANJO              HIGUERA             ERICK   \n",
      "38             GUEVARA              MONTAÑA             LUISA   \n",
      "39             DELGADO               CUEVAS             KEVIN   \n",
      "40             BARRERO               MORENO            ELIANA   \n",
      "41             NARANJO              HIGUERA             WENDY   \n",
      "42               SIGUA                TAPON             DANNA   \n",
      "43               CONDE                LOPEZ             MARIA   \n",
      "44             RIVEROS           DE SABOGAL             MARIA   \n",
      "45             JIMENEZ            RODRIGUEZ            RAFAEL   \n",
      "46              GARCIA                  NaN            SHAROL   \n",
      "47               LOPEZ                 ADAN             JEIMY   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_2_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...       JULIANA   \n",
      "1               NAOMI           07/05/2025     85  ...       HOLGUIN   \n",
      "2             JULIANA           28/08/2025     85  ...       GALVIZ    \n",
      "3                 NaN           06/06/1947     85  ...           NaN   \n",
      "4                 NaN           10/03/1952     85  ...         BRITO   \n",
      "5                None           03/12/1986     85  ...           NaN   \n",
      "6                JOEL           18/08/2025     85  ...          JOEL   \n",
      "7               ALAIA           22/08/2025     85  ...         ALAIA   \n",
      "8              ZAMIRA           25/08/2025     85  ...        ZAMIRA   \n",
      "9           ANTONELLA           25/09/2023     85  ...        TINOCO   \n",
      "10           VICTORIA           10/05/2024     85  ...         GOMEZ   \n",
      "11              NAOMI           07/05/2025     85  ...    1118583296   \n",
      "12             JAVIER           27/01/2025     85  ...    1115921012   \n",
      "13                NaN           28/08/2025     85  ...    1222147870   \n",
      "14            CENAIDA           21/01/1970     85  ...           NaN   \n",
      "15            SHADDAY           18/12/2021     85  ...  852500042203   \n",
      "16          ALEJANDRO           06/05/2013     85  ...  854300042206   \n",
      "17          SEBASTIAN           31/03/2019     85  ...  852500042203   \n",
      "18                NaN           14/03/2022     85  ...  852300042209   \n",
      "19              URIEL           25/07/2018     85  ...  851250042210   \n",
      "20             CAMILA           14/02/2018     85  ...  850100019001   \n",
      "21            ANTONIA           10/08/2022     85  ...  852500042203   \n",
      "22              NAOMI           02/11/2018     85  ...  850010014401   \n",
      "23          ALEJANDRO           18/04/2016     85  ...  851250042210   \n",
      "24            MILDREY           22/06/2017     85  ...  851390042204   \n",
      "25             NORELY           05/04/2013     85  ...  854400042202   \n",
      "26              DAVID           26/11/2010     85  ...  854400042202   \n",
      "27            DALIANA           03/09/2023     85  ...  851250042210   \n",
      "28              DAVID           02/05/2019     85  ...  852500042203   \n",
      "29              ANGEL           04/10/2014     85  ...  850100019001   \n",
      "30            YHURANI           22/02/2011     85  ...  851250042210   \n",
      "31            ESTEBAN           20/12/2016     85  ...  851250042210   \n",
      "32             FELIPE           19/02/2009     85  ...  850100019001   \n",
      "33              NICOL           05/08/2016     85  ...  852500042203   \n",
      "34             DURLEY           19/08/2007     85  ...  851390042204   \n",
      "35             CAMILO           15/11/2023     85  ...  853150042214   \n",
      "36             MATIAS           07/08/2018     85  ...  854300042206   \n",
      "37              MATEO           18/07/2022     85  ...  852500042203   \n",
      "38          ALEXANDRA           10/08/2022     85  ...  852500042203   \n",
      "39             ANDRES           15/12/2011     85  ...  852500042203   \n",
      "40           ISABELLA           26/05/2017     85  ...  854400042202   \n",
      "41            XIOMARA           31/08/2019     85  ...  852500042203   \n",
      "42             YARITH           07/01/2015     85  ...  854300042206   \n",
      "43               JOSE           20/02/2020     85  ...            11   \n",
      "44           DE JESUS           16/03/1926     85  ...            85   \n",
      "45           GILDARDO           09/03/2019     85  ...            85   \n",
      "46             JINETH           23/01/2007     85  ...            85   \n",
      "47              ZULAY           12/01/2001     85  ...            73   \n",
      "\n",
      "   COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD COD_7_NOVEDAD  \\\n",
      "0            NaN           NaN           NaN           NaN           NaN   \n",
      "1            NaN           NaN           NaN           NaN           NaN   \n",
      "2            NaN           NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4            NaN           NaN           NaN           NaN           NaN   \n",
      "5            NaN           NaN           NaN           NaN           NaN   \n",
      "6            NaN           NaN           NaN           NaN           NaN   \n",
      "7            NaN           NaN           NaN           NaN           NaN   \n",
      "8            NaN           NaN           NaN           NaN           NaN   \n",
      "9            NaN           NaN           NaN           NaN           NaN   \n",
      "10           NaN           NaN           NaN           NaN           NaN   \n",
      "11    07/05/2025             1           NaN           NaN           NaN   \n",
      "12    27/01/2025             1           NaN           NaN           NaN   \n",
      "13    28/08/2025             1           NaN           NaN           NaN   \n",
      "14           NaN           NaN           NaN           NaN           NaN   \n",
      "15           NaN           NaN           NaN           NaN           NaN   \n",
      "16           NaN           NaN           NaN           NaN           NaN   \n",
      "17           NaN           NaN           NaN           NaN           NaN   \n",
      "18           NaN           NaN           NaN           NaN           NaN   \n",
      "19           NaN           NaN           NaN           NaN           NaN   \n",
      "20           NaN           NaN           NaN           NaN           NaN   \n",
      "21           NaN           NaN           NaN           NaN           NaN   \n",
      "22           NaN           NaN           NaN           NaN           NaN   \n",
      "23           NaN           NaN           NaN           NaN           NaN   \n",
      "24           NaN           NaN           NaN           NaN           NaN   \n",
      "25           NaN           NaN           NaN           NaN           NaN   \n",
      "26           NaN           NaN           NaN           NaN           NaN   \n",
      "27           NaN           NaN           NaN           NaN           NaN   \n",
      "28           NaN           NaN           NaN           NaN           NaN   \n",
      "29           NaN           NaN           NaN           NaN           NaN   \n",
      "30           NaN           NaN           NaN           NaN           NaN   \n",
      "31           NaN           NaN           NaN           NaN           NaN   \n",
      "32           NaN           NaN           NaN           NaN           NaN   \n",
      "33           NaN           NaN           NaN           NaN           NaN   \n",
      "34           NaN           NaN           NaN           NaN           NaN   \n",
      "35           NaN           NaN           NaN           NaN           NaN   \n",
      "36           NaN           NaN           NaN           NaN           NaN   \n",
      "37           NaN           NaN           NaN           NaN           NaN   \n",
      "38           NaN           NaN           NaN           NaN           NaN   \n",
      "39           NaN           NaN           NaN           NaN           NaN   \n",
      "40           NaN           NaN           NaN           NaN           NaN   \n",
      "41           NaN           NaN           NaN           NaN           NaN   \n",
      "42           NaN           NaN           NaN           NaN           NaN   \n",
      "43           001           NaN           NaN           NaN           NaN   \n",
      "44           001           NaN           NaN           NaN           NaN   \n",
      "45           263           NaN           NaN           NaN           NaN   \n",
      "46           001           NaN           NaN           NaN           NaN   \n",
      "47           268           NaN           NaN           NaN           NaN   \n",
      "\n",
      "                                Glosa ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \\\n",
      "0                  GN0031(|||||||||);       EPS025                        AC   \n",
      "1                  GN0031(|||||||||);       EPS025                        AC   \n",
      "2                  GN0031(|||||||||);       EPS025                        AC   \n",
      "3                GN0035(MENDILVELSO);       EPS025                        AC   \n",
      "4                      GN0036(DIMAR);       EPS025                        AC   \n",
      "5                           GN0037();       EPS025                        AC   \n",
      "6          GN0036(JUAN);GN0037(JOEL);       EPS025                        AC   \n",
      "7        GN0036(AILYN);GN0037(ALAIA);       EPS025                        AC   \n",
      "8    GN0036(LUISSANA);GN0037(ZAMIRA);       EPS025                        AC   \n",
      "9      GN0036(ZOE);GN0037(ANTONELLA);       EPS025                        AC   \n",
      "10  GN0036(CARLOTA);GN0037(VICTORIA);       EPS025                        AC   \n",
      "11                            GN0059;       EPS025                        AC   \n",
      "12                            GN0059;       EPS025                        AC   \n",
      "13                            GN0059;       EPS025                        AC   \n",
      "14           GN0079(F|AC|01/02/2018);       EPS025                        AC   \n",
      "15        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "16        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "17        GN0084(cnd_afl|18/09/2025);       EPS025                        AC   \n",
      "18        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "19        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "20        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "21        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "22        GN0084(cnd_afl|15/09/2025);       EPS025                        AC   \n",
      "23        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "24        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "25        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "26        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "27        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "28        GN0084(cnd_afl|01/11/2025);       EPS025                        AC   \n",
      "29        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "30        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "31        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "32        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "33        GN0084(cnd_afl|01/11/2025);       EPS025                        AC   \n",
      "34        GN0084(cnd_afl|30/09/2025);       EPS025                        AC   \n",
      "35        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "36        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "37        GN0084(cnd_afl|20/09/2025);       EPS025                        AC   \n",
      "38        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "39        GN0084(cnd_afl|18/09/2025);       EPS025                        AC   \n",
      "40        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "41        GN0084(cnd_afl|20/09/2025);       EPS025                        AC   \n",
      "42        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "43        GN0084(cnd_afl|01/10/2025);       EPS025                        AC   \n",
      "44        GN0084(cnd_afl|12/09/2025);       EPS025                        AC   \n",
      "45        GN0084(cnd_afl|12/09/2025);       EPS025                        AC   \n",
      "46        GN0084(cnd_afl|12/09/2025);       EPS025                        AC   \n",
      "47        GN0084(cnd_afl|12/09/2025);       EPS025                        AC   \n",
      "\n",
      "   Where  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN  \n",
      "5    NaN  \n",
      "6    NaN  \n",
      "7    NaN  \n",
      "8    NaN  \n",
      "9    NaN  \n",
      "10   NaN  \n",
      "11   NaN  \n",
      "12   NaN  \n",
      "13   NaN  \n",
      "14   NaN  \n",
      "15   NaN  \n",
      "16   NaN  \n",
      "17   NaN  \n",
      "18   NaN  \n",
      "19   NaN  \n",
      "20   NaN  \n",
      "21   NaN  \n",
      "22   NaN  \n",
      "23   NaN  \n",
      "24   NaN  \n",
      "25   NaN  \n",
      "26   NaN  \n",
      "27   NaN  \n",
      "28   NaN  \n",
      "29   NaN  \n",
      "30   NaN  \n",
      "31   NaN  \n",
      "32   NaN  \n",
      "33   NaN  \n",
      "34   NaN  \n",
      "35   NaN  \n",
      "36   NaN  \n",
      "37   NaN  \n",
      "38   NaN  \n",
      "39   NaN  \n",
      "40   NaN  \n",
      "41   NaN  \n",
      "42   NaN  \n",
      "43   NaN  \n",
      "44   NaN  \n",
      "45   NaN  \n",
      "46   NaN  \n",
      "47   NaN  \n",
      "\n",
      "[48 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0088(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0088 (“Estado actual no válido para la novedad o igual al registrado en BDUA”).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0088('.\n",
    "      2. Copia esos registros a `DF_No_Enviar` con el motivo \"Estado actual no válido para la novedad o igual al registrado en BDUA.\"\n",
    "      3. Elimina la subcadena 'GN0088(...)' de `Glosa_2`.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vacío.\n",
    "      5. Mueve a `Df_NS_Envio` todos los registros de `Df_NS_NEG` con `No_Glosa == 0`\n",
    "         (ya no tienen glosas) y asigna \"Glosas\" en la columna `Where`.\n",
    "      6. Imprime los conteos de registros antes y después en cada DataFrame.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para envío sin glosas.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para registros no enviados.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros sin mover (glosas aún pendientes).\n",
    "            - Df_NS_Envio_updated (pd.DataFrame): Registros ya sin glosas listos para envío.\n",
    "            - DF_No_Enviar_updated (pd.DataFrame): Registros movidos con motivo.\n",
    "    \"\"\"\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0088\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0088\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Copiar registros a DF_No_Enviar con el motivo\n",
    "    df_gn['Motivo'] = \"Estado actual no válido para la novedad o igual al registrado en BDUA.\"\n",
    "    df_no_enviar = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_no_enviar], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0088(...)' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace(r'GN0088\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = Df_NS_NEG_updated['No_Glosa'] == 0\n",
    "    df_move = Df_NS_NEG_updated[mask_zero].copy()\n",
    "    df_move['Where'] = \"Glosas\"\n",
    "    df_move = df_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # Quedarse en Df_NS_NEG sólo los que aún tienen glosas\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_zero].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Uso de ejemplo:\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0088(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar)\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595d9cb",
   "metadata": {},
   "source": [
    "## 🚩 GN0122\n",
    "1. Grupo poblacional reportado, igual al registrado en la BDUA.\n",
    "2. GN0122(ST|1|5|2|B02|28/03/2022);\n",
    "3. GN0122(ModalidadSubsidio|NivelSisben|GrupoPoblacional|Metodologia|SubgrupoPoblacional|InicioCondicionAfiliacion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10653fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 88\n",
      "Antes: DF_No_Enviar = 29\n",
      "Después: Df_NS_NEG = 86\n",
      "Después: DF_No_Enviar = 31\n"
     ]
    }
   ],
   "source": [
    "def process_gn0122(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0122 (“Grupo poblacional reportado, igual al registrado en la BDUA.”).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0122('.\n",
    "      2. Copia esos registros a `DF_No_Enviar` con el motivo correspondiente.\n",
    "      3. Elimina la subcadena 'GN0122(...)' de Glosa_2.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vacío.\n",
    "      5. Devuelve los tres DataFrames actualizados.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para registros no enviados.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, DF_No_Enviar_updated)\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0122\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0122\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Copiar registros a DF_No_Enviar con el motivo\n",
    "    df_gn['Motivo'] = \"Grupo poblacional reportado, igual al registrado en la BDUA.\"\n",
    "    df_no_enviar = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_no_enviar], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0122(...)' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace(r'GN0122\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Quedarse en Df_NS_NEG sólo los que aún tienen glosas\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask].copy()\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, DF_No_Enviar_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0122(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2975f6f",
   "metadata": {},
   "source": [
    "## 🚩 GN0169\n",
    "1. Los datos del afiliado enviados no coinciden con los datos certificados por la RNEC.\n",
    "2. GN0169(|SEGUNDO APELLIDO|URDANETA|USDANETA||SEGUNDO NOMBRE|MANUEL||);\n",
    "   1. GN0169(|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad||SEGUNDO NOMBRE|DatoTablaReferencia|DatoEnviadoEntidad|);\n",
    "3. GN0169(PRIMER APELLIDO|GONZALEZ|AMAYA|SEGUNDO APELLIDO|PUSHAINA|GUERRA|||);\n",
    "   1. GN0169(PRIMER APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|||);\n",
    "4. GN0169(||||||);\n",
    "   1. GN0169(DocumentoNoExisteTablaReferencia); \n",
    "5. GN0169(||||FECHA NACIMIENTO|19/07/2016|01/07/2016);\n",
    "6. GN0169(||||SEXO|F|M);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81aab393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando process_gn0169 ---\n",
      "Antes: Df_NS_NEG = 86\n",
      "Antes: Df_NS_Envio = 48\n",
      "Antes: DF_059_169 = 15\n",
      "\n",
      "Después: Df_NS_NEG = 51\n",
      "Después: Df_NS_Envio = 83\n",
      "Después: DF_059_169 = 39\n",
      "   NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                     55  EPS025         RC                    1222147870   \n",
      "1                     69  EPS025         RC                    1118583296   \n",
      "2                     80  EPS025         RC                    1222147870   \n",
      "3                   1937  EPS025         CC                       4270683   \n",
      "4                     91  EPS025         CC                       6671964   \n",
      "..                   ...     ...        ...                           ...   \n",
      "78                    76  EPS025         RC                    1222147416   \n",
      "79                    79  EPS025         RC                    1222147836   \n",
      "80                    86  EPS025         PT                       5910481   \n",
      "81                    88  EPS025         TI                    1050611850   \n",
      "82                    90  EPS025         PT                       5926892   \n",
      "\n",
      "   AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               GALVIZ                 RIOS           HIJO DE   \n",
      "1                NAOMI              FAJARDO          MADISSON   \n",
      "2               GALVIZ                 RIOS             EMILY   \n",
      "3               ESTEPA          MENDILVELSO          NORBERTO   \n",
      "4            RODRIGUEZ                BRITO             DIMAR   \n",
      "..                 ...                  ...               ...   \n",
      "78             HOLGUIN              FAJARDO          MADISSON   \n",
      "79                LEON              HURTADO          LUISSANA   \n",
      "80              SUAREZ                GIRON           DELIMAR   \n",
      "81             MAYORGA             BERMUDES            SAMUEL   \n",
      "82             SMILIER            RODRIGUEZ           YOISBER   \n",
      "\n",
      "   AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_2_NOVEDAD  \\\n",
      "0                 NaN           28/08/2025     85  ...       JULIANA   \n",
      "1               NAOMI           07/05/2025     85  ...       HOLGUIN   \n",
      "2             JULIANA           28/08/2025     85  ...       GALVIZ    \n",
      "3                 NaN           06/06/1947     85  ...           NaN   \n",
      "4                 NaN           10/03/1952     85  ...         BRITO   \n",
      "..                ...                  ...    ...  ...           ...   \n",
      "78              NAOMI           07/05/2025     85  ...       HOLGUIN   \n",
      "79             ZAMIRA           25/08/2025     85  ...          LEON   \n",
      "80     DE LOS ANGELES           15/03/1998     85  ...        SUAREZ   \n",
      "81                NaN           31/12/2009     85  ...           NaN   \n",
      "82             FABIAN           11/10/2014     85  ...       JIMENEZ   \n",
      "\n",
      "   COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD COD_7_NOVEDAD  \\\n",
      "0            NaN           NaN           NaN           NaN           NaN   \n",
      "1            NaN           NaN           NaN           NaN           NaN   \n",
      "2            NaN           NaN           NaN           NaN           NaN   \n",
      "3            NaN           NaN           NaN           NaN           NaN   \n",
      "4            NaN           NaN           NaN           NaN           NaN   \n",
      "..           ...           ...           ...           ...           ...   \n",
      "78           NaN           NaN           NaN           NaN           NaN   \n",
      "79           NaN           NaN           NaN           NaN           NaN   \n",
      "80           NaN           NaN           NaN           NaN           NaN   \n",
      "81           NaN           NaN           NaN           NaN           NaN   \n",
      "82           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "                                                Glosa ENT_ID_ADRES  \\\n",
      "0                                  GN0031(|||||||||);       EPS025   \n",
      "1                                  GN0031(|||||||||);       EPS025   \n",
      "2                                  GN0031(|||||||||);       EPS025   \n",
      "3                                GN0035(MENDILVELSO);       EPS025   \n",
      "4                                      GN0036(DIMAR);       EPS025   \n",
      "..                                                ...          ...   \n",
      "78  GN0059;GN0169(PRIMER APELLIDO|HOLGUIN|PESCA|SE...       EPS025   \n",
      "79  GN0059;GN0169(PRIMER APELLIDO|LEON|CONTRERAS|S...       EPS025   \n",
      "80         GN0169(|SEGUNDO APELLIDO|GIRON|SUAREZ|||);       EPS025   \n",
      "81  GN0059;GN0169(PRIMER APELLIDO|MAYORGA|BERMUDEZ...       EPS025   \n",
      "82    GN0169(|SEGUNDO APELLIDO|RODRIGUEZ|JIMENEZ|||);       EPS025   \n",
      "\n",
      "   TPS_EST_AFL_ID_from_adres Where  \n",
      "0                         AC   NaN  \n",
      "1                         AC   NaN  \n",
      "2                         AC   NaN  \n",
      "3                         AC   NaN  \n",
      "4                         AC   NaN  \n",
      "..                       ...   ...  \n",
      "78                        AC   NaN  \n",
      "79                        AC   NaN  \n",
      "80                        AC   NaN  \n",
      "81                        AC   NaN  \n",
      "82                        AC   NaN  \n",
      "\n",
      "[83 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa de forma aislada la glosa GN0169 (“Datos del afiliado no coinciden con RNEC”).\n",
    "    Esta versión corregida asegura que solo se modifiquen los registros\n",
    "    relevantes y no se pierdan datos de otros DataFrames.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando process_gn0169 ---\")\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)}\")\n",
    "\n",
    "    # --- 1. Inicialización y Aislamiento ---\n",
    "    DF_059_169_updated = DF_059_169.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "    \n",
    "    # Se crea una máscara para encontrar los registros con la glosa GN0169.\n",
    "    mask_gn169 = Df_NS_NEG['Glosa_2'].str.contains('GN0169', na=False)\n",
    "    \n",
    "    # Se dividen los registros de NEG en dos grupos: los que se procesarán y el resto.\n",
    "    df_gn = Df_NS_NEG[mask_gn169].copy()\n",
    "    df_resto = Df_NS_NEG[~mask_gn169].copy()\n",
    "\n",
    "    # Si no hay nada que procesar, se retorna todo como estaba.\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0169.\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_059_169\n",
    "\n",
    "    # --- 2. Procesar SOLO los registros relevantes (df_gn) ---\n",
    "\n",
    "    # Copiar registros nuevos para auditoría (esta lógica no cambia).\n",
    "    key_cols = ['TPS_IDN_ID','HST_IDN_NUMERO_IDENTIFICACION','NOVEDAD']\n",
    "    if not DF_059_169_updated.empty:\n",
    "        existing_keys = DF_059_169_updated[key_cols].apply(tuple, axis=1)\n",
    "        new_mask = ~df_gn[key_cols].apply(tuple, axis=1).isin(existing_keys)\n",
    "        df_new_for_audit = df_gn[new_mask].copy()\n",
    "    else:\n",
    "        df_new_for_audit = df_gn.copy()\n",
    "    DF_059_169_updated = pd.concat([DF_059_169_updated, df_new_for_audit], ignore_index=True)\n",
    "\n",
    "    # Eliminar la glosa GN0169 y recalcular SOLO en el grupo de trabajo.\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'\\s*GN0169\\([^)]*\\);?', '', regex=True).str.strip()\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str.split(';').str[0].str[:6].replace('', '0')\n",
    "\n",
    "    # --- 3. Distribuir los registros procesados ---\n",
    "    \n",
    "    # Identificar los registros que quedaron sin glosas.\n",
    "    mask_move_to_envio = df_gn['No_Glosa'] == 0\n",
    "    df_move = df_gn[mask_move_to_envio].copy()\n",
    "    \n",
    "    # Identificar los que aún tienen glosas.\n",
    "    df_keep = df_gn[~mask_move_to_envio].copy()\n",
    "\n",
    "    # Mover los registros limpios a Df_NS_Envio.\n",
    "    if not df_move.empty:\n",
    "        df_move_clean = df_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "        df_aligned = df_move_clean.reindex(columns=Df_NS_Envio.columns)\n",
    "        Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_aligned], ignore_index=True)\n",
    "\n",
    "    # --- 4. Reconstrucción Final ---\n",
    "    \n",
    "    # El nuevo Df_NS_NEG es la unión del resto (que nunca se tocó) y los que se procesaron pero se quedan.\n",
    "    Df_NS_NEG_final = pd.concat([df_resto, df_keep], ignore_index=True)\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"\\nDespués: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Después: DF_059_169 = {len(DF_059_169_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a5667",
   "metadata": {},
   "source": [
    "## 🚩 GN0340\n",
    "1. IPS Primaria no es válida de acuerdo Registro Especial de Prestadores de Servicios de Salud.\n",
    "2. GN0340(Causal|IPSReportadaNovedad);\n",
    "3. Esta glosa presenta dos causales: \"1.\" La IPS reportada no se encuentra en el Registro Especial de Prestadores de Servicios de Salud \"2.\"  La IPS reportada es igual a la registrada en BDUA.\n",
    "4. GN0340;\n",
    "6. GN0340(1|171740068502);\n",
    "7. GN0340(2|500010149201); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7477ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG     = 51 registros\n",
      "Antes: Df_NS_Envio   = 83 registros\n",
      "Antes: DF_No_Enviar  = 31 registros\n",
      "Antes: Total Registros = 165\n",
      "No se encontraron registros con GN0340.\n",
      "Después: Df_NS_NEG     = 51 registros\n",
      "Después: Df_NS_Envio   = 83 registros\n",
      "Después: DF_No_Enviar  = 31 registros\n",
      "Después: Total Registros = 165\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Motivo', 'cod_ent'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def process_gn0340(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0340 (IPS Primaria no válida de acuerdo RIPS).\n",
    "    Asegura que la suma de los registros de salida sea igual a la de entrada.\n",
    "\n",
    "      - Filtrar GN0340.\n",
    "      - Extraer causal e IPSReportada (y un tercer valor para Causal 1).\n",
    "      - Para Causal=='2': limpiar glosa, recalcular, mover a DF_No_Enviar si No_Glosa==0.\n",
    "      - Para Causal=='1':\n",
    "          - Corregir 'COD_1_NOVEDAD' y 'COD_2_NOVEDAD' usando 'MNS_ID' y el mapeo.\n",
    "          - Limpiar glosa, recalcular No_Glosa y Code_Glosa.\n",
    "          - Mover a Df_NS_Envio si No_Glosa==0.\n",
    "          - Mantener en Df_NS_NEG si hay más glosas.\n",
    "      - Los registros con GN0340 pero con Causal no reconocida (ni '1' ni '2')\n",
    "        o extracción fallida serán enviados a DF_No_Enviar después de intentar limpiar la glosa.\n",
    "      - Mantener en Df_NS_NEG los no afectados y los con glosas restantes.\n",
    "    \"\"\"\n",
    "    initial_total_records = len(Df_NS_NEG) + len(Df_NS_Envio) + len(DF_No_Enviar)\n",
    "    print(f\"Antes: Df_NS_NEG     = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "    print(f\"Antes: Total Registros = {initial_total_records}\")\n",
    "\n",
    "    # 1) Separar registros a procesar (con GN0340) de los que no\n",
    "    mask_gn0340 = Df_NS_NEG['Glosa_2'].str.contains(r'GN0340\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0340].copy()\n",
    "    df_not_gn0340 = Df_NS_NEG[~mask_gn0340].copy() # Registros que no tienen GN0340\n",
    "\n",
    "    # Si no hay registros con GN0340, se retorna lo mismo que entró\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con GN0340.\")\n",
    "        print(f\"Después: Df_NS_NEG     = {len(Df_NS_NEG)} registros\")\n",
    "        print(f\"Después: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "        print(f\"Después: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "        print(f\"Después: Total Registros = {len(Df_NS_NEG) + len(Df_NS_Envio) + len(DF_No_Enviar)}\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "    # 2) Extraer causal e IPSReportada (y el tercer valor para Causal 1)\n",
    "    glosa_pattern_general = r'GN0340\\((\\d+)\\|([0-9]+)(?:\\|([0-9]+))?\\);?'\n",
    "    extracted_data = df_gn['Glosa_2'].str.extract(glosa_pattern_general)\n",
    "    df_gn['Causal'] = extracted_data[0]\n",
    "    df_gn['IPSReportada'] = extracted_data[1]\n",
    "\n",
    "    # Inicializar DataFrames para los resultados intermedios\n",
    "    df_remaining_in_neg = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "    \n",
    "    # 3) CASO 2: Causal == '2'\n",
    "    df_c2 = df_gn[df_gn['Causal']=='2'].copy()\n",
    "    if not df_c2.empty:\n",
    "        df_c2['Glosa_2'] = df_c2['Glosa_2'].str.replace(r'GN0340\\([^)]*\\);?', '', regex=True)\n",
    "        df_c2['Glosa_2'] = df_c2['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_c2['No_Glosa'] = df_c2['Glosa_2'].str.count(';')\n",
    "        df_c2['Code_Glosa'] = df_c2['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        mask_c2_to_no_enviar = (df_c2['No_Glosa']==0)\n",
    "        df_c2_move_to_no_enviar = df_c2[mask_c2_to_no_enviar].copy()\n",
    "        \n",
    "        if not df_c2_move_to_no_enviar.empty:\n",
    "            df_c2_move_to_no_enviar['Motivo'] = \"Caso 2. La IPS reportada es igual a la registrada en BDUA.\"\n",
    "            \n",
    "            # --- CORRECCIÓN CLAVE AQUÍ ---\n",
    "            # Asegura que DF_No_Enviar tenga todas las columnas de df_c2_move_to_no_enviar\n",
    "            for col in df_c2_move_to_no_enviar.columns:\n",
    "                if col not in DF_No_Enviar.columns:\n",
    "                    DF_No_Enviar[col] = pd.NA # Define un valor predeterminado si es necesario\n",
    "            \n",
    "            # Concatena el DataFrame movido. pd.concat manejará automáticamente\n",
    "            # las columnas que estén en DF_No_Enviar pero no en df_c2_move_to_no_enviar (rellenando con NaN)\n",
    "            DF_No_Enviar = pd.concat([DF_No_Enviar, df_c2_move_to_no_enviar], ignore_index=True)\n",
    "            # --- FIN CORRECCIÓN CLAVE ---\n",
    "\n",
    "        df_c2_remain = df_c2[~mask_c2_to_no_enviar].copy()\n",
    "        df_remaining_in_neg = pd.concat([df_remaining_in_neg, df_c2_remain], ignore_index=True)\n",
    "\n",
    "    # 4) CASO 1: Causal == '1'\n",
    "    df_c1 = df_gn[df_gn['Causal']=='1'].copy()\n",
    "    if not df_c1.empty:\n",
    "        ips_map = {\n",
    "            \"001\":  \"850010014401\", \"010\":  \"850100019001\", \"015\":  \"850150042216\",\n",
    "            \"125\":  \"851250042210\", \"136\":  \"851360042215\", \"139\":  \"851390042204\",\n",
    "            \"162\":  \"851620042205\", \"225\":  \"852250042212\", \"230\":  \"852300042209\",\n",
    "            \"250\":  \"852500042203\", \"263\":  \"852630042208\", \"279\":  \"852790042217\",\n",
    "            \"300\":  \"853000042211\", \"315\":  \"853150042214\", \"325\":  \"853250042213\",\n",
    "            \"400\":  \"854000042207\", \"410\":  \"854100008001\", \"430\":  \"854300042206\",\n",
    "            \"440\":  \"854400042202\"\n",
    "        }\n",
    "        \n",
    "        df_c1.loc[:, 'COD_1_NOVEDAD'] = df_c1['MNS_ID'].map(ips_map)\n",
    "        df_c1.loc[:, 'COD_2_NOVEDAD'] = df_c1['MNS_ID'].map(ips_map)\n",
    "\n",
    "        df_c1['Glosa_2'] = df_c1['Glosa_2'].str.replace(r'GN0340\\(1\\|[0-9]+\\|[0-9]+\\);?', '', regex=True)\n",
    "        df_c1['Glosa_2'] = df_c1['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_c1['No_Glosa'] = df_c1['Glosa_2'].str.count(';')\n",
    "        df_c1['Code_Glosa'] = df_c1['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        mask_c1_to_envio = (df_c1['No_Glosa']==0)\n",
    "        df_c1_move_to_envio = df_c1[mask_c1_to_envio].copy()\n",
    "        if not df_c1_move_to_envio.empty:\n",
    "            # --- CORRECCIÓN CLAVE AQUÍ ---\n",
    "            # Asegura que Df_NS_Envio tenga todas las columnas de df_c1_move_to_envio\n",
    "            for col in df_c1_move_to_envio.columns:\n",
    "                if col not in Df_NS_Envio.columns:\n",
    "                    Df_NS_Envio[col] = pd.NA # Define un valor predeterminado si es necesario\n",
    "            \n",
    "            # Concatena el DataFrame movido.\n",
    "            Df_NS_Envio = pd.concat([Df_NS_Envio, df_c1_move_to_envio], ignore_index=True)\n",
    "            # --- FIN CORRECCIÓN CLAVE ---\n",
    "\n",
    "        df_c1_remain = df_c1[~mask_c1_to_envio].copy()\n",
    "        df_remaining_in_neg = pd.concat([df_remaining_in_neg, df_c1_remain], ignore_index=True)\n",
    "\n",
    "    # 5) Manejar registros de df_gn que NO fueron Causal '1' ni '2' (o donde la extracción de causal falló)\n",
    "    handled_indices = set(df_c1.index).union(set(df_c2.index))\n",
    "    df_gn_unhandled = df_gn.loc[~df_gn.index.isin(handled_indices)].copy()\n",
    "\n",
    "    if not df_gn_unhandled.empty:\n",
    "        df_gn_unhandled['Glosa_2'] = df_gn_unhandled['Glosa_2'].str.replace(r'GN0340\\([^)]*\\);?', '', regex=True)\n",
    "        df_gn_unhandled['Glosa_2'] = df_gn_unhandled['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_gn_unhandled['No_Glosa'] = df_gn_unhandled['Glosa_2'].str.count(';')\n",
    "        df_gn_unhandled['Code_Glosa'] = df_gn_unhandled['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        df_gn_unhandled['Motivo'] = \"Causal GN0340 no reconocida o error de extracción.\"\n",
    "        \n",
    "        # --- CORRECCIÓN CLAVE AQUÍ ---\n",
    "        # Asegura que DF_No_Enviar tenga todas las columnas de df_gn_unhandled\n",
    "        for col in df_gn_unhandled.columns:\n",
    "            if col not in DF_No_Enviar.columns:\n",
    "                DF_No_Enviar[col] = pd.NA\n",
    "        \n",
    "        # Concatena el DataFrame movido.\n",
    "        DF_No_Enviar = pd.concat([DF_No_Enviar, df_gn_unhandled], ignore_index=True)\n",
    "        # --- FIN CORRECCIÓN CLAVE ---\n",
    "\n",
    "    # 6) Reconstruir Df_NS_NEG\n",
    "    Df_NS_NEG_updated = pd.concat([df_not_gn0340, df_remaining_in_neg], ignore_index=True)\n",
    "\n",
    "    # Se eliminan columnas temporales si no forman parte del esquema final de DF_No_Enviar.\n",
    "    # Esta línea se mantiene si 'Causal' y 'IPSReportada' NO deben estar en el DF_No_Enviar final.\n",
    "    # Si estas columnas sí deben estar, esta línea debe ser eliminada.\n",
    "    DF_No_Enviar = DF_No_Enviar.drop(columns=['Causal', 'IPSReportada'], errors='ignore')\n",
    "    Df_NS_Envio = Df_NS_Envio.drop(columns=['Code_Glosa', 'No_Glosa', 'Glosa_2'], errors='ignore')\n",
    "\n",
    "    final_total_records = len(Df_NS_NEG_updated) + len(Df_NS_Envio) + len(DF_No_Enviar)\n",
    "    print(f\"Después: Df_NS_NEG     = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Después: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "    print(f\"Después: Total Registros = {final_total_records}\")\n",
    "\n",
    "    if initial_total_records != final_total_records:\n",
    "        print(\"¡Advertencia! La suma de registros de salida no coincide con la entrada.\")\n",
    "        print(f\"Diferencia: {final_total_records - initial_total_records}\")\n",
    "    else:\n",
    "        print(\"¡Éxito! La suma de registros de salida coincide con la entrada.\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0340(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar)\n",
    "Df_NS_Envio = Df_NS_Envio.drop(columns=['Code_Glosa', 'No_Glosa', 'Glosa_2'], errors='ignore')\n",
    "print(DF_No_Enviar.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038927fe",
   "metadata": {},
   "source": [
    "## 🚩 GN0361\n",
    "1. La novedad no es reportada dentro de los dos mes siguientes, Decreto 780 de 2016.\n",
    "2. GN0361(01/05/2018|31/07/2018);\n",
    "3. GN0361(FechaInicioNovedad|FechaDosMesesDespues);\n",
    "4. La fecha de la novedad debe estar dentro del plazo establecido en Decreto 780 de 2016, dos meses antes teniendo en cuenta la fecha del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d41644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG = 51 registros\n",
      "Antes: Df_NS_Envio = 83 registros\n",
      "Después: Df_NS_NEG = 45 registros\n",
      "Después: Df_NS_Envio = 89 registros\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "\n",
    "def process_gn0361(Df_NS_NEG, Df_NS_Envio, fecha_hoy):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0361 en Df_NS_NEG:\n",
    "      - Valida si FECHA_NOVEDAD < fecha mínima de reporte (dos meses antes de fecha_hoy).\n",
    "      - Si es menor, actualiza FECHA_NOVEDAD a la fecha mínima.\n",
    "      - Elimina la glosa GN0361(...) de Glosa_2.\n",
    "      - Recalcula Code_Glosa y No_Glosa.\n",
    "      - Si No_Glosa == 0, mueve el registro a Df_NS_Envio.\n",
    "      - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # Filtrar registros con GN0361\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0361\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # Calcular fecha mínima de reporte (dos meses antes de fecha_hoy)\n",
    "    fecha_minima = datetime.strptime(fecha_hoy, '%d/%m/%Y') - relativedelta(months=2)\n",
    "\n",
    "    # Validar y actualizar FECHA_NOVEDAD solo si es menor a la fecha mínima\n",
    "    def validar_fecha(fecha_str):\n",
    "        try:\n",
    "            fecha_novedad = datetime.strptime(fecha_str, '%d/%m/%Y')\n",
    "            if fecha_novedad < fecha_minima:\n",
    "                return fecha_minima.strftime('%d/%m/%Y')\n",
    "            else:\n",
    "                return fecha_str\n",
    "        except Exception:\n",
    "            return fecha_str\n",
    "\n",
    "    df_gn['FECHA_NOVEDAD'] = df_gn['FECHA_NOVEDAD'].apply(validar_fecha)\n",
    "\n",
    "    # Eliminar la glosa GN0361(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0361\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # Mantener en Df_NS_NEG los que aún tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0361(Df_NS_NEG, Df_NS_Envio, Fecha)\n",
    "Df_NS_Envio = Df_NS_Envio.drop(columns=['Causal','IPSReportada'], errors='ignore')\n",
    "print(Df_NS_Envio.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52241f",
   "metadata": {},
   "source": [
    "## 🚩 GN0390\n",
    "1. Afiliado portabilidad es igual a la de BDUA.\n",
    "2. GN0390;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a06c1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG = 45 registros\n",
      "Antes: DF_No_Enviar = 31 registros\n",
      "Después: Df_NS_NEG = 24 registros\n",
      "Después: DF_No_Enviar = 52 registros\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def process_gn0390(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0390:\n",
    "    - Mueve todos los registros con GN0390 a DF_No_Enviar.\n",
    "    - Asigna el motivo \"Afiliado portabilidad es igual a la de BDUA.\" en la columna 'Motivo'.\n",
    "    - Elimina las columnas auxiliares ['Glosa_2', 'No_Glosa', 'Code_Glosa'] en DF_No_Enviar.\n",
    "    - Devuelve los DataFrames actualizados.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0390\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0390', na=False)\n",
    "    df_gn390 = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Asignar motivo\n",
    "    df_gn390['Motivo'] = \"Afiliado portabilidad es igual a la de BDUA.\"\n",
    "\n",
    "    # 3) Eliminar columnas auxiliares\n",
    "    df_gn390 = df_gn390.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "\n",
    "    # 4) Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_gn390], ignore_index=True)\n",
    "\n",
    "    # 5) Mantener en Df_NS_NEG solo los que no tienen GN0390\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask].copy()\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Implementación de ejemplo:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0390(Df_NS_NEG, DF_No_Enviar)\n",
    "print(Df_NS_Envio.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954158d8",
   "metadata": {},
   "source": [
    "## 🚩 GN0501\n",
    "1. El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\n",
    "2. GN0501(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a93e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 24\n",
      "Antes: DF_No_Enviar = 52\n",
      "Después: Df_NS_NEG = 18\n",
      "Después: DF_No_Enviar = 58\n"
     ]
    }
   ],
   "source": [
    "def process_gn0501(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0501:\n",
    "    - Mueve todos los registros con GN0501 a DF_No_Enviar.\n",
    "    - Asigna el motivo correspondiente en la columna 'Motivo'.\n",
    "    - Elimina las columnas auxiliares ['Glosa_2', 'No_Glosa', 'Code_Glosa'] en DF_No_Enviar.\n",
    "    - Devuelve los DataFrames actualizados.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # Filtrar registros con GN0501\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0501', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # Asignar motivo\n",
    "    df_gn['Motivo'] = \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"\n",
    "\n",
    "    # Eliminar columnas auxiliares\n",
    "    df_gn = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "\n",
    "    # Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_gn], ignore_index=True)\n",
    "\n",
    "    # Mantener en Df_NS_NEG solo los que no tienen GN0501\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask].copy()\n",
    "\n",
    "    print(f\"Después: Df_NS_NEG = {len(Df_NS_NEG_updated)}\")\n",
    "    print(f\"Después: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0501(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad7593",
   "metadata": {},
   "source": [
    "# 6. Procesar Envio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bc688",
   "metadata": {},
   "source": [
    "## 6.1. Cargue NS SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07793b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
       "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
       "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
       "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
       "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
       "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
       "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_NS_Envio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea916557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de registros en Df_NS_Envio: 89\n",
      "Número total de registros en Df_NS_Envio: 774\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_SIE\n",
    "new_data = pd.read_csv(R_NS_SIE, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"Where\"], errors='ignore')\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Marcar los registros existentes con \"Glosas\"\n",
    "Df_NS_Envio[\"Where\"] = \"Glosas\"\n",
    "\n",
    "# Marcar los nuevos registros con \"SIE\"\n",
    "new_data[\"Where\"] = \"SIE\"\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el número total de registros en Df_NS_Envio\n",
    "print(f\"Número total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10cb3a",
   "metadata": {},
   "source": [
    "## 6.2. Cargue novedades planos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "716c606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de registros en Df_NS_Envio: 774\n",
      "Número total de registros en Df_NS_Envio: 775\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el número total de registros en Df_NS_Envio\n",
    "print(f\"Número total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_Enviar\n",
    "new_data = pd.read_csv(R_NS_Enviar, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio, excepto las columnas faltantes\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"TPS_EST_AFL_ID_from_adres\"], errors='ignore')\n",
    "\n",
    "# La última columna del archivo cargado corresponde a la columna \"Where\"\n",
    "new_data[\"Where\"] = new_data.iloc[:, -1]\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el número total de registros en Df_NS_Envio\n",
    "print(f\"Número total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae014e4",
   "metadata": {},
   "source": [
    "## 6.3 Validar ADRES EPS y Regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06da815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT_ID_ADRES\n",
      "None      686\n",
      "EPS025     89\n",
      "Name: count, dtype: int64\n",
      "ENT_ID_ADRES\n",
      "EPS025    751\n",
      "NaN        20\n",
      "EPSC25      4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 0) Ver conteos previos (opcional)\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))\n",
    "\n",
    "# 1) Merge global para traer datos desde ADRES:\n",
    "df_adres_mini = (\n",
    "    DF_ADRES[[\n",
    "        \"TPS_IDN_ID\", \n",
    "        \"HST_IDN_NUMERO_IDENTIFICACION\", \n",
    "        \"ENT_ID_ADRES\", \n",
    "        \"TPS_EST_AFL_ID\"\n",
    "    ]]\n",
    "    .rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    df_adres_mini,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\",\"_new\")\n",
    ")\n",
    "\n",
    "# 2) Rellenar solo donde faltaba:\n",
    "Df_NS_Envio[\"ENT_ID_ADRES\"] = Df_NS_Envio[\"ENT_ID_ADRES\"].fillna(Df_NS_Envio[\"ENT_ID_ADRES_new\"])\n",
    "Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"].fillna(\n",
    "    Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres_new\"]\n",
    ")\n",
    "\n",
    "# 3) Eliminar columnas auxiliares del merge\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_ADRES_new\",\"TPS_EST_AFL_ID_from_adres_new\"], inplace=True)\n",
    "\n",
    "# 4) Continuar con tu lógica de evoluciones N01…\n",
    "mapa_n01 = (\n",
    "    Df_NS_Envio.loc[Df_NS_Envio[\"NOVEDAD\"] == \"N01\", \n",
    "                    [\"COD_1_NOVEDAD\",\"COD_2_NOVEDAD\",\"ENT_ID_ADRES\",\"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\":\"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\":\"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\":\"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\":\"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "mask_evol = (\n",
    "    Df_NS_Envio[\"ENT_ID_ADRES\"].isna() &\n",
    "    Df_NS_Envio[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    ")\n",
    "Df_NS_Envio.loc[mask_evol, \"ENT_ID_ADRES\"] = Df_NS_Envio.loc[mask_evol, \"ENT_ID_from_self\"]\n",
    "Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n",
    "\n",
    "# 5) Verificar conteos finales\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed89d7",
   "metadata": {},
   "source": [
    "### 6.3.1 Mover novedades de contributivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d4098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "Número de registros en Df_NS_Envio: 775\n",
      "Número de registros en DF_NS_EPSC25: 14\n",
      "Después del proceso:\n",
      "Número de registros en Df_NS_Envio: 771\n",
      "Número de registros en DF_NS_EPSC25: 18\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el número de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"Número de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"Número de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")\n",
    "\n",
    "# Asegurarse de que DF_NS_EPSC25 tenga la columna 'Where'\n",
    "if 'Where' not in DF_NS_EPSC25.columns:\n",
    "    DF_NS_EPSC25['Where'] = None\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES sea igual a EPSC25\n",
    "mask = Df_NS_Envio[\"ENT_ID_ADRES\"] == \"EPSC25\"\n",
    "to_move = Df_NS_Envio[mask].copy()\n",
    "\n",
    "# Mover los registros a DF_NS_EPSC25\n",
    "DF_NS_EPSC25 = pd.concat([DF_NS_EPSC25, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio[~mask].copy()\n",
    "\n",
    "# Imprimir el número de registros después del proceso\n",
    "print(f\"Después del proceso:\")\n",
    "print(f\"Número de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"Número de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478ba24",
   "metadata": {},
   "source": [
    "### 6.3.2 Mover a no enviar las que no tienen EPS capresoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e12207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "Número de registros en Df_NS_Envio: 771\n",
      "Número de registros en DF_No_Enviar: 58\n",
      "Después del proceso:\n",
      "Número de registros en Df_NS_Envio: 751\n",
      "Número de registros en DF_No_Enviar: 78\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el número de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"Número de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"Número de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES es nulo o vacío\n",
    "mask_empty = Df_NS_Envio[\"ENT_ID_ADRES\"].isna() | (Df_NS_Envio[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")\n",
    "to_move = Df_NS_Envio.loc[mask_empty].copy()\n",
    "\n",
    "# Asignar el motivo a los registros movidos\n",
    "to_move[\"Motivo\"] = \"No existen MS Adres actualmente\"\n",
    "\n",
    "# Concatenar los registros movidos al DataFrame DF_No_Enviar\n",
    "DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio.loc[~mask_empty].copy()\n",
    "\n",
    "# Imprimir el número de registros después del proceso\n",
    "print(f\"Después del proceso:\")\n",
    "print(f\"Número de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"Número de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46826b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
       "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
       "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
       "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
       "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
       "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
       "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Motivo', 'cod_ent',\n",
       "       'Where'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_No_Enviar[\"Where\"] = \"Glosas\"\n",
    "DF_No_Enviar.columns\n",
    "#print(DF_No_Enviar['Where'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de5c8649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NUM_SOLICITUD_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ENT_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TPS_IDN_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "HST_IDN_NUMERO_IDENTIFICACION",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_PRIMER_APELLIDO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_SEGUNDO_APELLIDO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_PRIMER_NOMBRE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_SEGUNDO_NOMBRE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_FECHA_NACIMIENTO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DPR_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MNS_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "FECHA_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_1_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_2_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_3_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_4_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_5_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_6_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_7_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Glosa",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ENT_ID_ADRES",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TPS_EST_AFL_ID_from_adres",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Where",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5bd76c11-7c37-401a-b9a4-a2ff76457d55",
       "rows": [],
       "shape": {
        "columns": 24,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_SOLICITUD_NOVEDAD</th>\n",
       "      <th>ENT_ID</th>\n",
       "      <th>TPS_IDN_ID</th>\n",
       "      <th>HST_IDN_NUMERO_IDENTIFICACION</th>\n",
       "      <th>AFL_PRIMER_APELLIDO</th>\n",
       "      <th>AFL_SEGUNDO_APELLIDO</th>\n",
       "      <th>AFL_PRIMER_NOMBRE</th>\n",
       "      <th>AFL_SEGUNDO_NOMBRE</th>\n",
       "      <th>AFL_FECHA_NACIMIENTO</th>\n",
       "      <th>DPR_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>COD_2_NOVEDAD</th>\n",
       "      <th>COD_3_NOVEDAD</th>\n",
       "      <th>COD_4_NOVEDAD</th>\n",
       "      <th>COD_5_NOVEDAD</th>\n",
       "      <th>COD_6_NOVEDAD</th>\n",
       "      <th>COD_7_NOVEDAD</th>\n",
       "      <th>Glosa</th>\n",
       "      <th>ENT_ID_ADRES</th>\n",
       "      <th>TPS_EST_AFL_ID_from_adres</th>\n",
       "      <th>Where</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NUM_SOLICITUD_NOVEDAD, ENT_ID, TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, AFL_PRIMER_APELLIDO, AFL_SEGUNDO_APELLIDO, AFL_PRIMER_NOMBRE, AFL_SEGUNDO_NOMBRE, AFL_FECHA_NACIMIENTO, DPR_ID, MNS_ID, NOVEDAD, FECHA_NOVEDAD, COD_1_NOVEDAD, COD_2_NOVEDAD, COD_3_NOVEDAD, COD_4_NOVEDAD, COD_5_NOVEDAD, COD_6_NOVEDAD, COD_7_NOVEDAD, Glosa, ENT_ID_ADRES, TPS_EST_AFL_ID_from_adres, Where]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_NS_EPSC25\n",
    "DF_No_Enviar\n",
    "Df_NS_Envio\n",
    "\n",
    "# Filtrar el registro donde \"HST_IDN_NUMERO_IDENTIFICACION\" es igual a \"74811048\"\n",
    "Df_NS_Envio[Df_NS_Envio[\"HST_IDN_NUMERO_IDENTIFICACION\"] == \"74811048\"]\n",
    "#Df_NS_NEG[Df_NS_NEG['No_Glosa'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5df940ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code_Glosa\n",
      "GN0258    4\n",
      "GN0014    3\n",
      "GN0130    3\n",
      "GN0018    2\n",
      "GN0031    2\n",
      "GN0404    2\n",
      "GN0011    1\n",
      "GN0113    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los registros únicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb895175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N01' 'N19' 'N21' 'N32' 'N39' 'N43' 'N25']\n",
      "No_Glosa\n",
      "1    18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los valores únicos\n",
    "print(Df_NS_NEG['NOVEDAD'].unique())\n",
    "print(Df_NS_NEG['No_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28f312e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code_Glosa\n",
      "GN0258    4\n",
      "GN0014    3\n",
      "GN0130    3\n",
      "GN0018    2\n",
      "GN0031    2\n",
      "GN0404    2\n",
      "GN0011    1\n",
      "GN0113    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Df_NS_NEG['Code_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1ec2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df_NS_NEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54bdef",
   "metadata": {},
   "source": [
    "# Guardar Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "137391f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado en: C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\09_Septiempre\\19\\DataFrames_Activos 19092025.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Guardar todos los DataFrames activos en un archivo Excel, una hoja por cada uno\n",
    "\n",
    "# Lista de DataFrames y nombres de hoja\n",
    "dfs_to_save = {\n",
    "    \"Df_NS_Envio\": Df_NS_Envio,\n",
    "    \"Df_NS_NEG\": Df_NS_NEG,\n",
    "    \"DF_NS_EPSC25\": DF_NS_EPSC25,\n",
    "    \"DF_No_Enviar\": DF_No_Enviar,\n",
    "    \"DF_ADRES\": DF_ADRES,\n",
    "    \"DF_059_169\": DF_059_169\n",
    "}\n",
    "\n",
    "# Ruta de salida y nombre de archivo\n",
    "output_path = Path(R_Salida) / f\"DataFrames_Activos {F_Envio}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name, df in dfs_to_save.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # Excel limita el nombre de hoja a 31 caracteres\n",
    "\n",
    "print(f\"Archivo guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
