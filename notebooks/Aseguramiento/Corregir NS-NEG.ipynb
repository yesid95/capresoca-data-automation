{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52061c12",
   "metadata": {},
   "source": [
    "# 1. Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e80f0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ece43",
   "metadata": {},
   "source": [
    "# 2. Rutas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257bdc",
   "metadata": {},
   "source": [
    "1. Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2401c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"17/10/2025\"\n",
    "F_Envio = \"17102025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db2d5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada \n",
    "\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0015102025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-02\\EPS025MS0015102025.TXT\"\n",
    "R_NS_NEG = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02510102025.NEG\"\n",
    "R_NS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\10_Octubre\\17\\SIE_NSEPS02517102025.txt\"\n",
    "R_NS_No_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\10_Octubre\\17\\Pendiente 17-10-2025.txt\"\n",
    "R_NS_Enviar = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\10_Octubre\\17\\Pendiente 17-10-2025.txt\"\n",
    "\n",
    "R_Ips = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Constantes\\IPS_CODIGO.txt\"\n",
    "\n",
    "# Ruta Salida\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\10_Octubre\\17\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e898105",
   "metadata": {},
   "source": [
    "2. Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ca22530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada\n",
    "\n",
    "#R_Ms_ADRES_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0020052025.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0020052025.TXT\"\n",
    "#R_NS_NEG = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02516052025.NEG\"\n",
    "\n",
    "# Ruta Salida\n",
    "\n",
    "#R_Salida = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d22915",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "249155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID_ADRES\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5851b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros en Df_NS_Anterior: 421\n"
     ]
    }
   ],
   "source": [
    "# Dataframe NS anterior\n",
    "new_columns = [\"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNS_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"]\n",
    "\n",
    "\n",
    "# Dataframe NS Glosado\n",
    "new_columns.append(\"Glosa\")\n",
    "Df_NS_NEG = pd.read_csv(R_NS_NEG, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NS_NEG.columns = new_columns\n",
    "print(\"N√∫mero de registros en Df_NS_Anterior:\", Df_NS_NEG.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1275f",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0167e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0‚Ä¶n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coiNSide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# 1) Tu merge original (trae ENT_ID_ADRES y TPS_EST_AFL_ID_from_adres de ADRES)\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer  = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "\n",
    "# 2) Preparamos un mini-dataframe s√≥lo con las filas N01 que s√≠ s√≠ cruzaron con ADRES\n",
    "mapa_n01 = (\n",
    "    Df_NS_NEG.loc[Df_NS_NEG[\"NOVEDAD\"] == \"N01\", \n",
    "                  [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .copy()  # Asegurarse de trabajar con una copia del DataFrame\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\": \"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\": \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\": \"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\": \"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# 3) Merge ‚Äúsecundario‚Äù contra esas N01 para rellenar vac√≠os\n",
    "Df_NS_NEG = Df_NS_NEG.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4) Para los que quedaron con ENT_ID_ADRES NaN, y son evoluciones (p.ej. N02, N03, ‚Ä¶),\n",
    "#    rellenamos con el valor tra√≠do de la fila N01\n",
    "mask = Df_NS_NEG[\"ENT_ID_ADRES\"].isna() & Df_NS_NEG[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    "Df_NS_NEG.loc[mask, \"ENT_ID_ADRES\"]                   = Df_NS_NEG.loc[mask, \"ENT_ID_from_self\"]\n",
    "Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_NEG.loc[mask, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "# 5) Limpiamos las columnas auxiliares\n",
    "Df_NS_NEG.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf23a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros en Df_NS_NEG: 421\n",
      "N√∫mero de registros en DF_NS_EPSC25: 8\n",
      "N√∫mero de registros en DF_No_Enviar: 0\n",
      "N√∫mero de registros en Df_NS_NEG: 413\n",
      "N√∫mero de registros en Df_NS_Envio: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"N√∫mero de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "\n",
    "# 1. Definimos la m√°scara con la l√≥gica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_NS_NEG[\"ENT_ID_ADRES\"] == \"EPSC25\")\n",
    ")\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_NS_EPSC25 = Df_NS_NEG.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_NS_NEG = Df_NS_NEG.loc[~mask].copy()\n",
    "\n",
    "# 4. Definimos la m√°scara con la l√≥gica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    Df_NS_NEG[\"ENT_ID_ADRES\"].isna()                 # NaN\n",
    "    | (Df_NS_NEG[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")  # \"\" o \"   \"\n",
    ")\n",
    "\n",
    "# Asignar un DataFrame vac√≠o con las mismas columnas que el original\n",
    "DF_059_169 = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "Df_NS_Envio = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "DF_No_Enviar = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "\n",
    "print(\"N√∫mero de registros en DF_NS_EPSC25:\", DF_NS_EPSC25.shape[0])\n",
    "print(\"N√∫mero de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"N√∫mero de registros en Df_NS_NEG:\", Df_NS_NEG.shape[0])\n",
    "print(\"N√∫mero de registros en Df_NS_Envio:\", Df_NS_Envio.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d474368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros √∫nicos en Code_Glosa: 22\n",
      "Code_Glosa\n",
      "GN0034    153\n",
      "GN0035    109\n",
      "GN0031     35\n",
      "GN0059     15\n",
      "GN0122     15\n",
      "GN0030     13\n",
      "GN0258     12\n",
      "GN0032     12\n",
      "GN0084     10\n",
      "GN0169      7\n",
      "GN0060      5\n",
      "GN0113      4\n",
      "GN0036      4\n",
      "GN0014      4\n",
      "GN0049      3\n",
      "GN0009      3\n",
      "GN0398      3\n",
      "GN0130      2\n",
      "GN0079      1\n",
      "GN0118      1\n",
      "GN0391      1\n",
      "GN0390      1\n",
      "Name: count, dtype: int64\n",
      "Valores √∫nicos en No_Glosa: [2 3 5 6 4]\n"
     ]
    }
   ],
   "source": [
    "# Crear la columna \"Code_Glosa\" con los primeros seis caracteres de la columna \"Glosa\"\n",
    "Df_NS_NEG[\"Code_Glosa\"] = Df_NS_NEG[\"Glosa\"].str[:6]\n",
    "\n",
    "# Crear la columna \"No_Glosa\" contando el car√°cter ';' en \"Glosa\" y sumando 1 si no est√° vac√≠a\n",
    "Df_NS_NEG[\"No_Glosa\"] = Df_NS_NEG[\"Glosa\"].apply(lambda x: x.count(';') + 1 if pd.notnull(x) and x != \"\" else 0)\n",
    "\n",
    "# Imprimir el n√∫mero de registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(\"Registros √∫nicos en Code_Glosa:\", Df_NS_NEG[\"Code_Glosa\"].nunique())\n",
    "# Imprimir los registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())\n",
    "\n",
    "# Imprimir los valores √∫nicos de la columna \"No_Glosa\"\n",
    "print(\"Valores √∫nicos en No_Glosa:\", Df_NS_NEG[\"No_Glosa\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f59425d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de columnas antes: 25\n",
      "N√∫mero de columnas despu√©s: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"N√∫mero de columnas antes:\", Df_NS_NEG.shape[1])\n",
    "Df_NS_NEG[\"Glosa_2\"] = Df_NS_NEG[\"Glosa\"]\n",
    "print(\"N√∫mero de columnas despu√©s:\", Df_NS_NEG.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44196c3d",
   "metadata": {},
   "source": [
    "## Calcular edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95a3622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def agregar_edad(df, col_fecha_nac, fecha_hoy, nuevo_nombre='EDAD'):\n",
    "    \"\"\"\n",
    "    Devuelve una copia de df con una columna nuevo_nombre que contiene\n",
    "    la edad (en a√±os cumplidos) calculada al d√≠a fecha_hoy,\n",
    "    partiendo de col_fecha_nac en formato 'dd/mm/yyyy'.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    \n",
    "    # 1) Parsear las fechas de nacimiento\n",
    "    nac_dt = pd.to_datetime(df2[col_fecha_nac], format='%d/%m/%Y', dayfirst=True)\n",
    "    \n",
    "    # 2) Convertir la fecha fija a datetime\n",
    "    hoy = datetime.strptime(fecha_hoy, '%d/%m/%Y')\n",
    "    \n",
    "    # 3) Determinar si ya cumpli√≥ a√±os este a√±o:\n",
    "    ha_cumplido = (\n",
    "        (nac_dt.dt.month < hoy.month) |\n",
    "        ((nac_dt.dt.month == hoy.month) & (nac_dt.dt.day <= hoy.day))\n",
    "    )\n",
    "    \n",
    "    # 4) Calcular edad: diferencia de a√±os menos 0/1 seg√∫n ha cumplido o no\n",
    "    df2[nuevo_nombre] = (\n",
    "        hoy.year - nac_dt.dt.year\n",
    "        - (~ha_cumplido).astype(int)\n",
    "    )\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "\n",
    "DF_ADRES = agregar_edad(\n",
    "    DF_ADRES,\n",
    "    col_fecha_nac='AFL_FECHA_NACIMIENTO',\n",
    "    fecha_hoy= Fecha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bfb05",
   "metadata": {},
   "source": [
    "# 5. Limpiar Glosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe5087",
   "metadata": {},
   "source": [
    "## üö© GN0009    \n",
    "\n",
    "1.  Cotizante principal (CNT) / Cabeza de familia (SBS), existe en la BDUA o en el Maestro de Ingresos, pero no pertenece a la misma entidad / r√©gimen o no esta en calidad de Cotizante principal / Cabeza de familia.\n",
    "2. GN0009(BDUA|C|EPS002|22/11/1997|B|RE|29/10/2007)\n",
    "3. GN0009(Origen|RegimenCot/Cab|EntidadCot/Cab|FechaAfilEntidadCot/Cab|TipoAfiliado|EstadoAfilCot/Cab|FechaInicioCondicion)\n",
    "4. El Cotizante principal /cabeza de familia que pretenden relacionar al beneficiario, existe en la BDUA o en el Maestro de Ingresos, pero no pertenece a la misma entidad / r√©gimen o no esta en calidad de Cotizante principal / Cabeza de familia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58de3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando process_gn0009 ---\n",
      "Antes: Df_NS_NEG = 413\n",
      "Antes: Df_NS_Envio = 0\n",
      "\n",
      "Despu√©s: Df_NS_NEG = 410\n",
      "Despu√©s: DF_NS_EPSC25 = 9\n",
      "Despu√©s: DF_No_Enviar = 1\n",
      "Despu√©s: Df_NS_Envio = 1\n",
      "--- Finalizando process_gn0009 ---\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                  2228  EPS025         RC                    1246128326   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0             ESTEVES             GUERRERO              LIAN   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0             ADRIAN           18/03/2022     85  ...            CC   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0    1118541125             B             2           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD                                              Glosa  \\\n",
      "0           NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "\n",
      "  ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0       EPS025                        AC  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_gn0009(\n",
    "    Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio, DF_ADRES, Fecha\n",
    "):\n",
    "    \"\"\"\n",
    "    Clasifica y procesa la glosa GN0009. Versi√≥n final con correcci√≥n para\n",
    "    prevenir la contaminaci√≥n de columnas y el error InvalidIndexError.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando process_gn0009 ---\")\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "\n",
    "    # --- 1. Inicializaci√≥n ---\n",
    "    DF_No_Enviar_updated = DF_No_Enviar.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "    DF_NS_EPSC25_updated = DF_NS_EPSC25.copy()\n",
    "\n",
    "    mask_gn0009 = Df_NS_NEG['Glosa_2'].str.contains('GN0009', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0009].copy()\n",
    "    \n",
    "    # CORRECCI√ìN 1: Eliminar columnas duplicadas del DataFrame de origen.\n",
    "    df_gn = df_gn.loc[:, ~df_gn.columns.duplicated()]\n",
    "\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0009.\")\n",
    "        return Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio\n",
    "\n",
    "    # --- 2. Extracci√≥n y Clasificaci√≥n ---\n",
    "    regex_entidad = r'GN0009\\([^|]*\\|([^|]*)\\|'\n",
    "    df_gn['EntidadCotCab'] = df_gn['Glosa_2'].str.extract(regex_entidad, expand=False).str.strip()\n",
    "\n",
    "    def classify_row(row):\n",
    "        if pd.isna(row['EntidadCotCab']) or row['EntidadCotCab'] == '': return 'Validar - Extraccion Fallida'\n",
    "        if row['NOVEDAD'] != 'N32': return 'Validar - Novedad Incorrecta'\n",
    "        if row['EntidadCotCab'] == 'EPS025': return 'Procesar Subsidiado'\n",
    "        if row['EntidadCotCab'] == 'EPSC25': return 'Procesar Contributivo'\n",
    "        return 'No Enviar'\n",
    "    df_gn['classification'] = df_gn.apply(classify_row, axis=1)\n",
    "    \n",
    "    # --- 3. Procesamiento por Categor√≠a ---\n",
    "    df_para_validar = pd.DataFrame()\n",
    "\n",
    "    # CASO: No Enviar (Otra EPS)\n",
    "    df_no_enviar_rows = df_gn[df_gn['classification'] == 'No Enviar'].copy()\n",
    "    if not df_no_enviar_rows.empty:\n",
    "        df_aligned = df_no_enviar_rows.reindex(columns=DF_No_Enviar_updated.columns)\n",
    "        df_aligned['Motivo'] = \"Cabeza de familia en otra EPS\"\n",
    "        DF_No_Enviar_updated = pd.concat([DF_No_Enviar_updated, df_aligned], ignore_index=True)\n",
    "\n",
    "    # CASO: Procesar Contributivo (EPSC25)\n",
    "    df_contributivo_rows = df_gn[df_gn['classification'] == 'Procesar Contributivo'].copy()\n",
    "    if not df_contributivo_rows.empty:\n",
    "        adres_epsc25 = DF_ADRES[DF_ADRES['ENT_ID_ADRES'] == 'EPSC25']\n",
    "        df_merged = pd.merge(df_contributivo_rows, adres_epsc25, how='left', \n",
    "                             left_on=['COD_1_NOVEDAD', 'COD_2_NOVEDAD'], \n",
    "                             right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], \n",
    "                             indicator=True, suffixes=('_original', '_ADRES'))\n",
    "        \n",
    "        df_encontrados = df_merged[df_merged['_merge'] == 'both'].copy()\n",
    "        if not df_encontrados.empty:\n",
    "            reconstructed_data = {}\n",
    "            for col_name in DF_NS_EPSC25_updated.columns:\n",
    "                col_adres, col_original = f\"{col_name}_ADRES\", f\"{col_name}_original\"\n",
    "                if col_adres in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_adres].values\n",
    "                elif col_original in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_original].values\n",
    "                elif col_name in df_encontrados.columns: reconstructed_data[col_name] = df_encontrados[col_name].values\n",
    "            \n",
    "            df_clean = pd.DataFrame(reconstructed_data, index=df_encontrados.index)\n",
    "            df_clean['Where'] = \"Reportar en R1, unificaci√≥n grupo familiar\"\n",
    "            DF_NS_EPSC25_updated = pd.concat([DF_NS_EPSC25_updated, df_clean], ignore_index=True)\n",
    "        \n",
    "        df_no_encontrados = df_merged[df_merged['_merge'] == 'left_only']\n",
    "        if not df_no_encontrados.empty:\n",
    "            # CORRECCI√ìN 2: Limpiar df_no_encontrados antes de a√±adirlo a la pila de validaci√≥n.\n",
    "            df_no_encontrados_clean = df_no_encontrados[df_gn.columns].copy()\n",
    "            df_para_validar = pd.concat([df_para_validar, df_no_encontrados_clean], ignore_index=True)\n",
    "\n",
    "    # CASO: Procesar Subsidiado (EPS025)\n",
    "    df_subsidiado_rows = df_gn[df_gn['classification'] == 'Procesar Subsidiado'].copy()\n",
    "    if not df_subsidiado_rows.empty:\n",
    "        df_subsidiado_rows['FECHA_NOVEDAD'] = Fecha\n",
    "        df_subsidiado_rows['Glosa_2'] = df_subsidiado_rows['Glosa_2'].str.replace(r'\\s*GN0009\\([^)]*\\);?', '', regex=True)\n",
    "        df_subsidiado_rows['No_Glosa'] = df_subsidiado_rows['Glosa_2'].str.count(';')\n",
    "        \n",
    "        df_envio_move = df_subsidiado_rows[df_subsidiado_rows['No_Glosa'] == 0].copy()\n",
    "        df_remain = df_subsidiado_rows[df_subsidiado_rows['No_Glosa'] > 0].copy()\n",
    "\n",
    "        if not df_envio_move.empty:\n",
    "            df_aligned = df_envio_move.reindex(columns=Df_NS_Envio_updated.columns)\n",
    "            Df_NS_Envio_updated = pd.concat([Df_NS_Envio_updated, df_aligned], ignore_index=True)\n",
    "        \n",
    "        if not df_remain.empty:\n",
    "            df_para_validar = pd.concat([df_para_validar, df_remain], ignore_index=True)\n",
    "\n",
    "    # --- 4. Reconstrucci√≥n Final ---\n",
    "    df_para_validar = pd.concat([\n",
    "        df_para_validar,\n",
    "        df_gn[df_gn['classification'].str.startswith('Validar')]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    if not df_para_validar.empty:\n",
    "        if 'NUM_SOLICITUD_NOVEDAD' in df_para_validar.columns:\n",
    "             df_para_validar = df_para_validar.drop_duplicates(subset=['NUM_SOLICITUD_NOVEDAD'])\n",
    "        df_para_validar['Observacion_Validacion'] = 'Validar Novedad/Glosa GN0009'\n",
    "        df_para_validar['Code_Glosa'] = 'GN0009'\n",
    "        df_aligned = df_para_validar.reindex(columns=Df_NS_NEG.columns)\n",
    "    else:\n",
    "        df_aligned = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "        \n",
    "    Df_NS_NEG_final = pd.concat([Df_NS_NEG[~mask_gn0009], df_aligned], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nDespu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: DF_NS_EPSC25 = {len(DF_NS_EPSC25_updated)}\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"--- Finalizando process_gn0009 ---\")\n",
    "\n",
    "    return Df_NS_NEG_final, DF_NS_EPSC25_updated, DF_No_Enviar_updated, Df_NS_Envio_updated\n",
    "\n",
    "\n",
    "# --- Ejemplo de uso ---\n",
    "# Aseg√∫rate de que este nombre coincida con el de la definici√≥n\n",
    "Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio = process_gn0009(\n",
    "     Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar, Df_NS_Envio, DF_ADRES, Fecha\n",
    ")\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110229e",
   "metadata": {},
   "source": [
    "## üö© GN0030    \n",
    "\n",
    "1.  Afiliado no pertenece a la entidad / R√©gimen.\n",
    "2.  GN0030(Regimen|EntidadAfil|FechaInicioAfil|Dpto|Mpio|TipoAfil|EstadoAfil|FechaInicioCond);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f89220ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 410 registros\n",
      "Antes: DF_NS_EPSC25 = 9 registros\n",
      "Antes: DF_No_Enviar = 1 registros\n",
      "Despu√©s: Df_NS_NEG = 397 registros\n",
      "Despu√©s: DF_NS_EPSC25 = 13 registros\n",
      "Despu√©s: DF_No_Enviar = 10 registros\n"
     ]
    }
   ],
   "source": [
    "def process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0030 (‚ÄúAfiliado no pertenece a la entidad / R√©gimen‚Äù) en un DataFrame.\n",
    "\n",
    "    Para cada registro en `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0030':\n",
    "      1. Extrae la fecha de condici√≥n (`FechaInicioCond`) de la glosa.\n",
    "      2. Convierte ambas fechas (`FECHA_NOVEDAD` y `FechaInicioCond`) a datetime.\n",
    "      3. Si `cod_ent == 'EPSC25'` y `FechaInicioCond >= FECHA_NOVEDAD`:\n",
    "         - Actualiza `FECHA_NOVEDAD` al d√≠a siguiente de `FechaInicioCond`.\n",
    "         - Mueve el registro a `DF_NS_EPSC25`.\n",
    "      4. En caso contrario:\n",
    "         - Mueve el registro a `DF_No_Enviar`.\n",
    "         - Asigna motivo \"Afiliado en proceso de traslado a otra EPS\".\n",
    "      5. En ambos DataFrames de destino elimina las columnas de validaci√≥n:\n",
    "         ['Glosa_2', 'No_Glosa', 'Code_Glosa'].\n",
    "      6. Imprime conteos de registros en los DataFrames antes y despu√©s de la operaci√≥n.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        DF_NS_EPSC25 (pd.DataFrame): DataFrame destino para registros EPSC25.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros originales sin GN0030.\n",
    "            - DF_NS_EPSC25_updated (pd.DataFrame): Registros EPSC25 procesados.\n",
    "            - DF_No_Enviar_updated (pd.DataFrame): Registros restantes con motivo de traslado.\n",
    "    \"\"\"\n",
    "    # Imprimir conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_NS_EPSC25 = {len(DF_NS_EPSC25)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # M√°scara para registros con GN0030\n",
    "    mask_gn0030 = Df_NS_NEG['Glosa_2'].str.contains('GN0030', na=False)\n",
    "    df_gn0030 = Df_NS_NEG[mask_gn0030].copy()\n",
    "\n",
    "    # 1) Extraer cod_ent (grupo 2)\n",
    "    df_gn0030['cod_ent'] = df_gn0030['Glosa_2'].str.extract(r'GN0030\\([^|]*\\|([^|]*)\\|')\n",
    "\n",
    "    # 2) Extraer FechaInicioCond (grupo √∫ltimo)\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "        \n",
    "    # Extraer FechaInicioCond de la glosa GN0030\n",
    "    df_gn0030['FechaInicioCond'] = df_gn0030['Glosa_2'].str.extract(\n",
    "        r'GN0030\\([^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);'\n",
    "    )\n",
    "    # Parsear fechas\n",
    "    df_gn0030['FechaInicioCond_dt'] = pd.to_datetime(df_gn0030['FechaInicioCond'], dayfirst=True, format='%d/%m/%Y')\n",
    "    df_gn0030['FECHA_NOVEDAD_dt'] = pd.to_datetime(df_gn0030['FECHA_NOVEDAD'], dayfirst=True, format='%d/%m/%Y')\n",
    "    \n",
    "    # Caso 1: EPSC25 y FechaInicioCond >= FECHA_NOVEDAD\n",
    "    mask1 = (df_gn0030['cod_ent'] == 'EPSC25') # & (df_gn0030['FechaInicioCond_dt'] >= df_gn0030['FECHA_NOVEDAD_dt'])\n",
    "    df_case1 = df_gn0030[mask1].copy()\n",
    "    # Actualizar FECHA_NOVEDAD a FechaInicioCond + 1 d√≠a\n",
    "    df_case1['FECHA_NOVEDAD'] = (df_case1['FechaInicioCond_dt'] + timedelta(days=1)).dt.strftime('%d/%m/%Y')\n",
    "    # Preparar para mover\n",
    "    df_case1 = df_case1.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Caso 2: resto de GN0030\n",
    "    df_case2 = df_gn0030[~mask1].copy()\n",
    "    df_case2['Motivo'] = \"Afiliado en proceso de traslado a otra EPS\"\n",
    "    df_case2 = df_case2.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa', 'FechaInicioCond', 'FechaInicioCond_dt', 'FECHA_NOVEDAD_dt'])\n",
    "    \n",
    "    # Mover registros\n",
    "    DF_NS_EPSC25_updated = pd.concat([DF_NS_EPSC25, df_case1], ignore_index=True)\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_case2], ignore_index=True)\n",
    "    \n",
    "    # Quedar en Df_NS_NEG solo los que no ten√≠an GN0030\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask_gn0030].copy()\n",
    "    \n",
    "    # Imprimir conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_NS_EPSC25 = {len(DF_NS_EPSC25_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG_updated, DF_NS_EPSC25_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Uso de ejemplo:\n",
    "Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar = process_gn0030(Df_NS_NEG, DF_NS_EPSC25, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbbd93",
   "metadata": {},
   "source": [
    "## üö© GN0031\n",
    "\n",
    "1. Afiliado no existe en la BDUA.\n",
    "2. GN0031(TI|1124822660|GARZON|MORALES|DUVAN|FELIPE|S|EPSS03|25|245);\n",
    "3. GN0031(|||||||||||||||||||);\n",
    "4. GN0031(TipoDocBDUA|DocumentoBDUA|PrimerApeBDUA|SegundoApeBDUA|PrimerNomBDUA|SegundoNomBDUA|Regimen|EntidadBDUA|Dpto|Mpio);\n",
    "5. GN0031(SinInformacionBDUA);\n",
    "6. Es muy frecuente que se solicite con un tipo de documento diferente al cargado en la BDUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80399309",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 397\n",
      "Antes: Df_NS_Envio = 1\n",
      "Antes: DF_No_Enviar = 10\n",
      "\n",
      "Despu√©s: Df_NS_NEG = 370\n",
      "Despu√©s: Df_NS_Envio = 10\n",
      "Despu√©s: DF_No_Enviar = 28\n",
      "  NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                  2228  EPS025         RC                    1246128326   \n",
      "1                    81  EPS025         RC                    1118127344   \n",
      "2                    82  EPS025         RC                    1222147970   \n",
      "3                    84  EPS025         RC                    1246129381   \n",
      "4                    89  EPS025         RC                    1117327408   \n",
      "5                   103  EPS025         RC                    1118583296   \n",
      "6                   117  EPS025         RC                    1118127344   \n",
      "7                   118  EPS025         RC                    1222147970   \n",
      "8                   120  EPS025         RC                    1246129381   \n",
      "9                   125  EPS025         RC                    1117327408   \n",
      "\n",
      "  AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0             ESTEVES             GUERRERO              LIAN   \n",
      "1                RIOS                RIA?O           HIJO DE   \n",
      "2             CAHUE?O               RIVERA           HIJO DE   \n",
      "3            COCINERO            CASTA?EDA           HIJO DE   \n",
      "4             CAMARGO              CAMARGO           HIJO DE   \n",
      "5               NAOMI              FAJARDO          MADISSON   \n",
      "6                RIOS                RIA?O              ANNY   \n",
      "7             CAHUE?O               RIVERA              EIMY   \n",
      "8            COCINERO            CASTA?EDA               IZA   \n",
      "9             CAMARGO              CAMARGO            ALIANI   \n",
      "\n",
      "  AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0             ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                NaN           20/09/2025     85  ...          ANNY   \n",
      "2                NaN           27/09/2025     85  ...          EIMY   \n",
      "3                NaN           28/09/2025     85  ...           IZA   \n",
      "4                NaN           30/09/2025     85  ...        ALIANI   \n",
      "5              NAOMI           07/05/2025     85  ...         PESCA   \n",
      "6              SOFIA           20/09/2025     85  ...        CUERVO   \n",
      "7            CELESTE           27/09/2025     85  ...       MORALES   \n",
      "8           VICTORIA           28/09/2025     85  ...          NI?O   \n",
      "9           VICTORIA           30/09/2025     85  ...        GARCIA   \n",
      "\n",
      "  COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0    1118541125             B             2           NaN           NaN   \n",
      "1         SOFIA           NaN           NaN           NaN           NaN   \n",
      "2       CELESTE           NaN           NaN           NaN           NaN   \n",
      "3      VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4      VICTORIA           NaN           NaN           NaN           NaN   \n",
      "5       HOLGUIN           NaN           NaN           NaN           NaN   \n",
      "6          RIOS           NaN           NaN           NaN           NaN   \n",
      "7       CAHUE?O           NaN           NaN           NaN           NaN   \n",
      "8      COCINERO           NaN           NaN           NaN           NaN   \n",
      "9       CAMARGO           NaN           NaN           NaN           NaN   \n",
      "\n",
      "  COD_7_NOVEDAD                                              Glosa  \\\n",
      "0           NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1           NaN                                 GN0031(|||||||||);   \n",
      "2           NaN                                 GN0031(|||||||||);   \n",
      "3           NaN                                 GN0031(|||||||||);   \n",
      "4           NaN                                 GN0031(|||||||||);   \n",
      "5           NaN                                 GN0031(|||||||||);   \n",
      "6           NaN                                 GN0031(|||||||||);   \n",
      "7           NaN                                 GN0031(|||||||||);   \n",
      "8           NaN                                 GN0031(|||||||||);   \n",
      "9           NaN                                 GN0031(|||||||||);   \n",
      "\n",
      "  ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0       EPS025                        AC  \n",
      "1       EPS025                        AC  \n",
      "2       EPS025                        AC  \n",
      "3       EPS025                        AC  \n",
      "4       EPS025                        AC  \n",
      "5       EPS025                        AC  \n",
      "6       EPS025                        AC  \n",
      "7       EPS025                        AC  \n",
      "8       EPS025                        AC  \n",
      "9       EPS025                        AC  \n",
      "\n",
      "[10 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_gn0031(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0031 de forma robusta, preservando la estructura de los DataFrames.\n",
    "    - Caso 1: N01 y ADRES vac√≠o -> No Enviar.\n",
    "    - Caso 2: ‚â† N01 y ADRES lleno -> Limpiar glosa y enviar si aplica.\n",
    "    - Caso 3: ‚â† N01 y ADRES vac√≠o -> Validaci√≥n manual.\n",
    "    - Caso 4: N01 y ADRES lleno -> Validaci√≥n manual (PQR).\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1. INICIAMOS COPIAS PARA ACUMULAR RESULTADOS\n",
    "    # Esto evita errores si un caso no encuentra registros.\n",
    "    DF_No_Enviar_updated = DF_No_Enviar.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "\n",
    "    # Validar y filtrar registros con GN0031\n",
    "    required_columns = ['Glosa_2', 'NOVEDAD', 'TPS_EST_AFL_ID_from_adres']\n",
    "    for col in required_columns:\n",
    "        if col not in Df_NS_NEG.columns:\n",
    "            raise ValueError(f\"Columna requerida no encontrada: {col}\")\n",
    "    \n",
    "    mask_gn0031 = Df_NS_NEG['Glosa_2'].str.contains('GN0031', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0031].copy()\n",
    "\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0031.\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "    # --- DataFrames temporales para cada caso (solo filas) ---\n",
    "    is_n01 = df_gn['NOVEDAD'] == 'N01'\n",
    "    is_adres_empty = df_gn['TPS_EST_AFL_ID_from_adres'].isna() | (df_gn['TPS_EST_AFL_ID_from_adres'].astype(str).str.strip() == \"\")\n",
    "\n",
    "    df_case1 = df_gn[is_n01 & is_adres_empty].copy()\n",
    "    df_case2 = df_gn[~is_n01 & ~is_adres_empty].copy()\n",
    "    df_case3 = df_gn[~is_n01 & is_adres_empty].copy()\n",
    "    df_case4 = df_gn[is_n01 & ~is_adres_empty].copy()\n",
    "\n",
    "    # --- Procesar cada caso ---\n",
    "\n",
    "    # CASO 1: Mover a 'No Enviar'\n",
    "    if not df_case1.empty:\n",
    "        df_case1['Motivo'] = \"Evoluci√≥n/correcci√≥n ya efectiva en ADRES\"\n",
    "        # 2. ALINEAMOS COLUMNAS ANTES DE UNIR\n",
    "        df_aligned = df_case1.reindex(columns=DF_No_Enviar_updated.columns)\n",
    "        DF_No_Enviar_updated = pd.concat([DF_No_Enviar_updated, df_aligned], ignore_index=True)\n",
    "\n",
    "    # CASO 2: Limpiar glosa y mover a 'Envio' si aplica\n",
    "    df_case2_remain = pd.DataFrame() # Para los que se quedan en NEG\n",
    "    if not df_case2.empty:\n",
    "        df_case2['Glosa_2'] = df_case2['Glosa_2'].str.replace(r'GN0031\\([^)]*\\);', '', regex=True)\n",
    "        df_case2['No_Glosa'] = df_case2['Glosa_2'].str.count(';')\n",
    "        df_case2['Code_Glosa'] = df_case2['Glosa_2'].str.extract(r'(GN\\d{4})', expand=False).fillna('0')\n",
    "        \n",
    "        # Mover los que ya no tienen glosas\n",
    "        df_envio_move = df_case2[df_case2['No_Glosa'] == 0]\n",
    "        if not df_envio_move.empty:\n",
    "            df_aligned = df_envio_move.reindex(columns=Df_NS_Envio_updated.columns)\n",
    "            Df_NS_Envio_updated = pd.concat([Df_NS_Envio_updated, df_aligned], ignore_index=True)\n",
    "        \n",
    "        # Guardar los que a√∫n tienen otras glosas\n",
    "        df_case2_remain = df_case2[df_case2['No_Glosa'] > 0]\n",
    "\n",
    "    # CASO 3: A√±adir observaci√≥n para validaci√≥n manual\n",
    "    if not df_case3.empty:\n",
    "        df_case3['Observacion_Validacion'] = \"Validar GN0031: posible error digitaci√≥n o falta de N01 previo.\"\n",
    "\n",
    "    # CASO 4: A√±adir observaci√≥n para PQR\n",
    "    if not df_case4.empty:\n",
    "        df_case4['Observacion_Validacion'] = \"Validar GN0031: ADRES lo tiene pero lo glosa. Requiere PQR.\"\n",
    "\n",
    "    # --- Reconstruir Df_NS_NEG final ---\n",
    "    \n",
    "    # 3. JUNTAMOS TODAS LAS PIEZAS QUE SE QUEDAN EN NEG\n",
    "    piezas_para_neg = [df_case2_remain, df_case3, df_case4]\n",
    "    df_para_neg_final = pd.concat(piezas_para_neg, ignore_index=True)\n",
    "    \n",
    "    # ALINEAMOS EL LOTE COMPLETO a la estructura original de Df_NS_NEG\n",
    "    df_aligned_neg = df_para_neg_final.reindex(columns=Df_NS_NEG.columns)\n",
    "    \n",
    "    Df_NS_NEG_final = pd.concat([\n",
    "        Df_NS_NEG[~mask_gn0031], # Los que no ten√≠an la glosa\n",
    "        df_aligned_neg           # Los que ten√≠an la glosa y se quedan\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nDespu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_No_Enviar_updated\n",
    "\n",
    "# --- 1. Llamado de la funci√≥n corregida ---\n",
    "# Aseg√∫rate de que las variables est√©n inicializadas previamente.\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0031(\n",
    "    Df_NS_NEG=Df_NS_NEG,\n",
    "    Df_NS_Envio=Df_NS_Envio,\n",
    "    DF_No_Enviar=DF_No_Enviar\n",
    ")\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e8ff2",
   "metadata": {},
   "source": [
    "## üö© GN0032\n",
    "\n",
    "1. Afiliado existe en BDUA en estado Fallecido.\n",
    "   1. GN0032(Regimen|Entidad|FechaInicioAfil|Dpto|Mpio|TipoAfiliado|EstadoBDUA|FechaInicioCond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cac71001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG tiene 370 registros\n",
      "Antes: DF_No_Enviar tiene 28 registros\n",
      "Despu√©s: Df_NS_NEG tiene 358 registros\n",
      "Despu√©s: DF_No_Enviar tiene 40 registros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def move_gn0032(Df_NS_NEG, DF_No_Enviar):\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    # M√°scara para glosa GN0032\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0032', na=False)\n",
    "    \n",
    "    # Extraer los registros a mover\n",
    "    to_move = Df_NS_NEG[mask].copy()\n",
    "    # Asignar motivo\n",
    "    to_move['Motivo'] = \"Fallecido en actualmente ADRES\"\n",
    "    # Eliminar columnas auxiliares\n",
    "    to_move = to_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    \n",
    "    # Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "    \n",
    "    # Quedarse en Df_NS_NEG con los que NO tienen GN0032\n",
    "    Df_NS_NEG = Df_NS_NEG[~mask].copy()\n",
    "    \n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG tiene {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar tiene {len(DF_No_Enviar)} registros\")\n",
    "    \n",
    "    return Df_NS_NEG, DF_No_Enviar\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = move_gn0032(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8d17c",
   "metadata": {},
   "source": [
    "## üö© GN0034\n",
    "\n",
    "1. Primer Apellido diferente al registrado en la BDUA.\n",
    "2. GN0034(MARTINEZ);\n",
    "3. GN0034(PrimerApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5db1795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 358 registros\n",
      "Antes: Df_NS_Envio = 10 registros\n",
      "Despu√©s: Df_NS_NEG = 251 registros\n",
      "Despu√©s: Df_NS_Envio = 117 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "112                  2900  EPS025         CC                       4162039   \n",
      "113                  2954  EPS025         CC                    1029645754   \n",
      "114                  2972  EPS025         CC                    1006405973   \n",
      "115                  2975  EPS025         CC                      23789561   \n",
      "116                  2991  EPS025         CC                      46382004   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "112                NI¬•O                  NaN             JORGE   \n",
      "113             FANDI¬•O              NAVARRO             KAROL   \n",
      "114              CATA¬•O           COTINCHARA         SOLIMENDA   \n",
      "115             BRICE¬•O                  NaN             HILDA   \n",
      "116               RIA¬•O              JIMENEZ            BLANCA   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "112             ANIBAL           01/11/1941     85  ...         85001   \n",
      "113            YULIANA           15/01/2007     85  ...         85001   \n",
      "114                NaN           26/03/2001     85  ...         85225   \n",
      "115              MARIA           08/01/1960     85  ...         85250   \n",
      "116             ELVIRA           20/04/1981     85  ...         85010   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "112           NaN           NaN           NaN           NaN           NaN   \n",
      "113           NaN           NaN           NaN           NaN           NaN   \n",
      "114           NaN           NaN           NaN           NaN           NaN   \n",
      "115           NaN           NaN           NaN           NaN           NaN   \n",
      "116           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "112           NaN                                      GN0034(NI¬•O);   \n",
      "113           NaN                                   GN0034(FANDI¬•O);   \n",
      "114           NaN                                    GN0034(CATA¬•O);   \n",
      "115           NaN                                   GN0034(BRICE¬•O);   \n",
      "116           NaN                                     GN0034(RIA¬•O);   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "112       EPS025                        AC  \n",
      "113       EPS025                        AC  \n",
      "114       EPS025                        AC  \n",
      "115       EPS025                        AC  \n",
      "116       EPS025                        AC  \n",
      "\n",
      "[117 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0034(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0034 (Primer Apellido diferente al registrado en la BDUA).\n",
    "    - Extrae el apellido correcto de la glosa GN0034(PrimerApeBDUA);\n",
    "    - Actualiza AFL_PRIMER_APELLIDO con ese valor;\n",
    "    - Elimina la glosa GN0034(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0034\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0034\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer apellido y actualizar AFL_PRIMER_APELLIDO\n",
    "    def extract_apellido(glosa):\n",
    "        match = re.search(r'GN0034\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_PRIMER_APELLIDO'] = df_gn['Glosa_2'].apply(extract_apellido)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0034(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0034\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0034(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c31f41",
   "metadata": {},
   "source": [
    "## üö© GN0035\n",
    "\n",
    "1. Segundo Apellido diferente al registrado en la BDUA.\n",
    "2. GN0035(SUAREZ);\n",
    "3. GN0035(SegundoApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42136af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 251 registros\n",
      "Antes: Df_NS_Envio = 117 registros\n",
      "Despu√©s: Df_NS_NEG = 121 registros\n",
      "Despu√©s: Df_NS_Envio = 247 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "242                   725  EPS025         CC                      17585243   \n",
      "243                  1113  EPS025         CC                       4047189   \n",
      "244                  2197  EPS025         CC                    1119666152   \n",
      "245                  2251  EPS025         CC                    1119666152   \n",
      "246                  2998  EPS025         CC                    1119666152   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "242                PE¬•A              ORDO¬•EZ             DUMAR   \n",
      "243             SALDA¬•A              SALDA¬•A             MIRTO   \n",
      "244                NI¬•O                 NI¬•O            HERMIS   \n",
      "245                NI¬•O                 NI¬•O            HERMIS   \n",
      "246                NI¬•O                 NI¬•O            HERMIS   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "242          FRANCISCO           16/04/1966     85  ...             2   \n",
      "243            ANTONIO           07/03/1967     85  ...             2   \n",
      "244                NaN           04/08/1994     85  ...  852630042208   \n",
      "245                NaN           04/08/1994     85  ...            CC   \n",
      "246                NaN           04/08/1994     85  ...         85400   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "242           B04             5             1            06           NaN   \n",
      "243           B03             5             1            06           NaN   \n",
      "244  852630042208           NaN           NaN           NaN           NaN   \n",
      "245      42547521             B             1           NaN           NaN   \n",
      "246           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "242           NaN                      GN0034(PE¬•A);GN0035(ORDO¬•EZ);   \n",
      "243           NaN                   GN0034(SALDA¬•A);GN0035(SALDA¬•A);   \n",
      "244           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "245           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "246           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "242       EPS025                        AC  \n",
      "243       EPS025                        AC  \n",
      "244       EPS025                        AC  \n",
      "245       EPS025                        AC  \n",
      "246       EPS025                        AC  \n",
      "\n",
      "[247 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0035(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0035 (Segundo Apellido diferente al registrado en la BDUA).\n",
    "    - Extrae el segundo apellido correcto de la glosa GN0035(SegundoApeBDUA);\n",
    "    - Actualiza AFL_SEGUNDO_APELLIDO con ese valor;\n",
    "    - Elimina la glosa GN0035(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0035\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0035\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer segundo apellido y actualizar AFL_SEGUNDO_APELLIDO\n",
    "    def extract_segundo_apellido(glosa):\n",
    "        match = re.search(r'GN0035\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_SEGUNDO_APELLIDO'] = df_gn['Glosa_2'].apply(extract_segundo_apellido)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0035(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0035\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0035(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417554e6",
   "metadata": {},
   "source": [
    "## üö© GN0036\n",
    "\n",
    "1. Primer Nombre diferente al registrado en la BDUA.\n",
    "2. GN0036(MARCELA);\n",
    "3. GN0036(PrimerNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9635ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 121 registros\n",
      "Antes: Df_NS_Envio = 247 registros\n",
      "Despu√©s: Df_NS_NEG = 119 registros\n",
      "Despu√©s: Df_NS_Envio = 249 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "244                  2197  EPS025         CC                    1119666152   \n",
      "245                  2251  EPS025         CC                    1119666152   \n",
      "246                  2998  EPS025         CC                    1119666152   \n",
      "247                    74  EPS025         RC                    1028676960   \n",
      "248                    75  EPS025         RC                    1028676959   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "244                NI¬•O                 NI¬•O            HERMIS   \n",
      "245                NI¬•O                 NI¬•O            HERMIS   \n",
      "246                NI¬•O                 NI¬•O            HERMIS   \n",
      "247              MENDEZ                IVICA            MATHEO   \n",
      "248              MENDEZ                IVICA          JERONIMO   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "244                NaN           04/08/1994     85  ...  852630042208   \n",
      "245                NaN           04/08/1994     85  ...            CC   \n",
      "246                NaN           04/08/1994     85  ...         85400   \n",
      "247                NaN           31/08/2025     85  ...        MATHEO   \n",
      "248                NaN           31/08/2025     85  ...      JERONIMO   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "244  852630042208           NaN           NaN           NaN           NaN   \n",
      "245      42547521             B             1           NaN           NaN   \n",
      "246           NaN           NaN           NaN           NaN           NaN   \n",
      "247           NaN           NaN           NaN           NaN           NaN   \n",
      "248           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "244           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "245           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "246           NaN                         GN0034(NI¬•O);GN0035(NI¬•O);   \n",
      "247           NaN       GN0034(MENDEZ);GN0035(IVICA);GN0036(MATHEO);   \n",
      "248           NaN     GN0034(MENDEZ);GN0035(IVICA);GN0036(JERONIMO);   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "244       EPS025                        AC  \n",
      "245       EPS025                        AC  \n",
      "246       EPS025                        AC  \n",
      "247       EPS025                        AC  \n",
      "248       EPS025                        AC  \n",
      "\n",
      "[249 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0036(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0036 (Primer Nombre diferente al registrado en la BDUA).\n",
    "    - Extrae el primer nombre correcto de la glosa GN0036(PrimerNomBDUA);\n",
    "    - Actualiza AFL_PRIMER_NOMBRE con ese valor;\n",
    "    - Elimina la glosa GN0036(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0036\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0036\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer primer nombre y actualizar AFL_PRIMER_NOMBRE\n",
    "    def extract_primer_nombre(glosa):\n",
    "        match = re.search(r'GN0036\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_PRIMER_NOMBRE'] = df_gn['Glosa_2'].apply(extract_primer_nombre)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0036(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0036\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0036(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e136c",
   "metadata": {},
   "source": [
    "## üö© GN0037\n",
    "\n",
    "1. Segundo nombre diferente al registrado en la BDUA.\n",
    "2. GN0037(CARLOS);\n",
    "3. GN0037(SegundoNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a49704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 119 registros\n",
      "Antes: Df_NS_Envio = 249 registros\n",
      "Despu√©s: Df_NS_NEG = 102 registros\n",
      "Despu√©s: Df_NS_Envio = 266 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "261                    91  EPS025         RC                    1118583567   \n",
      "262                    98  EPS025         RC                    1118583560   \n",
      "263                    99  EPS025         RC                    1246129376   \n",
      "264                   100  EPS025         RC                    1115692863   \n",
      "265                   101  EPS025         RC                    1118583418   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "261             PONGUTA            GUTIERREZ            OSIRIS   \n",
      "262               MARIN               CACHAY           MILEIDY   \n",
      "263            MONTA¬•EZ               SUAREZ            ASHLEY   \n",
      "264              MORENO               DEDIOS            MARIAM   \n",
      "265              FLOREZ                GOMEZ            EITHAN   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "261             ALANNA           11/09/2025     85  ...        OSIRIS   \n",
      "262            XIOMARA           14/09/2025     85  ...       MILEIDY   \n",
      "263             SOPHIA           25/09/2025     85  ...        ASHLEY   \n",
      "264            CELESTE           18/09/2025     85  ...        MARIAM   \n",
      "265             ABDIEL           08/08/2025     85  ...        EITHAN   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "261        ALANNA           NaN           NaN           NaN           NaN   \n",
      "262       XIOMARA           NaN           NaN           NaN           NaN   \n",
      "263        SOPHIA           NaN           NaN           NaN           NaN   \n",
      "264       CELESTE           NaN           NaN           NaN           NaN   \n",
      "265        ABDIEL           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "261           NaN  GN0034(PONGUTA);GN0035(GUTIERREZ);GN0036(OSIRI...   \n",
      "262           NaN  GN0034(MARIN);GN0035(CACHAY);GN0036(MILEIDY);G...   \n",
      "263           NaN  GN0034(MONTA¬•EZ);GN0035(SUAREZ);GN0036(ASHLEY)...   \n",
      "264           NaN  GN0034(MORENO);GN0035(DEDIOS);GN0036(MARIAM);G...   \n",
      "265           NaN  GN0034(FLOREZ);GN0035(GOMEZ);GN0036(EITHAN);GN...   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "261       EPS025                        AC  \n",
      "262       EPS025                        AC  \n",
      "263       EPS025                        AC  \n",
      "264       EPS025                        AC  \n",
      "265       EPS025                        AC  \n",
      "\n",
      "[266 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0037(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0037 (Segundo Nombre diferente al registrado en la BDUA).\n",
    "    - Extrae el Segundo nombre correcto de la glosa GN0037(SegundoNomBDUA);\n",
    "    - Actualiza AFL_SEGUNDO_NOMBRE con ese valor;\n",
    "    - Elimina la glosa GN0037(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0037\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0037\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer Segundo nombre y actualizar AFL_SEGUNDO_NOMBRE\n",
    "    def extract_segundo_nombre(glosa):\n",
    "        match = re.search(r'GN0037\\(([^)]+)\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_SEGUNDO_NOMBRE'] = df_gn['Glosa_2'].apply(extract_segundo_nombre)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0037(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0037\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0037(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200a06a",
   "metadata": {},
   "source": [
    "## üö© GN0049\n",
    "1. Fecha de nacimiento diferente al registrado en la BDUA.\n",
    "2. GN0049(13/12/1993);\n",
    "3. GN0049(FechNacBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "052a635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 102 registros\n",
      "Antes: Df_NS_Envio = 266 registros\n",
      "Despu√©s: Df_NS_NEG = 95 registros\n",
      "Despu√©s: Df_NS_Envio = 273 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "268                  2238  EPS025         TI                    1222121081   \n",
      "269                   104  EPS025         RC                    1246129368   \n",
      "270                    65  EPS025         RC                    1222147888   \n",
      "271                    69  EPS025         RC                    1222147937   \n",
      "272                    64  EPS025         RC                    1246129368   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "268             HURTADO                ORTIZ           JHORIAN   \n",
      "269             TARACHE              LINARES            AILANY   \n",
      "270         GUACARAPARE               PONARE              ALAN   \n",
      "271               VIVAS                 LUNA         KARIANGEL   \n",
      "272             TARACHE              LINARES            AILANY   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "268          ALEXANDER           18/02/2015     85  ...            CC   \n",
      "269            ZULIETH           13/09/2025     85  ...       TARACHE   \n",
      "270             FABIAN           05/09/2025     85  ...          ALAN   \n",
      "271            JOSELYM           18/09/2025     85  ...     KARIANGEL   \n",
      "272            ZULIETH           13/09/2025     85  ...        AILANY   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "268       9433186             B             2           NaN           NaN   \n",
      "269       LINARES           NaN           NaN           NaN           NaN   \n",
      "270        FABIAN           NaN           NaN           NaN           NaN   \n",
      "271       JOSELYM           NaN           NaN           NaN           NaN   \n",
      "272       ZULIETH           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "268           NaN                                GN0049(18/02/2015);   \n",
      "269           NaN  GN0034(TARACHE);GN0035(LINARES);GN0049(13/09/2...   \n",
      "270           NaN    GN0036(ALAN);GN0037(FABIAN);GN0049(05/09/2025);   \n",
      "271           NaN  GN0036(KARIANGEL);GN0037(JOSELYM);GN0049(18/09...   \n",
      "272           NaN  GN0034(TARACHE);GN0035(LINARES);GN0036(AILANY)...   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "268       EPS025                        AC  \n",
      "269       EPS025                        AC  \n",
      "270       EPS025                        AC  \n",
      "271       EPS025                        AC  \n",
      "272       EPS025                        AC  \n",
      "\n",
      "[273 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0049(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0049 (Fecha de nacimiento diferente al registrado en la BDUA).\n",
    "    - Extrae la fecha correcta de la glosa GN0049(FechNacBDUA);\n",
    "    - Actualiza AFL_FECHA_NACIMIENTO con ese valor;\n",
    "    - Elimina la glosa GN0049(...) de Glosa_2;\n",
    "    - Recalcula Code_Glosa y No_Glosa;\n",
    "    - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "    - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0049\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0049\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer fecha y actualizar AFL_FECHA_NACIMIENTO\n",
    "    def extract_fecha(glosa):\n",
    "        match = re.search(r'GN0049\\((\\d{2}/\\d{2}/\\d{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "\n",
    "    df_gn['AFL_FECHA_NACIMIENTO'] = df_gn['Glosa_2'].apply(extract_fecha)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0049(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0049\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0049(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501a9a9",
   "metadata": {},
   "source": [
    "## üö© GN0059\n",
    "1. Afiliado con datos certificados RNEC, no se puede aplicar esta novedad.\n",
    "2. GN0059;\n",
    "3. Los datos b√°sicos del afiliado reportados en el archivo no corresponden con los certificados por la Registraduria Nacional del Estado Civil, se hace necesario revisar el documento del usuario para corroborar la informaci√≥n en caso de coincidir notificar inconsistencia ante ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00fd5d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 95 registros\n",
      "Antes: Df_NS_Envio = 273 registros\n",
      "Antes: DF_059_169 = 0 registros\n",
      "Despu√©s: Df_NS_NEG = 93 registros\n",
      "Despu√©s: Df_NS_Envio = 275 registros\n",
      "Despu√©s: DF_059_169 = 15 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "270                    65  EPS025         RC                    1222147888   \n",
      "271                    69  EPS025         RC                    1222147937   \n",
      "272                    64  EPS025         RC                    1246129368   \n",
      "273                    52  EPS025         RC                    1222147416   \n",
      "274                    53  EPS025         RC                    1115921009   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "270         GUACARAPARE               PONARE              ALAN   \n",
      "271               VIVAS                 LUNA         KARIANGEL   \n",
      "272             TARACHE              LINARES            AILANY   \n",
      "273             HOLGUIN              FAJARDO          MADISSON   \n",
      "274              CASTRO                 DIAZ            EITHAN   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "270             FABIAN           05/09/2025     85  ...          ALAN   \n",
      "271            JOSELYM           18/09/2025     85  ...     KARIANGEL   \n",
      "272            ZULIETH           13/09/2025     85  ...        AILANY   \n",
      "273              NAOMI           07/05/2025     85  ...            RC   \n",
      "274             JAVIER           27/01/2025     85  ...            RC   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "270        FABIAN           NaN           NaN           NaN           NaN   \n",
      "271       JOSELYM           NaN           NaN           NaN           NaN   \n",
      "272       ZULIETH           NaN           NaN           NaN           NaN   \n",
      "273    1118583296    07/05/2025             1           NaN           NaN   \n",
      "274    1115921012    27/01/2025             1           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "270           NaN    GN0036(ALAN);GN0037(FABIAN);GN0049(05/09/2025);   \n",
      "271           NaN  GN0036(KARIANGEL);GN0037(JOSELYM);GN0049(18/09...   \n",
      "272           NaN  GN0034(TARACHE);GN0035(LINARES);GN0036(AILANY)...   \n",
      "273           NaN                                            GN0059;   \n",
      "274           NaN                                            GN0059;   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "270       EPS025                        AC  \n",
      "271       EPS025                        AC  \n",
      "272       EPS025                        AC  \n",
      "273       EPS025                        AC  \n",
      "274       EPS025                        AC  \n",
      "\n",
      "[275 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0059 (‚ÄúAfiliado con datos certificados RNEC, no se puede aplicar esta novedad‚Äù).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0059;'.\n",
    "      2. Copia todos esos registros a `DF_059_169` (para verificaci√≥n manual en auditor√≠a).\n",
    "      3. Elimina la subcadena 'GN0059;' de `Glosa_2`.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vac√≠o.\n",
    "      5. De los registros filtrados, mueve a `Df_NS_Envio` aquellos que pasaron a `No_Glosa == 0`.\n",
    "      6. Elimina las columnas auxiliares ['Glosa_2','No_Glosa','Code_Glosa'] \n",
    "         en los DataFrames `Df_NS_Envio` y `DF_059_169`.\n",
    "      7. Imprime conteos de registros antes y despu√©s en cada DataFrame.\n",
    "\n",
    "    Nota:\n",
    "      - La validaci√≥n en Registradur√≠a o Migraci√≥n (web scraping con captcha) \n",
    "        queda comentada para futura implementaci√≥n.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "        DF_059_169 (pd.DataFrame): DataFrame destino para auditor√≠a GN0059.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated)\n",
    "    \"\"\"\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0059;\n",
    "    mask_gn = Df_NS_NEG['Glosa_2'].str.contains('GN0059;', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn].copy()\n",
    "\n",
    "    # 2) Copiar todos a DF_059_169 para verificaci√≥n manual\n",
    "    df_059 = df_gn.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    DF_059_169_updated = pd.concat([DF_059_169, df_059], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0059;' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace('GN0059;', '', regex=False)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que pasaron a No_Glosa == 0 y ten√≠an GN0059\n",
    "    mask_move = mask_gn & (Df_NS_NEG_updated['No_Glosa'] == 0)\n",
    "    df_move = Df_NS_NEG_updated[mask_move].copy()\n",
    "    df_move = df_move.drop(columns=['Glosa_2','No_Glosa','Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # 6) Quedarse en Df_NS_NEG los registros restantes\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_move].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_059_169 = {len(DF_059_169_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0059(Df_NS_NEG, Df_NS_Envio, DF_059_169)\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58464ac",
   "metadata": {},
   "source": [
    "## üö© GN0079 \n",
    "\n",
    "1. Afiliado Cotizante, Cabeza de familia¬† o Adicional, diligensia la condici√≥n de estudiante o discapacitado.\n",
    "\n",
    "   **1. Se suma un dia a la fecha de la glosa**\n",
    "   \n",
    "   2. GN0079(C|AC|01/05/2016);\n",
    "   3. GN0079(TipoAfil|EstadoAfil|FechaInicioCondicion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60eaf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando clean_gn0079 ---\n",
      "Registros iniciales en Df_NS_NEG: 93\n",
      "Registros iniciales en Df_NS_Envio: 275\n",
      "\n",
      "Registros en Df_NS_NEG despu√©s de limpieza: 92\n",
      "Registros en Df_NS_Envio despu√©s de a√±adir: 276\n",
      "--- Finalizando clean_gn0079 ---\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "271                    69  EPS025         RC                    1222147937   \n",
      "272                    64  EPS025         RC                    1246129368   \n",
      "273                    52  EPS025         RC                    1222147416   \n",
      "274                    53  EPS025         RC                    1115921009   \n",
      "275                   257  EPS025         CC                      46379657   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "271               VIVAS                 LUNA         KARIANGEL   \n",
      "272             TARACHE              LINARES            AILANY   \n",
      "273             HOLGUIN              FAJARDO          MADISSON   \n",
      "274              CASTRO                 DIAZ            EITHAN   \n",
      "275                LARA              CAMARGO              AURA   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "271            JOSELYM           18/09/2025     85  ...     KARIANGEL   \n",
      "272            ZULIETH           13/09/2025     85  ...        AILANY   \n",
      "273              NAOMI           07/05/2025     85  ...            RC   \n",
      "274             JAVIER           27/01/2025     85  ...            RC   \n",
      "275            CENAIDA           21/01/1970     85  ...             D   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "271       JOSELYM           NaN           NaN           NaN           NaN   \n",
      "272       ZULIETH           NaN           NaN           NaN           NaN   \n",
      "273    1118583296    07/05/2025             1           NaN           NaN   \n",
      "274    1115921012    27/01/2025             1           NaN           NaN   \n",
      "275           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "271           NaN  GN0036(KARIANGEL);GN0037(JOSELYM);GN0049(18/09...   \n",
      "272           NaN  GN0034(TARACHE);GN0035(LINARES);GN0036(AILANY)...   \n",
      "273           NaN                                            GN0059;   \n",
      "274           NaN                                            GN0059;   \n",
      "275           NaN                           GN0079(F|AC|01/02/2018);   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "271       EPS025                        AC  \n",
      "272       EPS025                        AC  \n",
      "273       EPS025                        AC  \n",
      "274       EPS025                        AC  \n",
      "275       EPS025                        AC  \n",
      "\n",
      "[276 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_gn0079(df_neg, df_envio):\n",
    "    \"\"\"\n",
    "    Procesa y limpia la glosa GN0079 de forma aislada y segura,\n",
    "    a√±adiendo los registros limpios a df_envio sin eliminar los existentes.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando clean_gn0079 ---\")\n",
    "    print(f\"Registros iniciales en Df_NS_NEG: {len(df_neg)}\")\n",
    "    print(f\"Registros iniciales en Df_NS_Envio: {len(df_envio)}\")\n",
    "    \n",
    "    # --- 1. Inicializaci√≥n ---\n",
    "    # Crear copias para trabajar de forma segura sin modificar los DFs originales.\n",
    "    df_neg_updated = df_neg.copy()\n",
    "    df_envio_updated = df_envio.copy()\n",
    "\n",
    "    # --- 2. Aislar y Procesar Registros con GN0079 ---\n",
    "    # Se crea una m√°scara para operar SOLO sobre los registros relevantes.\n",
    "    mask = df_neg_updated['Glosa_2'].str.contains('GN0079', na=False)\n",
    "    \n",
    "    # Si no hay nada que procesar, retornar.\n",
    "    if not mask.any():\n",
    "        print(\"No se encontraron registros con la glosa GN0079.\")\n",
    "        return df_neg_updated, df_envio_updated\n",
    "\n",
    "    # Funci√≥n para extraer fecha (la l√≥gica de la fecha se mantiene como la ten√≠as).\n",
    "    def extract_and_increment(glosa):\n",
    "        import re\n",
    "        match = re.search(r'GN0079\\([^|]+\\|[^|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            fecha = datetime.strptime(match.group(1), '%d/%m/%Y')\n",
    "            nueva_fecha = fecha # + timedelta(days=1)\n",
    "            return nueva_fecha.strftime('%d/%m/%Y')\n",
    "        return None\n",
    "\n",
    "    # Se aplican los cambios S√ìLO a las filas con la glosa GN0079.\n",
    "    df_neg_updated.loc[mask, 'FECHA_NOVEDAD'] = df_neg_updated.loc[mask, 'Glosa_2'].apply(extract_and_increment)\n",
    "    df_neg_updated.loc[mask, 'Glosa_2'] = df_neg_updated.loc[mask, 'Glosa_2'].str.replace(r'\\s*GN0079\\([^)]*\\);?', '', regex=True)\n",
    "    df_neg_updated.loc[mask, 'No_Glosa'] = df_neg_updated.loc[mask, 'Glosa_2'].str.count(';')\n",
    "    df_neg_updated.loc[mask, 'Code_Glosa'] = df_neg_updated.loc[mask, 'Glosa_2'].str.split(';').str[0].str[:6].replace('', '0')\n",
    "\n",
    "    # --- 3. Mover Registros Limpios ---\n",
    "    \n",
    "    # Se identifican los registros que, tras la limpieza, quedaron sin glosas.\n",
    "    mask_move = mask & (df_neg_updated['No_Glosa'] == 0)\n",
    "    \n",
    "    if mask_move.any():\n",
    "        df_to_move = df_neg_updated[mask_move].copy()\n",
    "        \n",
    "        # Se eliminan las columnas de glosas antes de mover.\n",
    "        df_to_move_clean = df_to_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "        \n",
    "        # **LA CORRECCI√ìN CLAVE:** Se usa pd.concat para A√ëADIR los registros.\n",
    "        df_envio_updated = pd.concat([df_envio_updated, df_to_move_clean], ignore_index=True)\n",
    "        \n",
    "        # Se eliminan de df_neg_updated los registros que ya se movieron.\n",
    "        df_neg_updated = df_neg_updated.drop(df_neg_updated[mask_move].index)\n",
    "\n",
    "    # Imprimir conteos finales\n",
    "    print(f\"\\nRegistros en Df_NS_NEG despu√©s de limpieza: {len(df_neg_updated)}\")\n",
    "    print(f\"Registros en Df_NS_Envio despu√©s de a√±adir: {len(df_envio_updated)}\")\n",
    "    print(f\"--- Finalizando clean_gn0079 ---\")\n",
    "\n",
    "    return df_neg_updated, df_envio_updated\n",
    "\n",
    "# Aplicar la fuNSi√≥n al DataFrame existente\n",
    "Df_NS_NEG, Df_NS_Envio = clean_gn0079(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdaed3",
   "metadata": {},
   "source": [
    "## üö© GN0084\n",
    "1. Fecha de novedad inferior a la fecha de afiliaci√≥n a la entidad actual.\n",
    "2. GN0084(grp_fml_p|20/11/2016);  GN0084(ctz_apr|01/12/2016); GN0084(cnd_afl|01/11/2016);\n",
    "3. GN0084(FuenteValidacion|FechaInicioCondicion);\n",
    "4. La entidad debe reportar la novedad un d√¨a despu√©s de la fecha mostrada en la informaci√≥n de la glosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb101b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 92 registros\n",
      "Antes: Df_NS_Envio = 276 registros\n",
      "Despu√©s: Df_NS_NEG = 82 registros\n",
      "Despu√©s: Df_NS_Envio = 286 registros\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "281                  2122  EPS025         TI                    1115865748   \n",
      "282                  2258  EPS025         RC                    1115870702   \n",
      "283                  2274  EPS025         RC                    1118651869   \n",
      "284                  2339  EPS025         RC                    1222141425   \n",
      "285                  2391  EPS025         CC                    1118528634   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "281           ATEHORTUA             GUALDRON             WENDY   \n",
      "282              RINCON              BENITEZ            SAMARA   \n",
      "283               PEREZ                ROJAS            ZAMARA   \n",
      "284              RINCON                ABRIL          SOLLIVAN   \n",
      "285             NARANJO               BURGOS             MAYRA   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_1_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...            CC   \n",
      "1                  NaN           20/09/2025     85  ...          ANNY   \n",
      "2                  NaN           27/09/2025     85  ...          EIMY   \n",
      "3                  NaN           28/09/2025     85  ...           IZA   \n",
      "4                  NaN           30/09/2025     85  ...        ALIANI   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "281              NICOL           05/08/2016     85  ...  852500042203   \n",
      "282          ANTONELLA           04/07/2023     85  ...             F   \n",
      "283              ESTER           21/08/2024     85  ...             0   \n",
      "284          ALEXANDER           05/02/2022     85  ...             2   \n",
      "285          ALEJANDRA           09/04/2003     85  ...             0   \n",
      "\n",
      "    COD_2_NOVEDAD COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD  \\\n",
      "0      1118541125             B             2           NaN           NaN   \n",
      "1           SOFIA           NaN           NaN           NaN           NaN   \n",
      "2         CELESTE           NaN           NaN           NaN           NaN   \n",
      "3        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "4        VICTORIA           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "281  852500042203           NaN           NaN           NaN           NaN   \n",
      "282             2           A01             5             1            06   \n",
      "283            85           125           NaN           NaN           NaN   \n",
      "284            85           001           NaN           NaN           NaN   \n",
      "285            85           225           NaN           NaN           NaN   \n",
      "\n",
      "    COD_7_NOVEDAD                                              Glosa  \\\n",
      "0             NaN  GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...   \n",
      "1             NaN                                 GN0031(|||||||||);   \n",
      "2             NaN                                 GN0031(|||||||||);   \n",
      "3             NaN                                 GN0031(|||||||||);   \n",
      "4             NaN                                 GN0031(|||||||||);   \n",
      "..            ...                                                ...   \n",
      "281           NaN                        GN0084(cnd_afl|01/11/2025);   \n",
      "282           NaN                    GN0084(cnd_afl_sbs|01/10/2025);   \n",
      "283           NaN                        GN0084(cnd_afl|19/09/2025);   \n",
      "284           NaN                        GN0084(cnd_afl|19/09/2025);   \n",
      "285           NaN                        GN0084(cnd_afl|24/09/2025);   \n",
      "\n",
      "    ENT_ID_ADRES TPS_EST_AFL_ID_from_adres  \n",
      "0         EPS025                        AC  \n",
      "1         EPS025                        AC  \n",
      "2         EPS025                        AC  \n",
      "3         EPS025                        AC  \n",
      "4         EPS025                        AC  \n",
      "..           ...                       ...  \n",
      "281       EPS025                        AC  \n",
      "282       EPS025                        AC  \n",
      "283       EPS025                        AC  \n",
      "284       EPS025                        AC  \n",
      "285       EPS025                        AC  \n",
      "\n",
      "[286 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0084(Df_NS_NEG, Df_NS_Envio):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0084 en Df_NS_NEG:\n",
    "      - Extrae la fecha de la glosa GN0084(FuenteValidacion|FechaInicioCondicion);\n",
    "      - Actualiza FECHA_NOVEDAD a FechaInicioCondicion + 1 d√≠a;\n",
    "      - Elimina la glosa GN0084(...) de Glosa_2;\n",
    "      - Recalcula Code_Glosa y No_Glosa;\n",
    "      - Si No_Glosa == 0, mueve el registro a Df_NS_Envio (elimina columnas auxiliares);\n",
    "      - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o general.\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, Df_NS_Envio_updated)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0084\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0084\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Extraer fecha y actualizar FECHA_NOVEDAD\n",
    "    def extract_and_increment(glosa):\n",
    "        match = re.search(r'GN0084\\([^\\|]+\\|([0-9]{2}/[0-9]{2}/[0-9]{4})\\);', str(glosa))\n",
    "        if match:\n",
    "            fecha = datetime.strptime(match.group(1), '%d/%m/%Y')\n",
    "            nueva_fecha = fecha + timedelta(days=1)\n",
    "            return nueva_fecha.strftime('%d/%m/%Y')\n",
    "        return None\n",
    "\n",
    "    df_gn['FECHA_NOVEDAD'] = df_gn['Glosa_2'].apply(extract_and_increment)\n",
    "\n",
    "    # 3) Eliminar la glosa GN0084(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0084\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # 6) Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0084(Df_NS_NEG, Df_NS_Envio)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321422",
   "metadata": {},
   "source": [
    "## üö© GN0088\n",
    "1. Estado actual no valido para la novedad o igual al registrado en BDUA.\n",
    "2. GN0088(F|RE|01/12/2015)\n",
    "3. GN0088(TipoAfiliado|Estadp|FechaInicioCondicion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70c7848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 82\n",
      "Antes: Df_NS_Envio = 286\n",
      "Antes: DF_No_Enviar = 40\n",
      "Despu√©s: Df_NS_NEG = 82\n",
      "Despu√©s: Df_NS_Envio = 286\n",
      "Despu√©s: DF_No_Enviar = 40\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "281                  2122  EPS025         TI                    1115865748   \n",
      "282                  2258  EPS025         RC                    1115870702   \n",
      "283                  2274  EPS025         RC                    1118651869   \n",
      "284                  2339  EPS025         RC                    1222141425   \n",
      "285                  2391  EPS025         CC                    1118528634   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "281           ATEHORTUA             GUALDRON             WENDY   \n",
      "282              RINCON              BENITEZ            SAMARA   \n",
      "283               PEREZ                ROJAS            ZAMARA   \n",
      "284              RINCON                ABRIL          SOLLIVAN   \n",
      "285             NARANJO               BURGOS             MAYRA   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_2_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...    1118541125   \n",
      "1                  NaN           20/09/2025     85  ...         SOFIA   \n",
      "2                  NaN           27/09/2025     85  ...       CELESTE   \n",
      "3                  NaN           28/09/2025     85  ...      VICTORIA   \n",
      "4                  NaN           30/09/2025     85  ...      VICTORIA   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "281              NICOL           05/08/2016     85  ...  852500042203   \n",
      "282          ANTONELLA           04/07/2023     85  ...             2   \n",
      "283              ESTER           21/08/2024     85  ...            85   \n",
      "284          ALEXANDER           05/02/2022     85  ...            85   \n",
      "285          ALEJANDRA           09/04/2003     85  ...            85   \n",
      "\n",
      "    COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD COD_7_NOVEDAD  \\\n",
      "0               B             2           NaN           NaN           NaN   \n",
      "1             NaN           NaN           NaN           NaN           NaN   \n",
      "2             NaN           NaN           NaN           NaN           NaN   \n",
      "3             NaN           NaN           NaN           NaN           NaN   \n",
      "4             NaN           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "281           NaN           NaN           NaN           NaN           NaN   \n",
      "282           A01             5             1            06           NaN   \n",
      "283           125           NaN           NaN           NaN           NaN   \n",
      "284           001           NaN           NaN           NaN           NaN   \n",
      "285           225           NaN           NaN           NaN           NaN   \n",
      "\n",
      "                                                 Glosa ENT_ID_ADRES  \\\n",
      "0    GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...       EPS025   \n",
      "1                                   GN0031(|||||||||);       EPS025   \n",
      "2                                   GN0031(|||||||||);       EPS025   \n",
      "3                                   GN0031(|||||||||);       EPS025   \n",
      "4                                   GN0031(|||||||||);       EPS025   \n",
      "..                                                 ...          ...   \n",
      "281                        GN0084(cnd_afl|01/11/2025);       EPS025   \n",
      "282                    GN0084(cnd_afl_sbs|01/10/2025);       EPS025   \n",
      "283                        GN0084(cnd_afl|19/09/2025);       EPS025   \n",
      "284                        GN0084(cnd_afl|19/09/2025);       EPS025   \n",
      "285                        GN0084(cnd_afl|24/09/2025);       EPS025   \n",
      "\n",
      "    TPS_EST_AFL_ID_from_adres Where  \n",
      "0                          AC   NaN  \n",
      "1                          AC   NaN  \n",
      "2                          AC   NaN  \n",
      "3                          AC   NaN  \n",
      "4                          AC   NaN  \n",
      "..                        ...   ...  \n",
      "281                        AC   NaN  \n",
      "282                        AC   NaN  \n",
      "283                        AC   NaN  \n",
      "284                        AC   NaN  \n",
      "285                        AC   NaN  \n",
      "\n",
      "[286 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "def process_gn0088(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0088 (‚ÄúEstado actual no v√°lido para la novedad o igual al registrado en BDUA‚Äù).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0088('.\n",
    "      2. Copia esos registros a `DF_No_Enviar` con el motivo \"Estado actual no v√°lido para la novedad o igual al registrado en BDUA.\"\n",
    "      3. Elimina la subcadena 'GN0088(...)' de `Glosa_2`.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vac√≠o.\n",
    "      5. Mueve a `Df_NS_Envio` todos los registros de `Df_NS_NEG` con `No_Glosa == 0`\n",
    "         (ya no tienen glosas) y asigna \"Glosas\" en la columna `Where`.\n",
    "      6. Imprime los conteos de registros antes y despu√©s en cada DataFrame.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        Df_NS_Envio (pd.DataFrame): DataFrame destino para env√≠o sin glosas.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para registros no enviados.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - Df_NS_NEG_updated (pd.DataFrame): Registros sin mover (glosas a√∫n pendientes).\n",
    "            - Df_NS_Envio_updated (pd.DataFrame): Registros ya sin glosas listos para env√≠o.\n",
    "            - DF_No_Enviar_updated (pd.DataFrame): Registros movidos con motivo.\n",
    "    \"\"\"\n",
    "    # Conteos iniciales\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0088\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0088\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Copiar registros a DF_No_Enviar con el motivo\n",
    "    df_gn['Motivo'] = \"Estado actual no v√°lido para la novedad o igual al registrado en BDUA.\"\n",
    "    df_no_enviar = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_no_enviar], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0088(...)' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace(r'GN0088\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = Df_NS_NEG_updated['No_Glosa'] == 0\n",
    "    df_move = Df_NS_NEG_updated[mask_zero].copy()\n",
    "    df_move['Where'] = \"Glosas\"\n",
    "    df_move = df_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_move], ignore_index=True)\n",
    "\n",
    "    # Quedarse en Df_NS_NEG s√≥lo los que a√∫n tienen glosas\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask_zero].copy()\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Uso de ejemplo:\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0088(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar)\n",
    "\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595d9cb",
   "metadata": {},
   "source": [
    "## üö© GN0122\n",
    "1. Grupo poblacional reportado, igual al registrado en la BDUA.\n",
    "2. GN0122(ST|1|5|2|B02|28/03/2022);\n",
    "3. GN0122(ModalidadSubsidio|NivelSisben|GrupoPoblacional|Metodologia|SubgrupoPoblacional|InicioCondicionAfiliacion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "10653fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 82\n",
      "Antes: DF_No_Enviar = 40\n",
      "Despu√©s: Df_NS_NEG = 67\n",
      "Despu√©s: DF_No_Enviar = 55\n"
     ]
    }
   ],
   "source": [
    "def process_gn0122(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0122 (‚ÄúGrupo poblacional reportado, igual al registrado en la BDUA.‚Äù).\n",
    "\n",
    "    Pasos:\n",
    "      1. Filtra todos los registros de `Df_NS_NEG` cuya columna 'Glosa_2' contiene 'GN0122('.\n",
    "      2. Copia esos registros a `DF_No_Enviar` con el motivo correspondiente.\n",
    "      3. Elimina la subcadena 'GN0122(...)' de Glosa_2.\n",
    "      4. Recalcula:\n",
    "         - `No_Glosa` como el conteo de ';' en Glosa_2.\n",
    "         - `Code_Glosa` como los primeros 6 caracteres de Glosa_2 o '0' si queda vac√≠o.\n",
    "      5. Devuelve los tres DataFrames actualizados.\n",
    "\n",
    "    Args:\n",
    "        Df_NS_NEG (pd.DataFrame): DataFrame original con glosas pendientes.\n",
    "        DF_No_Enviar (pd.DataFrame): DataFrame destino para registros no enviados.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Df_NS_NEG_final, DF_No_Enviar_updated)\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0122\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0122\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Copiar registros a DF_No_Enviar con el motivo\n",
    "    df_gn['Motivo'] = \"Grupo poblacional reportado, igual al registrado en la BDUA.\"\n",
    "    df_no_enviar = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_no_enviar], ignore_index=True)\n",
    "\n",
    "    # 3) Eliminar la glosa 'GN0122(...)' de Glosa_2\n",
    "    Df_NS_NEG_updated = Df_NS_NEG.copy()\n",
    "    Df_NS_NEG_updated['Glosa_2'] = Df_NS_NEG_updated['Glosa_2'].str.replace(r'GN0122\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # 4) Recalcular No_Glosa y Code_Glosa\n",
    "    Df_NS_NEG_updated['No_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str.count(';')\n",
    "    Df_NS_NEG_updated['Code_Glosa'] = Df_NS_NEG_updated['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # 5) Quedarse en Df_NS_NEG s√≥lo los que a√∫n tienen glosas\n",
    "    Df_NS_NEG_final = Df_NS_NEG_updated[~mask].copy()\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, DF_No_Enviar_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0122(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2975f6f",
   "metadata": {},
   "source": [
    "## üö© GN0169\n",
    "1. Los datos del afiliado enviados no coinciden con los datos certificados por la RNEC.\n",
    "2. GN0169(|SEGUNDO APELLIDO|URDANETA|USDANETA||SEGUNDO NOMBRE|MANUEL||);\n",
    "   1. GN0169(|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad||SEGUNDO NOMBRE|DatoTablaReferencia|DatoEnviadoEntidad|);\n",
    "3. GN0169(PRIMER APELLIDO|GONZALEZ|AMAYA|SEGUNDO APELLIDO|PUSHAINA|GUERRA|||);\n",
    "   1. GN0169(PRIMER APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|SEGUNDO APELLIDO|DatoTablaReferencia|DatoEnviadoEntidad|||);\n",
    "4. GN0169(||||||);\n",
    "   1. GN0169(DocumentoNoExisteTablaReferencia); \n",
    "5. GN0169(||||FECHA NACIMIENTO|19/07/2016|01/07/2016);\n",
    "6. GN0169(||||SEXO|F|M);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81aab393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando process_gn0169 ---\n",
      "Antes: Df_NS_NEG = 67\n",
      "Antes: Df_NS_Envio = 286\n",
      "Antes: DF_059_169 = 15\n",
      "\n",
      "Despu√©s: Df_NS_NEG = 44\n",
      "Despu√©s: Df_NS_Envio = 309\n",
      "Despu√©s: DF_059_169 = 26\n",
      "    NUM_SOLICITUD_NOVEDAD  ENT_ID TPS_IDN_ID HST_IDN_NUMERO_IDENTIFICACION  \\\n",
      "0                    2228  EPS025         RC                    1246128326   \n",
      "1                      81  EPS025         RC                    1118127344   \n",
      "2                      82  EPS025         RC                    1222147970   \n",
      "3                      84  EPS025         RC                    1246129381   \n",
      "4                      89  EPS025         RC                    1117327408   \n",
      "..                    ...     ...        ...                           ...   \n",
      "304                   141  EPS025         PT                       5926892   \n",
      "305                    14  EPS025         CN                25094010327229   \n",
      "306                    13  EPS025         CN                25093310318112   \n",
      "307                    18  EPS025         CN                25098310328031   \n",
      "308                   143  EPS025         RC                    1246129376   \n",
      "\n",
      "    AFL_PRIMER_APELLIDO AFL_SEGUNDO_APELLIDO AFL_PRIMER_NOMBRE  \\\n",
      "0               ESTEVES             GUERRERO              LIAN   \n",
      "1                  RIOS                RIA?O           HIJO DE   \n",
      "2               CAHUE?O               RIVERA           HIJO DE   \n",
      "3              COCINERO            CASTA?EDA           HIJO DE   \n",
      "4               CAMARGO              CAMARGO           HIJO DE   \n",
      "..                  ...                  ...               ...   \n",
      "304             SMILIER            RODRIGUEZ           YOISBER   \n",
      "305             CAHUE¬•O               RIVERA           HIJO DE   \n",
      "306                RIOS                RIA¬•O           HIJO DE   \n",
      "307            COCINERO            CASTA¬•EDA           HIJO DE   \n",
      "308            MONTA¬•EZ               SUAREZ            ASHLEY   \n",
      "\n",
      "    AFL_SEGUNDO_NOMBRE AFL_FECHA_NACIMIENTO DPR_ID  ... COD_2_NOVEDAD  \\\n",
      "0               ADRIAN           18/03/2022     85  ...    1118541125   \n",
      "1                  NaN           20/09/2025     85  ...         SOFIA   \n",
      "2                  NaN           27/09/2025     85  ...       CELESTE   \n",
      "3                  NaN           28/09/2025     85  ...      VICTORIA   \n",
      "4                  NaN           30/09/2025     85  ...      VICTORIA   \n",
      "..                 ...                  ...    ...  ...           ...   \n",
      "304             FABIAN           11/10/2014     85  ...       JIMENEZ   \n",
      "305                NaN           27/09/2025     85  ...    1222147970   \n",
      "306                NaN           20/09/2025     85  ...    1118127344   \n",
      "307                NaN           28/09/2025     85  ...    1246129381   \n",
      "308             SOPHIA           25/09/2025     85  ...        SUAREZ   \n",
      "\n",
      "    COD_3_NOVEDAD COD_4_NOVEDAD COD_5_NOVEDAD COD_6_NOVEDAD COD_7_NOVEDAD  \\\n",
      "0               B             2           NaN           NaN           NaN   \n",
      "1             NaN           NaN           NaN           NaN           NaN   \n",
      "2             NaN           NaN           NaN           NaN           NaN   \n",
      "3             NaN           NaN           NaN           NaN           NaN   \n",
      "4             NaN           NaN           NaN           NaN           NaN   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "304           NaN           NaN           NaN           NaN           NaN   \n",
      "305    27/09/2025             0           NaN           NaN           NaN   \n",
      "306    20/09/2025             0           NaN           NaN           NaN   \n",
      "307    28/09/2025             0           NaN           NaN           NaN   \n",
      "308           NaN           NaN           NaN           NaN           NaN   \n",
      "\n",
      "                                                 Glosa ENT_ID_ADRES  \\\n",
      "0    GN0009(S|EPS025|25/08/2025|85|230|B|AC|25/08/2...       EPS025   \n",
      "1                                   GN0031(|||||||||);       EPS025   \n",
      "2                                   GN0031(|||||||||);       EPS025   \n",
      "3                                   GN0031(|||||||||);       EPS025   \n",
      "4                                   GN0031(|||||||||);       EPS025   \n",
      "..                                                 ...          ...   \n",
      "304    GN0169(|SEGUNDO APELLIDO|RODRIGUEZ|JIMENEZ|||);       EPS025   \n",
      "305                    GN0034(CAHUE¬•O);GN0169(||||||);       EPS025   \n",
      "306                      GN0035(RIA¬•O);GN0169(||||||);       EPS025   \n",
      "307                  GN0035(CASTA¬•EDA);GN0169(||||||);       EPS025   \n",
      "308  GN0059;GN0034(MONTA¬•EZ);GN0035(SUAREZ);GN0169(...       EPS025   \n",
      "\n",
      "    TPS_EST_AFL_ID_from_adres Where  \n",
      "0                          AC   NaN  \n",
      "1                          AC   NaN  \n",
      "2                          AC   NaN  \n",
      "3                          AC   NaN  \n",
      "4                          AC   NaN  \n",
      "..                        ...   ...  \n",
      "304                        AC   NaN  \n",
      "305                        AC   NaN  \n",
      "306                        AC   NaN  \n",
      "307                        AC   NaN  \n",
      "308                        AC   NaN  \n",
      "\n",
      "[309 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169):\n",
    "    \"\"\"\n",
    "    Procesa de forma aislada la glosa GN0169 (‚ÄúDatos del afiliado no coinciden con RNEC‚Äù).\n",
    "    Esta versi√≥n corregida asegura que solo se modifiquen los registros\n",
    "    relevantes y no se pierdan datos de otros DataFrames.\n",
    "    \"\"\"\n",
    "    print(f\"--- Iniciando process_gn0169 ---\")\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)}\")\n",
    "    print(f\"Antes: DF_059_169 = {len(DF_059_169)}\")\n",
    "\n",
    "    # --- 1. Inicializaci√≥n y Aislamiento ---\n",
    "    DF_059_169_updated = DF_059_169.copy()\n",
    "    Df_NS_Envio_updated = Df_NS_Envio.copy()\n",
    "    \n",
    "    # Se crea una m√°scara para encontrar los registros con la glosa GN0169.\n",
    "    mask_gn169 = Df_NS_NEG['Glosa_2'].str.contains('GN0169', na=False)\n",
    "    \n",
    "    # Se dividen los registros de NEG en dos grupos: los que se procesar√°n y el resto.\n",
    "    df_gn = Df_NS_NEG[mask_gn169].copy()\n",
    "    df_resto = Df_NS_NEG[~mask_gn169].copy()\n",
    "\n",
    "    # Si no hay nada que procesar, se retorna todo como estaba.\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con la glosa GN0169.\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_059_169\n",
    "\n",
    "    # --- 2. Procesar SOLO los registros relevantes (df_gn) ---\n",
    "\n",
    "    # Copiar registros nuevos para auditor√≠a (esta l√≥gica no cambia).\n",
    "    key_cols = ['TPS_IDN_ID','HST_IDN_NUMERO_IDENTIFICACION','NOVEDAD']\n",
    "    if not DF_059_169_updated.empty:\n",
    "        existing_keys = DF_059_169_updated[key_cols].apply(tuple, axis=1)\n",
    "        new_mask = ~df_gn[key_cols].apply(tuple, axis=1).isin(existing_keys)\n",
    "        df_new_for_audit = df_gn[new_mask].copy()\n",
    "    else:\n",
    "        df_new_for_audit = df_gn.copy()\n",
    "    DF_059_169_updated = pd.concat([DF_059_169_updated, df_new_for_audit], ignore_index=True)\n",
    "\n",
    "    # Eliminar la glosa GN0169 y recalcular SOLO en el grupo de trabajo.\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'\\s*GN0169\\([^)]*\\);?', '', regex=True).str.strip()\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str.split(';').str[0].str[:6].replace('', '0')\n",
    "\n",
    "    # --- 3. Distribuir los registros procesados ---\n",
    "    \n",
    "    # Identificar los registros que quedaron sin glosas.\n",
    "    mask_move_to_envio = df_gn['No_Glosa'] == 0\n",
    "    df_move = df_gn[mask_move_to_envio].copy()\n",
    "    \n",
    "    # Identificar los que a√∫n tienen glosas.\n",
    "    df_keep = df_gn[~mask_move_to_envio].copy()\n",
    "\n",
    "    # Mover los registros limpios a Df_NS_Envio.\n",
    "    if not df_move.empty:\n",
    "        df_move_clean = df_move.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "        df_aligned = df_move_clean.reindex(columns=Df_NS_Envio.columns)\n",
    "        Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_aligned], ignore_index=True)\n",
    "\n",
    "    # --- 4. Reconstrucci√≥n Final ---\n",
    "    \n",
    "    # El nuevo Df_NS_NEG es la uni√≥n del resto (que nunca se toc√≥) y los que se procesaron pero se quedan.\n",
    "    Df_NS_NEG_final = pd.concat([df_resto, df_keep], ignore_index=True)\n",
    "\n",
    "    # Conteos finales\n",
    "    print(f\"\\nDespu√©s: Df_NS_NEG = {len(Df_NS_NEG_final)}\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)}\")\n",
    "    print(f\"Despu√©s: DF_059_169 = {len(DF_059_169_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_final, Df_NS_Envio_updated, DF_059_169_updated\n",
    "\n",
    "Df_NS_NEG, Df_NS_Envio, DF_059_169 = process_gn0169(Df_NS_NEG, Df_NS_Envio, DF_059_169)\n",
    "\n",
    "print(Df_NS_Envio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a5667",
   "metadata": {},
   "source": [
    "## üö© GN0340\n",
    "1. IPS Primaria no es v√°lida de acuerdo Registro Especial de Prestadores de Servicios de Salud.\n",
    "2. GN0340(Causal|IPSReportadaNovedad);\n",
    "3. Esta glosa presenta dos causales: \"1.\" La IPS reportada no se encuentra en el Registro Especial de Prestadores de Servicios de Salud \"2.\"  La IPS reportada es igual a la registrada en BDUA.\n",
    "4. GN0340;\n",
    "6. GN0340(1|171740068502);\n",
    "7. GN0340(2|500010149201); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7477ebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG     = 44 registros\n",
      "Antes: Df_NS_Envio   = 309 registros\n",
      "Antes: DF_No_Enviar  = 55 registros\n",
      "Antes: Total Registros = 408\n",
      "No se encontraron registros con GN0340.\n",
      "Despu√©s: Df_NS_NEG     = 44 registros\n",
      "Despu√©s: Df_NS_Envio   = 309 registros\n",
      "Despu√©s: DF_No_Enviar  = 55 registros\n",
      "Despu√©s: Total Registros = 408\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Motivo', 'cod_ent'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def process_gn0340(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0340 (IPS Primaria no v√°lida de acuerdo RIPS).\n",
    "    Asegura que la suma de los registros de salida sea igual a la de entrada.\n",
    "\n",
    "      - Filtrar GN0340.\n",
    "      - Extraer causal e IPSReportada (y un tercer valor para Causal 1).\n",
    "      - Para Causal=='2': limpiar glosa, recalcular, mover a DF_No_Enviar si No_Glosa==0.\n",
    "      - Para Causal=='1':\n",
    "          - Corregir 'COD_1_NOVEDAD' y 'COD_2_NOVEDAD' usando 'MNS_ID' y el mapeo.\n",
    "          - Limpiar glosa, recalcular No_Glosa y Code_Glosa.\n",
    "          - Mover a Df_NS_Envio si No_Glosa==0.\n",
    "          - Mantener en Df_NS_NEG si hay m√°s glosas.\n",
    "      - Los registros con GN0340 pero con Causal no reconocida (ni '1' ni '2')\n",
    "        o extracci√≥n fallida ser√°n enviados a DF_No_Enviar despu√©s de intentar limpiar la glosa.\n",
    "      - Mantener en Df_NS_NEG los no afectados y los con glosas restantes.\n",
    "    \"\"\"\n",
    "    initial_total_records = len(Df_NS_NEG) + len(Df_NS_Envio) + len(DF_No_Enviar)\n",
    "    print(f\"Antes: Df_NS_NEG     = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "    print(f\"Antes: Total Registros = {initial_total_records}\")\n",
    "\n",
    "    # 1) Separar registros a procesar (con GN0340) de los que no\n",
    "    mask_gn0340 = Df_NS_NEG['Glosa_2'].str.contains(r'GN0340\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask_gn0340].copy()\n",
    "    df_not_gn0340 = Df_NS_NEG[~mask_gn0340].copy() # Registros que no tienen GN0340\n",
    "\n",
    "    # Si no hay registros con GN0340, se retorna lo mismo que entr√≥\n",
    "    if df_gn.empty:\n",
    "        print(\"No se encontraron registros con GN0340.\")\n",
    "        print(f\"Despu√©s: Df_NS_NEG     = {len(Df_NS_NEG)} registros\")\n",
    "        print(f\"Despu√©s: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "        print(f\"Despu√©s: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "        print(f\"Despu√©s: Total Registros = {len(Df_NS_NEG) + len(Df_NS_Envio) + len(DF_No_Enviar)}\")\n",
    "        return Df_NS_NEG, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "    # 2) Extraer causal e IPSReportada (y el tercer valor para Causal 1)\n",
    "    glosa_pattern_general = r'GN0340\\((\\d+)\\|([0-9]+)(?:\\|([0-9]+))?\\);?'\n",
    "    extracted_data = df_gn['Glosa_2'].str.extract(glosa_pattern_general)\n",
    "    df_gn['Causal'] = extracted_data[0]\n",
    "    df_gn['IPSReportada'] = extracted_data[1]\n",
    "\n",
    "    # Inicializar DataFrames para los resultados intermedios\n",
    "    df_remaining_in_neg = pd.DataFrame(columns=Df_NS_NEG.columns)\n",
    "    \n",
    "    # 3) CASO 2: Causal == '2'\n",
    "    df_c2 = df_gn[df_gn['Causal']=='2'].copy()\n",
    "    if not df_c2.empty:\n",
    "        df_c2['Glosa_2'] = df_c2['Glosa_2'].str.replace(r'GN0340\\([^)]*\\);?', '', regex=True)\n",
    "        df_c2['Glosa_2'] = df_c2['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_c2['No_Glosa'] = df_c2['Glosa_2'].str.count(';')\n",
    "        df_c2['Code_Glosa'] = df_c2['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        mask_c2_to_no_enviar = (df_c2['No_Glosa']==0)\n",
    "        df_c2_move_to_no_enviar = df_c2[mask_c2_to_no_enviar].copy()\n",
    "        \n",
    "        if not df_c2_move_to_no_enviar.empty:\n",
    "            df_c2_move_to_no_enviar['Motivo'] = \"Caso 2. La IPS reportada es igual a la registrada en BDUA.\"\n",
    "            \n",
    "            # --- CORRECCI√ìN CLAVE AQU√ç ---\n",
    "            # Asegura que DF_No_Enviar tenga todas las columnas de df_c2_move_to_no_enviar\n",
    "            for col in df_c2_move_to_no_enviar.columns:\n",
    "                if col not in DF_No_Enviar.columns:\n",
    "                    DF_No_Enviar[col] = pd.NA # Define un valor predeterminado si es necesario\n",
    "            \n",
    "            # Concatena el DataFrame movido. pd.concat manejar√° autom√°ticamente\n",
    "            # las columnas que est√©n en DF_No_Enviar pero no en df_c2_move_to_no_enviar (rellenando con NaN)\n",
    "            DF_No_Enviar = pd.concat([DF_No_Enviar, df_c2_move_to_no_enviar], ignore_index=True)\n",
    "            # --- FIN CORRECCI√ìN CLAVE ---\n",
    "\n",
    "        df_c2_remain = df_c2[~mask_c2_to_no_enviar].copy()\n",
    "        df_remaining_in_neg = pd.concat([df_remaining_in_neg, df_c2_remain], ignore_index=True)\n",
    "\n",
    "    # 4) CASO 1: Causal == '1'\n",
    "    df_c1 = df_gn[df_gn['Causal']=='1'].copy()\n",
    "    if not df_c1.empty:\n",
    "        ips_map = {\n",
    "            \"001\":  \"850010014401\", \"010\":  \"850100019001\", \"015\":  \"850150042216\",\n",
    "            \"125\":  \"851250042210\", \"136\":  \"851360042215\", \"139\":  \"851390042204\",\n",
    "            \"162\":  \"851620042205\", \"225\":  \"852250042212\", \"230\":  \"852300042209\",\n",
    "            \"250\":  \"852500042203\", \"263\":  \"852630042208\", \"279\":  \"852790042217\",\n",
    "            \"300\":  \"853000042211\", \"315\":  \"853150042214\", \"325\":  \"853250042213\",\n",
    "            \"400\":  \"854000042207\", \"410\":  \"854100008001\", \"430\":  \"854300042206\",\n",
    "            \"440\":  \"854400042202\"\n",
    "        }\n",
    "        \n",
    "        df_c1.loc[:, 'COD_1_NOVEDAD'] = df_c1['MNS_ID'].map(ips_map)\n",
    "        df_c1.loc[:, 'COD_2_NOVEDAD'] = df_c1['MNS_ID'].map(ips_map)\n",
    "\n",
    "        df_c1['Glosa_2'] = df_c1['Glosa_2'].str.replace(r'GN0340\\(1\\|[0-9]+\\|[0-9]+\\);?', '', regex=True)\n",
    "        df_c1['Glosa_2'] = df_c1['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_c1['No_Glosa'] = df_c1['Glosa_2'].str.count(';')\n",
    "        df_c1['Code_Glosa'] = df_c1['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        mask_c1_to_envio = (df_c1['No_Glosa']==0)\n",
    "        df_c1_move_to_envio = df_c1[mask_c1_to_envio].copy()\n",
    "        if not df_c1_move_to_envio.empty:\n",
    "            # --- CORRECCI√ìN CLAVE AQU√ç ---\n",
    "            # Asegura que Df_NS_Envio tenga todas las columnas de df_c1_move_to_envio\n",
    "            for col in df_c1_move_to_envio.columns:\n",
    "                if col not in Df_NS_Envio.columns:\n",
    "                    Df_NS_Envio[col] = pd.NA # Define un valor predeterminado si es necesario\n",
    "            \n",
    "            # Concatena el DataFrame movido.\n",
    "            Df_NS_Envio = pd.concat([Df_NS_Envio, df_c1_move_to_envio], ignore_index=True)\n",
    "            # --- FIN CORRECCI√ìN CLAVE ---\n",
    "\n",
    "        df_c1_remain = df_c1[~mask_c1_to_envio].copy()\n",
    "        df_remaining_in_neg = pd.concat([df_remaining_in_neg, df_c1_remain], ignore_index=True)\n",
    "\n",
    "    # 5) Manejar registros de df_gn que NO fueron Causal '1' ni '2' (o donde la extracci√≥n de causal fall√≥)\n",
    "    handled_indices = set(df_c1.index).union(set(df_c2.index))\n",
    "    df_gn_unhandled = df_gn.loc[~df_gn.index.isin(handled_indices)].copy()\n",
    "\n",
    "    if not df_gn_unhandled.empty:\n",
    "        df_gn_unhandled['Glosa_2'] = df_gn_unhandled['Glosa_2'].str.replace(r'GN0340\\([^)]*\\);?', '', regex=True)\n",
    "        df_gn_unhandled['Glosa_2'] = df_gn_unhandled['Glosa_2'].str.replace(r';+', ';', regex=True).str.strip(';')\n",
    "        df_gn_unhandled['No_Glosa'] = df_gn_unhandled['Glosa_2'].str.count(';')\n",
    "        df_gn_unhandled['Code_Glosa'] = df_gn_unhandled['Glosa_2'].apply(lambda x: x.split(';')[0][:6] if x and x.split(';')[0] else '0')\n",
    "        \n",
    "        df_gn_unhandled['Motivo'] = \"Causal GN0340 no reconocida o error de extracci√≥n.\"\n",
    "        \n",
    "        # --- CORRECCI√ìN CLAVE AQU√ç ---\n",
    "        # Asegura que DF_No_Enviar tenga todas las columnas de df_gn_unhandled\n",
    "        for col in df_gn_unhandled.columns:\n",
    "            if col not in DF_No_Enviar.columns:\n",
    "                DF_No_Enviar[col] = pd.NA\n",
    "        \n",
    "        # Concatena el DataFrame movido.\n",
    "        DF_No_Enviar = pd.concat([DF_No_Enviar, df_gn_unhandled], ignore_index=True)\n",
    "        # --- FIN CORRECCI√ìN CLAVE ---\n",
    "\n",
    "    # 6) Reconstruir Df_NS_NEG\n",
    "    Df_NS_NEG_updated = pd.concat([df_not_gn0340, df_remaining_in_neg], ignore_index=True)\n",
    "\n",
    "    # Se eliminan columnas temporales si no forman parte del esquema final de DF_No_Enviar.\n",
    "    # Esta l√≠nea se mantiene si 'Causal' y 'IPSReportada' NO deben estar en el DF_No_Enviar final.\n",
    "    # Si estas columnas s√≠ deben estar, esta l√≠nea debe ser eliminada.\n",
    "    DF_No_Enviar = DF_No_Enviar.drop(columns=['Causal', 'IPSReportada'], errors='ignore')\n",
    "    Df_NS_Envio = Df_NS_Envio.drop(columns=['Code_Glosa', 'No_Glosa', 'Glosa_2'], errors='ignore')\n",
    "\n",
    "    final_total_records = len(Df_NS_NEG_updated) + len(Df_NS_Envio) + len(DF_No_Enviar)\n",
    "    print(f\"Despu√©s: Df_NS_NEG     = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio   = {len(Df_NS_Envio)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar  = {len(DF_No_Enviar)} registros\")\n",
    "    print(f\"Despu√©s: Total Registros = {final_total_records}\")\n",
    "\n",
    "    if initial_total_records != final_total_records:\n",
    "        print(\"¬°Advertencia! La suma de registros de salida no coincide con la entrada.\")\n",
    "        print(f\"Diferencia: {final_total_records - initial_total_records}\")\n",
    "    else:\n",
    "        print(\"¬°√âxito! La suma de registros de salida coincide con la entrada.\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio, DF_No_Enviar\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, Df_NS_Envio, DF_No_Enviar = process_gn0340(Df_NS_NEG, Df_NS_Envio, DF_No_Enviar)\n",
    "Df_NS_Envio = Df_NS_Envio.drop(columns=['Code_Glosa', 'No_Glosa', 'Glosa_2'], errors='ignore')\n",
    "print(DF_No_Enviar.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038927fe",
   "metadata": {},
   "source": [
    "## üö© GN0361\n",
    "1. La novedad no es reportada dentro de los dos mes siguientes, Decreto 780 de 2016.\n",
    "2. GN0361(01/05/2018|31/07/2018);\n",
    "3. GN0361(FechaInicioNovedad|FechaDosMesesDespues);\n",
    "4. La fecha de la novedad debe estar dentro del plazo establecido en Decreto 780 de 2016, dos meses antes teniendo en cuenta la fecha del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41d41644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG = 44 registros\n",
      "Antes: Df_NS_Envio = 309 registros\n",
      "Despu√©s: Df_NS_NEG = 44 registros\n",
      "Despu√©s: Df_NS_Envio = 309 registros\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "\n",
    "def process_gn0361(Df_NS_NEG, Df_NS_Envio, fecha_hoy):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0361 en Df_NS_NEG:\n",
    "      - Valida si FECHA_NOVEDAD < fecha m√≠nima de reporte (dos meses antes de fecha_hoy).\n",
    "      - Si es menor, actualiza FECHA_NOVEDAD a la fecha m√≠nima.\n",
    "      - Elimina la glosa GN0361(...) de Glosa_2.\n",
    "      - Recalcula Code_Glosa y No_Glosa.\n",
    "      - Si No_Glosa == 0, mueve el registro a Df_NS_Envio.\n",
    "      - Si quedan glosas, permanece en Df_NS_NEG.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: Df_NS_Envio = {len(Df_NS_Envio)} registros\")\n",
    "\n",
    "    # Filtrar registros con GN0361\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0361\\\\(', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # Calcular fecha m√≠nima de reporte (dos meses antes de fecha_hoy)\n",
    "    fecha_minima = datetime.strptime(fecha_hoy, '%d/%m/%Y') - relativedelta(months=2)\n",
    "\n",
    "    # Validar y actualizar FECHA_NOVEDAD solo si es menor a la fecha m√≠nima\n",
    "    def validar_fecha(fecha_str):\n",
    "        try:\n",
    "            fecha_novedad = datetime.strptime(fecha_str, '%d/%m/%Y')\n",
    "            if fecha_novedad < fecha_minima:\n",
    "                return fecha_minima.strftime('%d/%m/%Y')\n",
    "            else:\n",
    "                return fecha_str\n",
    "        except Exception:\n",
    "            return fecha_str\n",
    "\n",
    "    df_gn['FECHA_NOVEDAD'] = df_gn['FECHA_NOVEDAD'].apply(validar_fecha)\n",
    "\n",
    "    # Eliminar la glosa GN0361(...) de Glosa_2\n",
    "    df_gn['Glosa_2'] = df_gn['Glosa_2'].str.replace(r'GN0361\\([^)]*\\);', '', regex=True)\n",
    "\n",
    "    # Recalcular No_Glosa y Code_Glosa\n",
    "    df_gn['No_Glosa'] = df_gn['Glosa_2'].str.count(';')\n",
    "    df_gn['Code_Glosa'] = df_gn['Glosa_2'].str[:6].replace('', '0')\n",
    "\n",
    "    # Mover a Df_NS_Envio los que ya no tienen glosa\n",
    "    mask_zero = df_gn['No_Glosa'] == 0\n",
    "    df_envio_move = df_gn[mask_zero].drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "    Df_NS_Envio_updated = pd.concat([Df_NS_Envio, df_envio_move], ignore_index=True)\n",
    "\n",
    "    # Mantener en Df_NS_NEG los que a√∫n tienen glosas\n",
    "    df_gn_remain = df_gn[~mask_zero].copy()\n",
    "    Df_NS_NEG_updated = pd.concat([Df_NS_NEG[~mask], df_gn_remain], ignore_index=True)\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: Df_NS_Envio = {len(Df_NS_Envio_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, Df_NS_Envio_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, Df_NS_Envio = process_gn0361(Df_NS_NEG, Df_NS_Envio, Fecha)\n",
    "Df_NS_Envio = Df_NS_Envio.drop(columns=['Causal','IPSReportada'], errors='ignore')\n",
    "print(Df_NS_Envio.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52241f",
   "metadata": {},
   "source": [
    "## üö© GN0390\n",
    "1. Afiliado portabilidad es igual a la de BDUA.\n",
    "2. GN0390;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a06c1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n",
      "Antes: Df_NS_NEG = 44 registros\n",
      "Antes: DF_No_Enviar = 55 registros\n",
      "Despu√©s: Df_NS_NEG = 43 registros\n",
      "Despu√©s: DF_No_Enviar = 56 registros\n",
      "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
      "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
      "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
      "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
      "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
      "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
      "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def process_gn0390(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0390:\n",
    "    - Mueve todos los registros con GN0390 a DF_No_Enviar.\n",
    "    - Asigna el motivo \"Afiliado portabilidad es igual a la de BDUA.\" en la columna 'Motivo'.\n",
    "    - Elimina las columnas auxiliares ['Glosa_2', 'No_Glosa', 'Code_Glosa'] en DF_No_Enviar.\n",
    "    - Devuelve los DataFrames actualizados.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)} registros\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)} registros\")\n",
    "\n",
    "    # 1) Filtrar registros con GN0390\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0390', na=False)\n",
    "    df_gn390 = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # 2) Asignar motivo\n",
    "    df_gn390['Motivo'] = \"Afiliado portabilidad es igual a la de BDUA.\"\n",
    "\n",
    "    # 3) Eliminar columnas auxiliares\n",
    "    df_gn390 = df_gn390.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "\n",
    "    # 4) Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_gn390], ignore_index=True)\n",
    "\n",
    "    # 5) Mantener en Df_NS_NEG solo los que no tienen GN0390\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask].copy()\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)} registros\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)} registros\")\n",
    "\n",
    "    return Df_NS_NEG_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Implementaci√≥n de ejemplo:\n",
    "print(Df_NS_Envio.columns)\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0390(Df_NS_NEG, DF_No_Enviar)\n",
    "print(Df_NS_Envio.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954158d8",
   "metadata": {},
   "source": [
    "## üö© GN0501\n",
    "1. El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\n",
    "2. GN0501(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a93e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: Df_NS_NEG = 43\n",
      "Antes: DF_No_Enviar = 56\n",
      "Despu√©s: Df_NS_NEG = 43\n",
      "Despu√©s: DF_No_Enviar = 56\n"
     ]
    }
   ],
   "source": [
    "def process_gn0501(Df_NS_NEG, DF_No_Enviar):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0501:\n",
    "    - Mueve todos los registros con GN0501 a DF_No_Enviar.\n",
    "    - Asigna el motivo correspondiente en la columna 'Motivo'.\n",
    "    - Elimina las columnas auxiliares ['Glosa_2', 'No_Glosa', 'Code_Glosa'] en DF_No_Enviar.\n",
    "    - Devuelve los DataFrames actualizados.\n",
    "    \"\"\"\n",
    "    print(f\"Antes: Df_NS_NEG = {len(Df_NS_NEG)}\")\n",
    "    print(f\"Antes: DF_No_Enviar = {len(DF_No_Enviar)}\")\n",
    "\n",
    "    # Filtrar registros con GN0501\n",
    "    mask = Df_NS_NEG['Glosa_2'].str.contains('GN0501', na=False)\n",
    "    df_gn = Df_NS_NEG[mask].copy()\n",
    "\n",
    "    # Asignar motivo\n",
    "    df_gn['Motivo'] = \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"\n",
    "\n",
    "    # Eliminar columnas auxiliares\n",
    "    df_gn = df_gn.drop(columns=['Glosa_2', 'No_Glosa', 'Code_Glosa'], errors='ignore')\n",
    "\n",
    "    # Concatenar con DF_No_Enviar\n",
    "    DF_No_Enviar_updated = pd.concat([DF_No_Enviar, df_gn], ignore_index=True)\n",
    "\n",
    "    # Mantener en Df_NS_NEG solo los que no tienen GN0501\n",
    "    Df_NS_NEG_updated = Df_NS_NEG[~mask].copy()\n",
    "\n",
    "    print(f\"Despu√©s: Df_NS_NEG = {len(Df_NS_NEG_updated)}\")\n",
    "    print(f\"Despu√©s: DF_No_Enviar = {len(DF_No_Enviar_updated)}\")\n",
    "\n",
    "    return Df_NS_NEG_updated, DF_No_Enviar_updated\n",
    "\n",
    "# Ejemplo de uso:\n",
    "Df_NS_NEG, DF_No_Enviar = process_gn0501(Df_NS_NEG, DF_No_Enviar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad7593",
   "metadata": {},
   "source": [
    "# 6. Procesar Envio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bc688",
   "metadata": {},
   "source": [
    "## 6.1. Cargue NS SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07793b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
       "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
       "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
       "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
       "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
       "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
       "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Where'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_NS_Envio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea916557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de registros en Df_NS_Envio: 309\n",
      "N√∫mero total de registros en Df_NS_Envio: 24964\n"
     ]
    }
   ],
   "source": [
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_SIE\n",
    "new_data = pd.read_csv(R_NS_SIE, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"Where\"], errors='ignore')\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Marcar los registros existentes con \"Glosas\"\n",
    "Df_NS_Envio[\"Where\"] = \"Glosas\"\n",
    "\n",
    "# Marcar los nuevos registros con \"SIE\"\n",
    "new_data[\"Where\"] = \"SIE\"\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10cb3a",
   "metadata": {},
   "source": [
    "## 6.2. Cargue novedades planos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "716c606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de registros en Df_NS_Envio: 24964\n",
      "N√∫mero total de registros en Df_NS_Envio: 25007\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")\n",
    "# Cargar el archivo desde la ruta R_NS_Enviar\n",
    "new_data = pd.read_csv(R_NS_Enviar, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "\n",
    "# Asignar las mismas columnas que tiene Df_NS_Envio, excepto las columnas faltantes\n",
    "new_data.columns = Df_NS_Envio.columns.drop([\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"TPS_EST_AFL_ID_from_adres\"], errors='ignore')\n",
    "\n",
    "# La √∫ltima columna del archivo cargado corresponde a la columna \"Where\"\n",
    "new_data[\"Where\"] = new_data.iloc[:, -1]\n",
    "\n",
    "# Agregar las columnas faltantes con valores NaN\n",
    "for col in [\"Glosa\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\", \"TPS_EST_AFL_ID_from_adres\"]:\n",
    "    if col not in new_data.columns:\n",
    "        new_data[col] = None\n",
    "\n",
    "# Concatenar los nuevos registros al DataFrame existente\n",
    "Df_NS_Envio = pd.concat([Df_NS_Envio, new_data], ignore_index=True)\n",
    "\n",
    "# Imprimir el n√∫mero total de registros en Df_NS_Envio\n",
    "print(f\"N√∫mero total de registros en Df_NS_Envio: {Df_NS_Envio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae014e4",
   "metadata": {},
   "source": [
    "## 6.3 Validar ADRES EPS y Regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06da815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENT_ID_ADRES\n",
      "None      24698\n",
      "EPS025      308\n",
      "NaN           1\n",
      "Name: count, dtype: int64\n",
      "ENT_ID_ADRES\n",
      "EPS025    24048\n",
      "NaN         486\n",
      "EPSC25      473\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 0) Ver conteos previos (opcional)\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))\n",
    "\n",
    "# 1) Merge global para traer datos desde ADRES:\n",
    "df_adres_mini = (\n",
    "    DF_ADRES[[\n",
    "        \"TPS_IDN_ID\", \n",
    "        \"HST_IDN_NUMERO_IDENTIFICACION\", \n",
    "        \"ENT_ID_ADRES\", \n",
    "        \"TPS_EST_AFL_ID\"\n",
    "    ]]\n",
    "    .rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    df_adres_mini,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\",\"_new\")\n",
    ")\n",
    "\n",
    "# 2) Rellenar solo donde faltaba:\n",
    "Df_NS_Envio[\"ENT_ID_ADRES\"] = Df_NS_Envio[\"ENT_ID_ADRES\"].fillna(Df_NS_Envio[\"ENT_ID_ADRES_new\"])\n",
    "Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres\"].fillna(\n",
    "    Df_NS_Envio[\"TPS_EST_AFL_ID_from_adres_new\"]\n",
    ")\n",
    "\n",
    "# 3) Eliminar columnas auxiliares del merge\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_ADRES_new\",\"TPS_EST_AFL_ID_from_adres_new\"], inplace=True)\n",
    "\n",
    "# 4) Continuar con tu l√≥gica de evoluciones N01‚Ä¶\n",
    "mapa_n01 = (\n",
    "    Df_NS_Envio.loc[Df_NS_Envio[\"NOVEDAD\"] == \"N01\", \n",
    "                    [\"COD_1_NOVEDAD\",\"COD_2_NOVEDAD\",\"ENT_ID_ADRES\",\"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\":\"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\":\"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\":\"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\":\"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "Df_NS_Envio = Df_NS_Envio.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "mask_evol = (\n",
    "    Df_NS_Envio[\"ENT_ID_ADRES\"].isna() &\n",
    "    Df_NS_Envio[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    ")\n",
    "Df_NS_Envio.loc[mask_evol, \"ENT_ID_ADRES\"] = Df_NS_Envio.loc[mask_evol, \"ENT_ID_from_self\"]\n",
    "Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_adres\"] = Df_NS_Envio.loc[mask_evol, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "Df_NS_Envio.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n",
    "\n",
    "# 5) Verificar conteos finales\n",
    "print(Df_NS_Envio['ENT_ID_ADRES'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed89d7",
   "metadata": {},
   "source": [
    "### 6.3.1 Mover novedades de contributivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99d4098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 25007\n",
      "N√∫mero de registros en DF_NS_EPSC25: 13\n",
      "Despu√©s del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 24534\n",
      "N√∫mero de registros en DF_NS_EPSC25: 486\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")\n",
    "\n",
    "# Asegurarse de que DF_NS_EPSC25 tenga la columna 'Where'\n",
    "if 'Where' not in DF_NS_EPSC25.columns:\n",
    "    DF_NS_EPSC25['Where'] = None\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES sea igual a EPSC25\n",
    "mask = Df_NS_Envio[\"ENT_ID_ADRES\"] == \"EPSC25\"\n",
    "to_move = Df_NS_Envio[mask].copy()\n",
    "\n",
    "# Mover los registros a DF_NS_EPSC25\n",
    "DF_NS_EPSC25 = pd.concat([DF_NS_EPSC25, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio[~mask].copy()\n",
    "\n",
    "# Imprimir el n√∫mero de registros despu√©s del proceso\n",
    "print(f\"Despu√©s del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_NS_EPSC25: {len(DF_NS_EPSC25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478ba24",
   "metadata": {},
   "source": [
    "### 6.3.2 Mover a no enviar las que no tienen EPS capresoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e12207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 24534\n",
      "N√∫mero de registros en DF_No_Enviar: 56\n",
      "Despu√©s del proceso:\n",
      "N√∫mero de registros en Df_NS_Envio: 24048\n",
      "N√∫mero de registros en DF_No_Enviar: 542\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el n√∫mero de registros antes del proceso\n",
    "print(f\"Antes del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")\n",
    "\n",
    "# Filtrar los registros donde ENT_ID_ADRES es nulo o vac√≠o\n",
    "mask_empty = Df_NS_Envio[\"ENT_ID_ADRES\"].isna() | (Df_NS_Envio[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")\n",
    "to_move = Df_NS_Envio.loc[mask_empty].copy()\n",
    "\n",
    "# Asignar el motivo a los registros movidos\n",
    "to_move[\"Motivo\"] = \"No existen MS Adres actualmente\"\n",
    "\n",
    "# Concatenar los registros movidos al DataFrame DF_No_Enviar\n",
    "DF_No_Enviar = pd.concat([DF_No_Enviar, to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar los registros movidos de Df_NS_Envio\n",
    "Df_NS_Envio = Df_NS_Envio.loc[~mask_empty].copy()\n",
    "\n",
    "# Imprimir el n√∫mero de registros despu√©s del proceso\n",
    "print(f\"Despu√©s del proceso:\")\n",
    "print(f\"N√∫mero de registros en Df_NS_Envio: {len(Df_NS_Envio)}\")\n",
    "print(f\"N√∫mero de registros en DF_No_Enviar: {len(DF_No_Enviar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46826b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NUM_SOLICITUD_NOVEDAD', 'ENT_ID', 'TPS_IDN_ID',\n",
       "       'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
       "       'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
       "       'AFL_FECHA_NACIMIENTO', 'DPR_ID', 'MNS_ID', 'NOVEDAD', 'FECHA_NOVEDAD',\n",
       "       'COD_1_NOVEDAD', 'COD_2_NOVEDAD', 'COD_3_NOVEDAD', 'COD_4_NOVEDAD',\n",
       "       'COD_5_NOVEDAD', 'COD_6_NOVEDAD', 'COD_7_NOVEDAD', 'Glosa',\n",
       "       'ENT_ID_ADRES', 'TPS_EST_AFL_ID_from_adres', 'Motivo', 'cod_ent',\n",
       "       'Where'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_No_Enviar[\"Where\"] = \"Glosas\"\n",
    "DF_No_Enviar.columns\n",
    "#print(DF_No_Enviar['Where'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "de5c8649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "NUM_SOLICITUD_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ENT_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TPS_IDN_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "HST_IDN_NUMERO_IDENTIFICACION",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_PRIMER_APELLIDO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_SEGUNDO_APELLIDO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_PRIMER_NOMBRE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_SEGUNDO_NOMBRE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AFL_FECHA_NACIMIENTO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DPR_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MNS_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "FECHA_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_1_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_2_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_3_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_4_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_5_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_6_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COD_7_NOVEDAD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Glosa",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ENT_ID_ADRES",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TPS_EST_AFL_ID_from_adres",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Where",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6dd5b0f1-6313-4807-bb94-714fb43a66b4",
       "rows": [],
       "shape": {
        "columns": 24,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_SOLICITUD_NOVEDAD</th>\n",
       "      <th>ENT_ID</th>\n",
       "      <th>TPS_IDN_ID</th>\n",
       "      <th>HST_IDN_NUMERO_IDENTIFICACION</th>\n",
       "      <th>AFL_PRIMER_APELLIDO</th>\n",
       "      <th>AFL_SEGUNDO_APELLIDO</th>\n",
       "      <th>AFL_PRIMER_NOMBRE</th>\n",
       "      <th>AFL_SEGUNDO_NOMBRE</th>\n",
       "      <th>AFL_FECHA_NACIMIENTO</th>\n",
       "      <th>DPR_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>COD_2_NOVEDAD</th>\n",
       "      <th>COD_3_NOVEDAD</th>\n",
       "      <th>COD_4_NOVEDAD</th>\n",
       "      <th>COD_5_NOVEDAD</th>\n",
       "      <th>COD_6_NOVEDAD</th>\n",
       "      <th>COD_7_NOVEDAD</th>\n",
       "      <th>Glosa</th>\n",
       "      <th>ENT_ID_ADRES</th>\n",
       "      <th>TPS_EST_AFL_ID_from_adres</th>\n",
       "      <th>Where</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NUM_SOLICITUD_NOVEDAD, ENT_ID, TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, AFL_PRIMER_APELLIDO, AFL_SEGUNDO_APELLIDO, AFL_PRIMER_NOMBRE, AFL_SEGUNDO_NOMBRE, AFL_FECHA_NACIMIENTO, DPR_ID, MNS_ID, NOVEDAD, FECHA_NOVEDAD, COD_1_NOVEDAD, COD_2_NOVEDAD, COD_3_NOVEDAD, COD_4_NOVEDAD, COD_5_NOVEDAD, COD_6_NOVEDAD, COD_7_NOVEDAD, Glosa, ENT_ID_ADRES, TPS_EST_AFL_ID_from_adres, Where]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 24 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_NS_EPSC25\n",
    "DF_No_Enviar\n",
    "Df_NS_Envio\n",
    "\n",
    "# Filtrar el registro donde \"HST_IDN_NUMERO_IDENTIFICACION\" es igual a \"74811048\"\n",
    "Df_NS_Envio[Df_NS_Envio[\"HST_IDN_NUMERO_IDENTIFICACION\"] == \"74811048\"]\n",
    "#Df_NS_NEG[Df_NS_NEG['No_Glosa'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5df940ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code_Glosa\n",
      "GN0258    14\n",
      "GN0031     8\n",
      "GN0060     6\n",
      "GN0014     4\n",
      "GN0113     4\n",
      "GN0398     3\n",
      "GN0130     2\n",
      "GN0118     1\n",
      "GN0391     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los registros √∫nicos de la columna \"Code_Glosa\"\n",
    "print(Df_NS_NEG['Code_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bb895175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N01' 'N04' 'N14' 'N21' 'N32' 'N36' 'N39' 'N43']\n",
      "No_Glosa\n",
      "1    43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los valores √∫nicos\n",
    "print(Df_NS_NEG['NOVEDAD'].unique())\n",
    "print(Df_NS_NEG['No_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28f312e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code_Glosa\n",
      "GN0258    14\n",
      "GN0031     8\n",
      "GN0060     6\n",
      "GN0014     4\n",
      "GN0113     4\n",
      "GN0398     3\n",
      "GN0130     2\n",
      "GN0118     1\n",
      "GN0391     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Df_NS_NEG['Code_Glosa'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1ec2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df_NS_NEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54bdef",
   "metadata": {},
   "source": [
    "# Guardar Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "137391f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado en: C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2025\\10_Octubre\\17\\DataFrames_Activos 17102025.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Guardar todos los DataFrames activos en un archivo Excel, una hoja por cada uno\n",
    "\n",
    "# Lista de DataFrames y nombres de hoja\n",
    "dfs_to_save = {\n",
    "    \"Df_NS_Envio\": Df_NS_Envio,\n",
    "    \"Df_NS_NEG\": Df_NS_NEG,\n",
    "    \"DF_NS_EPSC25\": DF_NS_EPSC25,\n",
    "    \"DF_No_Enviar\": DF_No_Enviar,\n",
    "    \"DF_ADRES\": DF_ADRES,\n",
    "    \"DF_059_169\": DF_059_169\n",
    "}\n",
    "\n",
    "# Ruta de salida y nombre de archivo\n",
    "output_path = Path(R_Salida) / f\"DataFrames_Activos {F_Envio}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name, df in dfs_to_save.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name[:31], index=False)  # Excel limita el nombre de hoja a 31 caracteres\n",
    "\n",
    "print(f\"Archivo guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
