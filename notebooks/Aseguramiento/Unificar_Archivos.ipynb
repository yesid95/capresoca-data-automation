{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Finacieros semanales en un archivo por año EPSC25\n",
    "\n",
    "Se unifica los archivos financieros, información de aportantes, planillas, periodos, IBC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#Home\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\\Financiero_2025.TXT\"\n",
    "\n",
    "#Office\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\\Financiero_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', header=None)\n",
    "    df['nombre_Archivo'] = file\n",
    "    df['Fecha_Archivo'] = file[-10:-4]  # Extraer la fecha del nombre del archivo\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['TIPO REGISTRO', 'Nit', 'Razon_Soacial', 'COD BANCO AUTORIZADOR', 'Num_rad_planilla', 'PERIODO', 'CANAL DE PAGO', 'CANTIDAD REGISTROS', 'COD OPERADOR', 'VALOR PLANILLA', 'HORA MINUTO', 'NRO SECUENCIA', 'ENTIDAD', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 15:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 15 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. financieros consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',')\n",
    "    df_list.append(df)  # Agregar el DataFrame a la lista\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['TIPO REGISTRO', 'Nit', 'Razon_Soacial', 'COD BANCO AUTORIZADOR', 'Num_rad_planilla', 'PERIODO', 'CANAL DE PAGO', 'CANTIDAD REGISTROS', 'COD OPERADOR', 'VALOR PLANILLA', 'HORA MINUTO', 'NRO SECUENCIA', 'ENTIDAD', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 15:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 15 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PILA semanales en un archivo por año EPSC25\n",
    "\n",
    "Inofrmación de planilla, periodos, información de usuarios, novedades, ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\\Pila_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\\Pila_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', header=None)\n",
    "    df['nombre_Archivo'] = file\n",
    "    df['Fecha_Archivo'] = file[-10:-4]  # Extraer la fecha del nombre del archivo\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['codigo_operador', 'Num_rad_planilla', 'perido_pago_del_aportante', 'fecha_pago', 'cod_eps', 'digito_nit', 'Tipo_doc_cotizante', 'doc_cotizante', 'serial_benef_upc_adicional', 'tipo_doc_benef_upc_adicional', 'Doc_benef_adicional', 'Tipo_cotizante', 'subtipo_cotizante', 'tipo_pensionado', 'tipo_pension', 'pension_compartida', 'extranjero_no_obligado_compensar', 'colombiano_residente_eterior', 'cod_dept_ubicacion_laboral', 'mun_ubicacion_laboral', 'apel_1', 'apel_2', 'nom_1', 'nom_2', 'ingreso', 'retiro', 'traslado-otra_administradora', 'traslado_a_otra_adminstradora', 'variacion_permanente_salario', 'variacion_trnsitoria_salario', 'suspension_temporal_de_contrato', 'vacaciones', 'dias_cotizados', 'salario_basico', 'ibc', 'tarifa', 'cotizacion_obligatoria', 'valor_upc_adcional', 'Campo39', 'Campo40', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 42:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 42 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Unifica PILA consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['codigo_operador', 'Num_rad_planilla', 'perido_pago_del_aportante', 'fecha_pago', 'cod_eps', 'digito_nit', 'Tipo_doc_cotizante', 'doc_cotizante', 'serial_benef_upc_adicional', 'tipo_doc_benef_upc_adicional', 'Doc_benef_adicional', 'Tipo_cotizante', 'subtipo_cotizante', 'tipo_pensionado', 'tipo_pension', 'pension_compartida', 'extranjero_no_obligado_compensar', 'colombiano_residente_eterior', 'cod_dept_ubicacion_laboral', 'mun_ubicacion_laboral', 'apel_1', 'apel_2', 'nom_1', 'nom_2', 'ingreso', 'retiro', 'traslado-otra_administradora', 'traslado_a_otra_adminstradora', 'variacion_permanente_salario', 'variacion_trnsitoria_salario', 'suspension_temporal_de_contrato', 'vacaciones', 'dias_cotizados', 'salario_basico', 'ibc', 'tarifa', 'cotizacion_obligatoria', 'valor_upc_adcional', 'Campo39', 'Campo40', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 42:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    for i, df in enumerate(df_list):\n",
    "        if len(df.columns) != 42:\n",
    "            print(f\"El DataFrame en el archivo {txt_files[i]} no tiene 42 columnas.\")\n",
    "            print(df.head())\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 42 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Consolida el financiero y la pila para en un solo archivo tener aportantes y usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#pila_unificado_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "#financiero_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "pila_unificado_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "financiero_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "# Leer el archivo Financiero_Unificado.TXT\n",
    "df_financiero = pd.read_csv(financiero_file, encoding='utf-16', sep=',')\n",
    "# Leer el archivo Pila_Unificado.TXT\n",
    "df_unificado = pd.read_csv(pila_unificado_file, encoding='utf-16', sep=',')\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "df_financiero_selected = df_financiero[['Num_rad_planilla', 'Nit', 'Razon_Soacial']]\n",
    "\n",
    "# Renombrar las columnas para que coincidan con las columnas ID de Pila_Unificado.TXT\n",
    "df_financiero_selected.rename(columns={'Num_rad_planilla': 'Num_rad_planilla'}, inplace=True)\n",
    "\n",
    "# Convertir 'Num_rad_planilla' a string en ambos dataframes\n",
    "df_financiero_selected['Num_rad_planilla'] = df_financiero_selected['Num_rad_planilla'].astype(str)\n",
    "df_unificado['Num_rad_planilla'] = df_unificado['Num_rad_planilla'].astype(str)\n",
    "# Unir los dataframes\n",
    "df_resultado = pd.merge(df_unificado, df_financiero_selected, on='Num_rad_planilla', how='left')\n",
    "\n",
    "# Transformar las columnas 'cod_dept_ubicacion_laboral' y 'mun_ubicacion_laboral'\n",
    "df_resultado['cod_dept_ubicacion_laboral'] = df_resultado['cod_dept_ubicacion_laboral'].apply(lambda x: f\"{int(x):02d}\" if pd.notnull(x) else x)\n",
    "df_resultado['mun_ubicacion_laboral'] = df_resultado['mun_ubicacion_laboral'].apply(lambda x: f\"{int(x):03d}\" if pd.notnull(x) else x)\n",
    "\n",
    "# Asegurarse de que la columna 'Num_rad_planilla' sea de tipo entero\n",
    "df_resultado['Num_rad_planilla'] = df_resultado['Num_rad_planilla'].astype(str)\n",
    "# Reemplazar valores NaN en 'Nit' antes de convertir a entero\n",
    "df_resultado['Nit'] = df_resultado['Nit'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "# Guardar el dataframe resultante en un archivo .TXT\n",
    "#output_file_resultado = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila_Unificado_Con_Aportante_2018_2025.TXT\"\n",
    "output_file_resultado = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila_Unificado_Con_Aportante_2018_2025.TXT\"\n",
    "\n",
    "\n",
    "df_resultado.to_csv(output_file_resultado, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unificación de ABX en un único archivo por año (EPSC25)\n",
    "\n",
    "**Referencias normativas y contexto histórico**\n",
    "\n",
    "1. **Nota Externa 5215 de 2012**  \n",
    "   - **Fecha de emisión:** 14 de diciembre de 2012  \n",
    "   - **Publicación:** *Diario Oficial* No. 48.648 (18 de diciembre de 2012)  \n",
    "   - **Origen y propósito:**  \n",
    "     Esta Nota Externa, emitida por el entonces Ministerio de Salud y Protección Social, estableció lineamientos para la estructuración de ciertos archivos de datos relacionados con la compensación en el régimen contributivo de salud, contemplando variables como la UPC Adicional a favor del FOSYGA (Fondo de Solidaridad y Garantía). Su objetivo fue uniformar la presentación de información entre las EPS y el Estado, facilitando el control de los recursos y la validación de los giros efectuados.\n",
    "\n",
    "2. **Resolución 03341 de 2020**  \n",
    "   - **Fecha de emisión:** 30 de octubre de 2020  \n",
    "   - **Publicación oficial:** Disponible a través de la **ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud)**  \n",
    "   - **Transformación y relevancia:**  \n",
    "     La Resolución 03341 de 2020 introdujo cambios en la forma de reportar y validar la información de compensación, alineados con la transición del antiguo FOSYGA hacia la ADRES. Entre otras modificaciones, se suprimieron columnas asociadas al FOSYGA y se incorporaron nuevas variables (*Departamento* y *Municipio*) para la ubicación geográfica de los afiliados, reflejando los ajustes en la administración de recursos del sistema de salud colombiano.  \n",
    "     Esta Resolución derogó lineamientos previos (como la Resolución 1431 y 2935 de 2020) y consolidó las directrices para la presentación y procesamiento de la información en las bases de datos de aseguramiento.\n",
    "\n",
    "**Objetivo de la unificación**  \n",
    "En esta sección, se describe el proceso de unificación de la base de datos **ABX** por cada año calendario, con el fin de:\n",
    "- Generar un **archivo consolidado** anual que facilite el control y la trazabilidad de los datos.  \n",
    "- Homogeneizar la información proveniente de dos estructuras normativas diferentes (la antigua, asociada al FOSYGA, y la nueva, adaptada a la ADRES).  \n",
    "- Proporcionar una base sólida para análisis descriptivos, diagnósticos, predictivos y prescriptivos en el área de aseguramiento.\n",
    "\n",
    "De esta manera, garantizamos la coherencia de los datos a lo largo del tiempo y la posibilidad de comparar información histórica (2018–2021.01) con la posterior a la implementación de la Resolución 03341 de 2020 (desde 2021.02 en adelante).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuración de rutas: ajusta estas rutas para cada año procesado\n",
    "input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\2025\"\n",
    "output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\\ABX_2025.TXT\"\n",
    "\n",
    "#input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\2025\"\n",
    "#output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\\ABX_2025.TXT\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Conjuntos para identificar las versiones de la estructura\n",
    "# Estructura antigua: basada en Nota-Externa-5215-2012\n",
    "norma_vieja_folders = [\"2018\", \"2019\", \"2020\", \"2021.01\"]\n",
    "# Estructura nueva: basada en Resolución 03341 de 2020\n",
    "norma_nueva_folders = [\"2021.02\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "# Obtener el nombre de la carpeta actual (última parte del path)\n",
    "folder_name = os.path.basename(os.path.normpath(input_folder))\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT, agregando el nombre del archivo a cada registro\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', sep=',', header=None)\n",
    "    df['Nombre_Archivo'] = file\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Procesamiento condicional según la carpeta\n",
    "if folder_name in norma_vieja_folders:\n",
    "    # --- Estructura antigua (Nota-Externa-5215-2012) ---\n",
    "    columnas_vieja = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'UPC_Adicional_a_favor_Fosyga', 'Valor_Solidaridad_UPC_Adicional_a_favor_Fosyga',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Nombre_Archivo'\n",
    "    ]\n",
    "    if len(df_unificado.columns) == 25:\n",
    "        df_unificado.columns = columnas_vieja\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame unificado no tiene 25 columnas en estructura antigua.\")\n",
    "\n",
    "    # Eliminar las columnas de Fosyga y agregar las columnas de Departamento y Municipio\n",
    "    df_unificado = df_unificado.drop(['UPC_Adicional_a_favor_Fosyga', 'Valor_Solidaridad_UPC_Adicional_a_favor_Fosyga'], axis=1)\n",
    "    # Insertar las nuevas columnas con valores por defecto (comodín: '85' para Departamento y '001' para Municipio)\n",
    "    df_unificado.insert(22, 'Departamento', '85')\n",
    "    df_unificado.insert(23, 'Municipio', '001')\n",
    "    \n",
    "    # Reordenar las columnas para que coincidan con la estructura nueva\n",
    "    columnas_nueva = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "    ]\n",
    "    df_unificado = df_unificado[columnas_nueva]\n",
    "    \n",
    "    print(\"Norma aplicada: Nota-Externa-5215-2012. Se asignan 'Departamento' = '85' y 'Municipio' = '001' como comodín.\")\n",
    "    \n",
    "elif folder_name in norma_nueva_folders:\n",
    "    # --- Estructura nueva (Resolución 03341 de 2020) ---\n",
    "    columnas_nueva = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "    ]\n",
    "    if len(df_unificado.columns) == 25:\n",
    "        df_unificado.columns = columnas_nueva\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame unificado no tiene 25 columnas en estructura nueva.\")\n",
    "    \n",
    "    print(\"Norma aplicada: Resolución 03341 de 2020.\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"El nombre de la carpeta '{}' no coincide con ninguna estructura definida.\".format(folder_name))\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Se unifica archivos ABX consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\"\n",
    "output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "\n",
    "#input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\"\n",
    "#output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = [\n",
    "    'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "    'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "    'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "    'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "    'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "]\n",
    "if len(df_unificado.columns) == 25:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 25 columnas.\")\n",
    "\n",
    "# Asegurar que 'Departamento' tenga 2 dígitos y 'Municipio' 3 dígitos\n",
    "df_unificado['Departamento'] = df_unificado['Departamento'].astype(str).str.zfill(2)\n",
    "df_unificado['Municipio'] = df_unificado['Municipio'].astype(str).str.zfill(3)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unificar ACX en un archivo por año EPSC25\n",
    "\n",
    "**Referencias normativas y contexto histórico**\n",
    "\n",
    "1. **Nota Externa 5215 de 2012**  \n",
    "   - **Fecha de emisión:** 14 de diciembre de 2012  \n",
    "   - **Publicación:** *Diario Oficial* No. 48.648 (18 de diciembre de 2012)  \n",
    "   - **Origen y propósito:**  \n",
    "     Esta Nota Externa, emitida por el entonces Ministerio de Salud y Protección Social, estableció lineamientos para la estructuración de ciertos archivos de datos relacionados con la compensación en el régimen contributivo de salud, contemplando variables como la UPC Adicional a favor del FOSYGA (Fondo de Solidaridad y Garantía). Su objetivo fue uniformar la presentación de información entre las EPS y el Estado, facilitando el control de los recursos y la validación de los giros efectuados.\n",
    "\n",
    "2. **Resolución 03341 de 2020**  \n",
    "   - **Fecha de emisión:** 30 de octubre de 2020  \n",
    "   - **Publicación oficial:** Disponible a través de la **ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud)**  \n",
    "   - **Transformación y relevancia:**  \n",
    "     La Resolución 03341 de 2020 introdujo cambios en la forma de reportar y validar la información de compensación, alineados con la transición del antiguo FOSYGA hacia la ADRES. Entre otras modificaciones, se suprimieron columnas asociadas al FOSYGA y se incorporaron nuevas variables (*Departamento* y *Municipio*) para la ubicación geográfica de los afiliados, reflejando los ajustes en la administración de recursos del sistema de salud colombiano.  \n",
    "     Esta Resolución derogó lineamientos previos (como la Resolución 1431 y 2935 de 2020) y consolidó las directrices para la presentación y procesamiento de la información en las bases de datos de aseguramiento.\n",
    "\n",
    "**Objetivo de la unificación**  \n",
    "En esta sección, se describe el proceso de unificación de la base de datos **ACX** por cada año calendario, con el fin de:\n",
    "- Generar un **archivo consolidado** anual que facilite el control y la trazabilidad de los datos.  \n",
    "- Homogeneizar la información proveniente de dos estructuras normativas diferentes (la antigua, asociada al FOSYGA, y la nueva, adaptada a la ADRES).  \n",
    "- Proporcionar una base sólida para análisis descriptivos, diagnósticos, predictivos y prescriptivos en el área de aseguramiento.\n",
    "\n",
    "De esta manera, garantizamos la coherencia de los datos a lo largo del tiempo y la posibilidad de comparar información histórica (2018–2021.01) con la posterior a la implementación de la Resolución 03341 de 2020 (desde 2021.02 en adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuración de rutas: ajusta estas rutas para cada año procesado\n",
    "input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\2025\"\n",
    "output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\\ACX_2025.TXT\"\n",
    "\n",
    "#input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\2025\"\n",
    "#output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\\ACX_2025.TXT\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Conjuntos para identificar las versiones de la estructura\n",
    "# Estructura antigua: basada en Nota Externa 5215 de 2012\n",
    "norma_antigua_folders = [\"2018\", \"2019\", \"2020\", \"2021.01\"]\n",
    "# Estructura nueva: basada en Resolución 03341 de 2020\n",
    "norma_nueva_folders = [\"2021.02\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "# Obtener el nombre de la carpeta actual (última parte del path)\n",
    "folder_name = os.path.basename(os.path.normpath(input_folder))\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT, agregando el nombre del archivo a cada registro\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', sep=',', header=None)\n",
    "    df['Nombre_Archivo'] = file\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Definir los esquemas de columnas\n",
    "\n",
    "# Estructura Antigua (Nota Externa 5215 de 2012) - 26 columnas + Nombre_Archivo = 27\n",
    "columnas_antiguas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\", \n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Fosyga_Fondo_PyP\", \"Recursos_PyP\", \"Valor_Cotización_Subcuenta_Solidaridad\",\n",
    "    \"Serial_BDUA\", \"Serial_HA\", \"Nombre_Archivo\"\n",
    "]\n",
    "\n",
    "# Estructura Nueva (Resolución 03341 de 2020) - 27 columnas + Nombre_Archivo = 28\n",
    "columnas_nuevas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\",\n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Recursos_PyP\", \"Serial_BDUA\", \"Serial_HA\", \"Departamento\", \"Municipio\", \"Exoneración\",\n",
    "    \"Nombre_Archivo\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Procesamiento condicional según la carpeta\n",
    "if folder_name in norma_antigua_folders:\n",
    "    # --- Estructura Antigua ---\n",
    "    if len(df_unificado.columns) == 27:\n",
    "        df_unificado.columns = columnas_antiguas\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame no coincide con las 27 columnas esperadas (Estructura Antigua).\")\n",
    "    \n",
    "    # Eliminar columnas relacionadas a Fosyga\n",
    "    df_unificado = df_unificado.drop([\"Fosyga_Fondo_PyP\", \"Valor_Cotización_Subcuenta_Solidaridad\"], axis=1)\n",
    "    \n",
    "    # Agregar 3 columnas nuevas: Departamento, Municipio, Exoneración\n",
    "    df_unificado.insert(24, \"Departamento\", \"85\")\n",
    "    df_unificado.insert(25, \"Municipio\", \"001\")\n",
    "    df_unificado.insert(26, \"Exoneración\", \"No Aplica\")\n",
    "    \n",
    "    # Reordenar las columnas para que coincidan con la estructura nueva (28 columnas)\n",
    "    df_unificado = df_unificado[columnas_nuevas]\n",
    "    \n",
    "    print(\"Estructura Antigua detectada. Se eliminaron columnas de FOSYGA y se agregaron 'Departamento', 'Municipio' y 'Exoneración'.\")\n",
    "    \n",
    "elif folder_name in norma_nueva_folders:\n",
    "    # --- Estructura Nueva ---\n",
    "    if len(df_unificado.columns) == 28:\n",
    "        df_unificado.columns = columnas_nuevas\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame no coincide con las 28 columnas esperadas (Estructura Nueva).\")\n",
    "    \n",
    "    print(\"Estructura Nueva detectada. Columnas asignadas correctamente.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"La carpeta '{folder_name}' no coincide con ninguna estructura definida.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Se unifica archivos ACX consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\"\n",
    "output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "\n",
    "#input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\"\n",
    "#output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\",\n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Recursos_PyP\", \"Serial_BDUA\", \"Serial_HA\", \"Departamento\", \"Municipio\", \"Exoneración\",\n",
    "    \"Nombre_Archivo\"\n",
    "]\n",
    "if len(df_unificado.columns) == 28:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 28 columnas.\")\n",
    "\n",
    "# Asegurar que 'Departamento' tenga 2 dígitos y 'Municipio' 3 dígitos\n",
    "df_unificado['Departamento'] = df_unificado['Departamento'].astype(str).str.zfill(2)\n",
    "df_unificado['Municipio'] = df_unificado['Municipio'].astype(str).str.zfill(3)\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Automatico-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_33 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_44 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "\n",
    "# Encabezados para archivos de 44 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"ENT_ID_ORIGEN\", \"NUM_SOLICITUD_TRASLADO\", \"TIPO_TRASLADO\", \n",
    "    \"TPS_IDN_CF_ID_2\", \"HST_IDN_NUMERO_CF_IDENTIFICACION_2\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"TIPO_COTIZANTE\", \n",
    "    \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_IDN_ID_APORTANTE\", \n",
    "    \"HST_IDN_No_IDENTIFICACION_APORTANTE\", \"COD_CIIU\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 44 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_44 = header_33 + [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\"]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_33 + folders_44:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_33:\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:  # folder in folders_46\n",
    "        if df_consolidated.shape[1] != 46:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 45 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_44\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "    \n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 47:\n",
    "        cols_to_drop = [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_25 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_34 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_42 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_25 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_34 = header_25 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_42 = header_34 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "header_25 = header_25 + [\"GLOSA\"]\n",
    "header_34 = header_34 + [\"GLOSA\"]\n",
    "header_41 = header_42 + [\"GLOSA\"]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_25 + folders_34 + folders_42:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_25:\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    elif folder in folders_34:  # folder in folders_46\n",
    "        if df_consolidated.shape[1] != 35:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 35 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_34\n",
    "    else:  # folder in folders_43\n",
    "        if df_consolidated.shape[1] != 42:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 42 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_41\n",
    "\n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "header_43 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35, 42]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"GLOSA\"\n",
    "        # Note que en header_26, \"GLOSA\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"GLOSA\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    elif df.shape[1] == 35:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_43\n",
    "    \n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 43:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    # Asegurarse que la última columna sea \"GLOSA\" (según header_35)\n",
    "    df = df[header_35]\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "# Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "df_unificado[\"No_Glosas\"] = df_unificado[\"GLOSA\"].apply(contar_glosas)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Novedades validadas NC.VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All_2025.TXT\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', header=None)\n",
    "    # Add filename column as first column\n",
    "    df.insert(0, 'Nombre_Archivo', file)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, header=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All_NC_VAL-2018-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', header=None)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, header=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 S1 automatico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_33 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_40 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\"\n",
    "base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#_________________OFICCE__________________\n",
    "#base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\"\n",
    "#base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_33 = header_24 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_40 = header_33 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_33 + folders_40:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_33:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:\n",
    "        if df_consolidated.shape[1] != 41:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 41 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_40\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_42 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Columna34\", \"Columna35\", \"Columna36\", \"Columna37\", \"Columna38\",\n",
    "    \"Columna39\", \"Columna40\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "\n",
    "#input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 42:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"Fecha_Efectiva\"\n",
    "        # Note que en header_26, \"Fecha_Efectiva\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"Fecha_Efectiva\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "        \n",
    "    # Asegurarse que la última columna sea \"Fecha_Efectiva\" (según header_35)\n",
    "    df = df[header_35]\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 MC Maestro de ingresos contributibo\n",
    "Se unifican los archivos MC maestro de ingresos contributivo, del 2018 al 2025.\n",
    "Capresoca EPS no tiene un registro preciso de los archivos que ha reportado a la ADRES, asi las cosas se procede a unificar los archivos validado y negados. Los archivos del enero 2018 a abril del 2023  la estructura del archivo contiene 23 columnas, apartir de Mayo 2023 a 2025 la estructura tiene 24 columnas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 MC Validado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_23 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023-1\"]\n",
    "folders_24 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_35 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "\n",
    "# Encabezados para archivos de 23 columnas\n",
    "header_23 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_24 = header_23 + [\"COD_IPS_OD\"]\n",
    "header_35 = header_24 + [\"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\",\n",
    "                         \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_23 + folders_24 + folders_35:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_23:\n",
    "        if df_consolidated.shape[1] != 24:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 24 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_23\n",
    "    elif folder in folders_24:  # folder in folders_24\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    else:  # folder in folders_24\n",
    "        if df_consolidated.shape[1] != 36:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 36 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_35\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 MC NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023-1\"]\n",
    "folders_25 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_36 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "\n",
    "# Encabezados para archivos de 23 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\",\n",
    "    \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_25 = [\"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "    \"GLOSA\"\n",
    "    ]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_36 = [\"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "    \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\",\n",
    "    \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_25 + folders_36:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "\n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_25:  # folder in folders_26\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    else:  # folder in folders_36\n",
    "        if df_consolidated.shape[1] != 37:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 37 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_36\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 MC.VAL Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "    # Determinar qué columnas eliminar en función del número de columnas\n",
    "    # Para archivos con 24 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\"]\n",
    "    # Para archivos con 25 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 24:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"]\n",
    "    elif ncols == 25:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 36:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "                        \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "                        \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 MC.NEG Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "    # Determinar qué columnas eliminar en función del número de columnas\n",
    "    # Para archivos con 25 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\"]\n",
    "    # Para archivos con 26 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 25:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"]\n",
    "    elif ncols == 26:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 37:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "                        \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "                        \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    # Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "    df_temp[\"No_Glosas\"] = df_temp[\"GLOSA\"].apply(contar_glosas)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "cols.remove(\"GLOSA\")\n",
    "cols.append(\"GLOSA\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Consolidar MC [NEG, VAL] en un unico Archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas de entrada para el régimen contributivo (MC)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"GLOSA\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"Glosas\" y \"Glosa_Principal\" a partir de la columna \"GLOSA\"\n",
    "df_total[\"Glosas\"] = df_total[\"GLOSA\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"GLOSA\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 MS Maestro de ingresos subsidiado\n",
    "Se unifican los archivos MS maestro de ingresos Subsidiado, del 2018 al 2025.\n",
    "Capresoca EPS no tiene un registro preciso de los archivos que ha reportado a la ADRES, asi las cosas se procede a unificar los archivos validado y negados. Los archivos del enero 2018 a febrero del 2022 la estructura del archivo contiene 17 columnas, apartir de marzo 2022 a abril de 2023 la estructura tiene 24 columnas, y desde mayo del 2023 a 2025 la columna tienen 27 columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 MS Validado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_17 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_24 = [\"2022-2\", \"2023-1\"]\n",
    "folders_27 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_33 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_17 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\n",
    "    \"ZNS_ID\",\"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\"TPS_AFL_ID\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 27 columnas\n",
    "header_27 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 33 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\",\n",
    "    \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_17 + folders_24 + folders_27 + folders_33:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_17:\n",
    "        if df_consolidated.shape[1] != 18:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 18 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_17\n",
    "    elif folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_27:\n",
    "        if df_consolidated.shape[1] != 28:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 28 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_27\n",
    "    else:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 33:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 32 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 MS NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_18 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_25 = [\"2022-2\", \"2023-1\"]\n",
    "folders_29 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_33 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_18 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\n",
    "    \"ZNS_ID\",\"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_25 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\"TPS_AFL_ID\",\n",
    "    \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 27 columnas\n",
    "header_29 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 33 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\",\n",
    "    \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_18 + folders_25 + folders_29 + folders_33:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_18:\n",
    "        if df_consolidated.shape[1] != 19:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 19 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_18\n",
    "    elif folder in folders_25:\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    elif folder in folders_29:\n",
    "        if df_consolidated.shape[1] != 29:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 29 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_29\n",
    "    else:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 MS.VAL Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 18:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\"]\n",
    "        new_cols = [\"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_IDN_CF_ID\",\n",
    "                \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 25:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\", \"CON_DISCAPACIDAD\"]\n",
    "        new_cols = [\"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 28:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 33:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\", \n",
    "                        \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "if \"Fecha_Proceso\" in cols:\n",
    "    cols.remove(\"Fecha_Proceso\")\n",
    "    cols.append(\"Fecha_Proceso\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 MS.NEG Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 19:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\"]\n",
    "        new_cols = [\"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_IDN_CF_ID\",\n",
    "                \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 26:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\", \"CON_DISCAPACIDAD\"]\n",
    "        new_cols = [\"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 29:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 34:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\", \n",
    "                        \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "    df_temp[\"No_Glosas\"] = df_temp[\"GLOSA\"].apply(contar_glosas)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "if \"Fecha_Proceso\" in cols:\n",
    "    cols.remove(\"Fecha_Proceso\")\n",
    "    cols.append(\"Fecha_Proceso\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "cols.remove(\"GLOSA\")\n",
    "cols.append(\"GLOSA\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Consolidar MS [NEG, VAL] en un unico Archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Rutas de entrada (archivos ya consolidados y con encabezados)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\Consolidado_MS.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\Consolidado_MS.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"GLOSA\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"Glosas\" y \"Glosa_Principal\" a partir de la columna \"GLOSA\"\n",
    "df_total[\"Glosas\"] = df_total[\"GLOSA\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"GLOSA\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 NC  Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Consolidar NS All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\\\all-NS-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. S1.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_33 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_40 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\"\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_33 = header_24 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_40 = header_33 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_33 + folders_40:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_33:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:\n",
    "        if df_consolidated.shape[1] != 41:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 41 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_40\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_42 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Columna34\", \"Columna35\", \"Columna36\", \"Columna37\", \"Columna38\",\n",
    "    \"Columna39\", \"Columna40\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All-AUTO-S1.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 42:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"Fecha_Efectiva\"\n",
    "        # Note que en header_26, \"Fecha_Efectiva\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"Fecha_Efectiva\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "        \n",
    "    # Asegurarse que la última columna sea \"Fecha_Efectiva\" (según header_35)\n",
    "    df = df[header_35]\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendiente Unificar Maestro subsidiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\All\\MS_2025.TXT\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', header=None)\n",
    "    # Add filename column as first column\n",
    "    df.insert(0, 'Nombre_Archivo', file)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, header=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\All_MS_2023_2025.TXT\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', header=None)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, header=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
