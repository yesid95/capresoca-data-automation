{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Finacieros semanales en un archivo por año EPSC25\n",
    "\n",
    "Se unifica los archivos financieros, información de aportantes, planillas, periodos, IBC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#Home\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\\Financiero_2025.TXT\"\n",
    "\n",
    "Carpeta = \"2025\"\n",
    "\n",
    "#Office\n",
    "input_folder = fr\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\{Carpeta}\"\n",
    "output_file = fr\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\\Financiero_{Carpeta}.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', header=None, dtype=str)\n",
    "    df['nombre_Archivo'] = file\n",
    "    df['Fecha_Archivo'] = file[-10:-4]  # Extraer la fecha del nombre del archivo\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['TIPO REGISTRO', 'Nit', 'Razon_Soacial', 'COD BANCO AUTORIZADOR', 'Num_rad_planilla', 'PERIODO', 'CANAL DE PAGO', 'CANTIDAD REGISTROS', 'COD OPERADOR', 'VALOR PLANILLA', 'HORA MINUTO', 'NRO SECUENCIA', 'ENTIDAD', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 15:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 15 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. financieros consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', dtype=str)\n",
    "    df_list.append(df)  # Agregar el DataFrame a la lista\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['TIPO REGISTRO', 'Nit', 'Razon_Soacial', 'COD BANCO AUTORIZADOR', 'Num_rad_planilla', 'PERIODO', 'CANAL DE PAGO', 'CANTIDAD REGISTROS', 'COD OPERADOR', 'VALOR PLANILLA', 'HORA MINUTO', 'NRO SECUENCIA', 'ENTIDAD', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 15:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 15 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PILA semanales en un archivo por año EPSC25\n",
    "\n",
    "Inofrmación de planilla, periodos, información de usuarios, novedades, ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\\Pila_2025.TXT\"\n",
    "\n",
    "Carpeta = \"2025\"\n",
    "\n",
    "input_folder = fr\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\{Carpeta}\"\n",
    "output_file = fr\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\\Pila_{Carpeta}.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', header=None, dtype=str)\n",
    "    df['nombre_Archivo'] = file\n",
    "    df['Fecha_Archivo'] = file[-10:-4]  # Extraer la fecha del nombre del archivo\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['codigo_operador', 'Num_rad_planilla', 'perido_pago_del_aportante', 'fecha_pago', 'cod_eps', 'digito_nit', 'Tipo_doc_cotizante', 'doc_cotizante', 'serial_benef_upc_adicional', 'tipo_doc_benef_upc_adicional', 'Doc_benef_adicional', 'Tipo_cotizante', 'subtipo_cotizante', 'tipo_pensionado', 'tipo_pension', 'pension_compartida', 'extranjero_no_obligado_compensar', 'colombiano_residente_eterior', 'cod_dept_ubicacion_laboral', 'mun_ubicacion_laboral', 'apel_1', 'apel_2', 'nom_1', 'nom_2', 'ingreso', 'retiro', 'traslado-otra_administradora', 'traslado_a_otra_adminstradora', 'variacion_permanente_salario', 'variacion_trnsitoria_salario', 'suspension_temporal_de_contrato', 'vacaciones', 'dias_cotizados', 'salario_basico', 'ibc', 'tarifa', 'cotizacion_obligatoria', 'valor_upc_adcional', 'Campo39', 'Campo40', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 42:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 42 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Unifica PILA consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', dtype=str)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = ['codigo_operador', 'Num_rad_planilla', 'perido_pago_del_aportante', 'fecha_pago', 'cod_eps', 'digito_nit', 'Tipo_doc_cotizante', 'doc_cotizante', 'serial_benef_upc_adicional', 'tipo_doc_benef_upc_adicional', 'Doc_benef_adicional', 'Tipo_cotizante', 'subtipo_cotizante', 'tipo_pensionado', 'tipo_pension', 'pension_compartida', 'extranjero_no_obligado_compensar', 'colombiano_residente_eterior', 'cod_dept_ubicacion_laboral', 'mun_ubicacion_laboral', 'apel_1', 'apel_2', 'nom_1', 'nom_2', 'ingreso', 'retiro', 'traslado-otra_administradora', 'traslado_a_otra_adminstradora', 'variacion_permanente_salario', 'variacion_trnsitoria_salario', 'suspension_temporal_de_contrato', 'vacaciones', 'dias_cotizados', 'salario_basico', 'ibc', 'tarifa', 'cotizacion_obligatoria', 'valor_upc_adcional', 'Campo39', 'Campo40', 'nombre_Archivo', 'Fecha_Archivo']\n",
    "if len(df_unificado.columns) == 42:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    for i, df in enumerate(df_list):\n",
    "        if len(df.columns) != 42:\n",
    "            print(f\"El DataFrame en el archivo {txt_files[i]} no tiene 42 columnas.\")\n",
    "            print(df.head())\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 42 columnas.\")\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Consolida el financiero y la pila para en un solo archivo tener aportantes y usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1971185151.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_financiero_selected.rename(columns={'Num_rad_planilla': 'Num_rad_planilla'}, inplace=True)\n",
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1971185151.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_financiero_selected['Num_rad_planilla'] = df_financiero_selected['Num_rad_planilla'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#pila_unificado_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "#financiero_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "#output_file_resultado = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila_Unificado_Con_Aportante_2018_2025.TXT\"\n",
    "\n",
    "output_file_resultado = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila_Unificado_Con_Aportante_2018_2025.TXT\"\n",
    "pila_unificado_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Pila\\Merg_Pila_2018_2025.TXT\"\n",
    "financiero_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\Pila consiliada ADRES\\Financiero\\Merg_Financiero_2018_2025.TXT\"\n",
    "\n",
    "# Leer el archivo Financiero_Unificado.TXT\n",
    "df_financiero = pd.read_csv(financiero_file, encoding='utf-16', sep=',', dtype=str)\n",
    "# Leer el archivo Pila_Unificado.TXT\n",
    "df_unificado = pd.read_csv(pila_unificado_file, encoding='utf-16', sep=',', dtype=str)\n",
    "\n",
    "# Seleccionar las columnas necesarias\n",
    "df_financiero_selected = df_financiero[['Num_rad_planilla', 'Nit', 'Razon_Soacial']]\n",
    "\n",
    "# Renombrar las columnas para que coincidan con las columnas ID de Pila_Unificado.TXT\n",
    "df_financiero_selected.rename(columns={'Num_rad_planilla': 'Num_rad_planilla'}, inplace=True)\n",
    "\n",
    "# Convertir 'Num_rad_planilla' a string en ambos dataframes\n",
    "df_financiero_selected['Num_rad_planilla'] = df_financiero_selected['Num_rad_planilla'].astype(str)\n",
    "df_unificado['Num_rad_planilla'] = df_unificado['Num_rad_planilla'].astype(str)\n",
    "# Unir los dataframes\n",
    "df_resultado = pd.merge(df_unificado, df_financiero_selected, on='Num_rad_planilla', how='left')\n",
    "\n",
    "# Transformar las columnas 'cod_dept_ubicacion_laboral' y 'mun_ubicacion_laboral'\n",
    "df_resultado['cod_dept_ubicacion_laboral'] = df_resultado['cod_dept_ubicacion_laboral'].apply(lambda x: f\"{int(x):02d}\" if pd.notnull(x) else x)\n",
    "df_resultado['mun_ubicacion_laboral'] = df_resultado['mun_ubicacion_laboral'].apply(lambda x: f\"{int(x):03d}\" if pd.notnull(x) else x)\n",
    "\n",
    "# Asegurarse de que la columna 'Num_rad_planilla' sea de tipo entero\n",
    "df_resultado['Num_rad_planilla'] = df_resultado['Num_rad_planilla'].astype(str)\n",
    "# Reemplazar valores NaN en 'Nit' antes de convertir a entero\n",
    "df_resultado['Nit'] = df_resultado['Nit'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "df_resultado.to_csv(output_file_resultado, index=False, encoding='utf-16', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unificación de ABX en un único archivo por año (EPSC25)\n",
    "\n",
    "**Referencias normativas y contexto histórico**\n",
    "\n",
    "1. **Nota Externa 5215 de 2012**  \n",
    "   - **Fecha de emisión:** 14 de diciembre de 2012  \n",
    "   - **Publicación:** *Diario Oficial* No. 48.648 (18 de diciembre de 2012)  \n",
    "   - **Origen y propósito:**  \n",
    "     Esta Nota Externa, emitida por el entonces Ministerio de Salud y Protección Social, estableció lineamientos para la estructuración de ciertos archivos de datos relacionados con la compensación en el régimen contributivo de salud, contemplando variables como la UPC Adicional a favor del FOSYGA (Fondo de Solidaridad y Garantía). Su objetivo fue uniformar la presentación de información entre las EPS y el Estado, facilitando el control de los recursos y la validación de los giros efectuados.\n",
    "\n",
    "2. **Resolución 03341 de 2020**  \n",
    "   - **Fecha de emisión:** 30 de octubre de 2020  \n",
    "   - **Publicación oficial:** Disponible a través de la **ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud)**  \n",
    "   - **Transformación y relevancia:**  \n",
    "     La Resolución 03341 de 2020 introdujo cambios en la forma de reportar y validar la información de compensación, alineados con la transición del antiguo FOSYGA hacia la ADRES. Entre otras modificaciones, se suprimieron columnas asociadas al FOSYGA y se incorporaron nuevas variables (*Departamento* y *Municipio*) para la ubicación geográfica de los afiliados, reflejando los ajustes en la administración de recursos del sistema de salud colombiano.  \n",
    "     Esta Resolución derogó lineamientos previos (como la Resolución 1431 y 2935 de 2020) y consolidó las directrices para la presentación y procesamiento de la información en las bases de datos de aseguramiento.\n",
    "\n",
    "**Objetivo de la unificación**  \n",
    "En esta sección, se describe el proceso de unificación de la base de datos **ABX** por cada año calendario, con el fin de:\n",
    "- Generar un **archivo consolidado** anual que facilite el control y la trazabilidad de los datos.  \n",
    "- Homogeneizar la información proveniente de dos estructuras normativas diferentes (la antigua, asociada al FOSYGA, y la nueva, adaptada a la ADRES).  \n",
    "- Proporcionar una base sólida para análisis descriptivos, diagnósticos, predictivos y prescriptivos en el área de aseguramiento.\n",
    "\n",
    "De esta manera, garantizamos la coherencia de los datos a lo largo del tiempo y la posibilidad de comparar información histórica (2018–2021.01) con la posterior a la implementación de la Resolución 03341 de 2020 (desde 2021.02 en adelante).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norma aplicada: Resolución 03341 de 2020.\n",
      "Archivo guardado exitosamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\\ABX_2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuración de rutas: ajusta estas rutas para cada año procesado\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\\ABX_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\\ABX_2025.TXT\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Conjuntos para identificar las versiones de la estructura\n",
    "# Estructura antigua: basada en Nota-Externa-5215-2012\n",
    "norma_vieja_folders = [\"2018\", \"2019\", \"2020\", \"2021.01\"]\n",
    "# Estructura nueva: basada en Resolución 03341 de 2020\n",
    "norma_nueva_folders = [\"2021.02\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "# Obtener el nombre de la carpeta actual (última parte del path)\n",
    "folder_name = os.path.basename(os.path.normpath(input_folder))\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT, agregando el nombre del archivo a cada registro\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', sep=',', header=None)\n",
    "    df['Nombre_Archivo'] = file\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Procesamiento condicional según la carpeta\n",
    "if folder_name in norma_vieja_folders:\n",
    "    # --- Estructura antigua (Nota-Externa-5215-2012) ---\n",
    "    columnas_vieja = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'UPC_Adicional_a_favor_Fosyga', 'Valor_Solidaridad_UPC_Adicional_a_favor_Fosyga',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Nombre_Archivo'\n",
    "    ]\n",
    "    if len(df_unificado.columns) == 25:\n",
    "        df_unificado.columns = columnas_vieja\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame unificado no tiene 25 columnas en estructura antigua.\")\n",
    "\n",
    "    # Eliminar las columnas de Fosyga y agregar las columnas de Departamento y Municipio\n",
    "    df_unificado = df_unificado.drop(['UPC_Adicional_a_favor_Fosyga', 'Valor_Solidaridad_UPC_Adicional_a_favor_Fosyga'], axis=1)\n",
    "    # Insertar las nuevas columnas con valores por defecto (comodín: '85' para Departamento y '001' para Municipio)\n",
    "    df_unificado.insert(22, 'Departamento', '85')\n",
    "    df_unificado.insert(23, 'Municipio', '001')\n",
    "    \n",
    "    # Reordenar las columnas para que coincidan con la estructura nueva\n",
    "    columnas_nueva = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "    ]\n",
    "    df_unificado = df_unificado[columnas_nueva]\n",
    "    \n",
    "    print(\"Norma aplicada: Nota-Externa-5215-2012. Se asignan 'Departamento' = '85' y 'Municipio' = '001' como comodín.\")\n",
    "    \n",
    "elif folder_name in norma_nueva_folders:\n",
    "    # --- Estructura nueva (Resolución 03341 de 2020) ---\n",
    "    columnas_nueva = [\n",
    "        'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "        'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "        'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "        'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "        'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "    ]\n",
    "    if len(df_unificado.columns) == 25:\n",
    "        df_unificado.columns = columnas_nueva\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame unificado no tiene 25 columnas en estructura nueva.\")\n",
    "    \n",
    "    print(\"Norma aplicada: Resolución 03341 de 2020.\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"El nombre de la carpeta '{}' no coincide con ninguna estructura definida.\".format(folder_name))\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Se unifica archivos ABX consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = [\n",
    "    'Cod_EPS', 'Fecha_Giro', 'Periodo_Compensado', 'Tp_Do_Cf', 'No_Do_Cf', 'Tp_Do',\n",
    "    'No_Do', 'Tp_Usuario', 'Parentesco', 'Dias_Compensados', 'Valor_UPC_Adicional',\n",
    "    'Fecha_Consignación_UPC_Adiciona', 'Cod_Operador', 'Num_Planilla_PILA', 'Estado',\n",
    "    'Glosas', 'Grupo_Edad', 'Zona', 'UPC_Reconocida', 'UPC_Promoción_Prevención',\n",
    "    'Serial_BDUA', 'Serial_HA', 'Departamento', 'Municipio', 'Nombre_Archivo'\n",
    "]\n",
    "if len(df_unificado.columns) == 25:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 25 columnas.\")\n",
    "\n",
    "# Asegurar que 'Departamento' tenga 2 dígitos y 'Municipio' 3 dígitos\n",
    "df_unificado['Departamento'] = df_unificado['Departamento'].astype(str).str.zfill(2)\n",
    "df_unificado['Municipio'] = df_unificado['Municipio'].astype(str).str.zfill(3)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unificar ACX en un archivo por año EPSC25\n",
    "\n",
    "**Referencias normativas y contexto histórico**\n",
    "\n",
    "1. **Nota Externa 5215 de 2012**  \n",
    "   - **Fecha de emisión:** 14 de diciembre de 2012  \n",
    "   - **Publicación:** *Diario Oficial* No. 48.648 (18 de diciembre de 2012)  \n",
    "   - **Origen y propósito:**  \n",
    "     Esta Nota Externa, emitida por el entonces Ministerio de Salud y Protección Social, estableció lineamientos para la estructuración de ciertos archivos de datos relacionados con la compensación en el régimen contributivo de salud, contemplando variables como la UPC Adicional a favor del FOSYGA (Fondo de Solidaridad y Garantía). Su objetivo fue uniformar la presentación de información entre las EPS y el Estado, facilitando el control de los recursos y la validación de los giros efectuados.\n",
    "\n",
    "2. **Resolución 03341 de 2020**  \n",
    "   - **Fecha de emisión:** 30 de octubre de 2020  \n",
    "   - **Publicación oficial:** Disponible a través de la **ADRES (Administradora de los Recursos del Sistema General de Seguridad Social en Salud)**  \n",
    "   - **Transformación y relevancia:**  \n",
    "     La Resolución 03341 de 2020 introdujo cambios en la forma de reportar y validar la información de compensación, alineados con la transición del antiguo FOSYGA hacia la ADRES. Entre otras modificaciones, se suprimieron columnas asociadas al FOSYGA y se incorporaron nuevas variables (*Departamento* y *Municipio*) para la ubicación geográfica de los afiliados, reflejando los ajustes en la administración de recursos del sistema de salud colombiano.  \n",
    "     Esta Resolución derogó lineamientos previos (como la Resolución 1431 y 2935 de 2020) y consolidó las directrices para la presentación y procesamiento de la información en las bases de datos de aseguramiento.\n",
    "\n",
    "**Objetivo de la unificación**  \n",
    "En esta sección, se describe el proceso de unificación de la base de datos **ACX** por cada año calendario, con el fin de:\n",
    "- Generar un **archivo consolidado** anual que facilite el control y la trazabilidad de los datos.  \n",
    "- Homogeneizar la información proveniente de dos estructuras normativas diferentes (la antigua, asociada al FOSYGA, y la nueva, adaptada a la ADRES).  \n",
    "- Proporcionar una base sólida para análisis descriptivos, diagnósticos, predictivos y prescriptivos en el área de aseguramiento.\n",
    "\n",
    "De esta manera, garantizamos la coherencia de los datos a lo largo del tiempo y la posibilidad de comparar información histórica (2018–2021.01) con la posterior a la implementación de la Resolución 03341 de 2020 (desde 2021.02 en adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura Nueva detectada. Columnas asignadas correctamente.\n",
      "Archivo guardado exitosamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\\ACX_2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuración de rutas: ajusta estas rutas para cada año procesado\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\2025\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\\ACX_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\2025\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\\ACX_2025.TXT\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Conjuntos para identificar las versiones de la estructura\n",
    "# Estructura antigua: basada en Nota Externa 5215 de 2012\n",
    "norma_antigua_folders = [\"2018\", \"2019\", \"2020\", \"2021.01\"]\n",
    "# Estructura nueva: basada en Resolución 03341 de 2020\n",
    "norma_nueva_folders = [\"2021.02\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "# Obtener el nombre de la carpeta actual (última parte del path)\n",
    "folder_name = os.path.basename(os.path.normpath(input_folder))\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT, agregando el nombre del archivo a cada registro\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', sep=',', header=None)\n",
    "    df['Nombre_Archivo'] = file\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Definir los esquemas de columnas\n",
    "\n",
    "# Estructura Antigua (Nota Externa 5215 de 2012) - 26 columnas + Nombre_Archivo = 27\n",
    "columnas_antiguas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\", \n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Fosyga_Fondo_PyP\", \"Recursos_PyP\", \"Valor_Cotización_Subcuenta_Solidaridad\",\n",
    "    \"Serial_BDUA\", \"Serial_HA\", \"Nombre_Archivo\"\n",
    "]\n",
    "\n",
    "# Estructura Nueva (Resolución 03341 de 2020) - 27 columnas + Nombre_Archivo = 28\n",
    "columnas_nuevas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\",\n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Recursos_PyP\", \"Serial_BDUA\", \"Serial_HA\", \"Departamento\", \"Municipio\", \"Exoneración\",\n",
    "    \"Nombre_Archivo\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Procesamiento condicional según la carpeta\n",
    "if folder_name in norma_antigua_folders:\n",
    "    # --- Estructura Antigua ---\n",
    "    if len(df_unificado.columns) == 27:\n",
    "        df_unificado.columns = columnas_antiguas\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame no coincide con las 27 columnas esperadas (Estructura Antigua).\")\n",
    "    \n",
    "    # Eliminar columnas relacionadas a Fosyga\n",
    "    df_unificado = df_unificado.drop([\"Fosyga_Fondo_PyP\", \"Valor_Cotización_Subcuenta_Solidaridad\"], axis=1)\n",
    "    \n",
    "    # Agregar 3 columnas nuevas: Departamento, Municipio, Exoneración\n",
    "    df_unificado.insert(24, \"Departamento\", \"85\")\n",
    "    df_unificado.insert(25, \"Municipio\", \"001\")\n",
    "    df_unificado.insert(26, \"Exoneración\", \"No Aplica\")\n",
    "    \n",
    "    # Reordenar las columnas para que coincidan con la estructura nueva (28 columnas)\n",
    "    df_unificado = df_unificado[columnas_nuevas]\n",
    "    \n",
    "    print(\"Estructura Antigua detectada. Se eliminaron columnas de FOSYGA y se agregaron 'Departamento', 'Municipio' y 'Exoneración'.\")\n",
    "    \n",
    "elif folder_name in norma_nueva_folders:\n",
    "    # --- Estructura Nueva ---\n",
    "    if len(df_unificado.columns) == 28:\n",
    "        df_unificado.columns = columnas_nuevas\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame no coincide con las 28 columnas esperadas (Estructura Nueva).\")\n",
    "    \n",
    "    print(\"Estructura Nueva detectada. Columnas asignadas correctamente.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"La carpeta '{folder_name}' no coincide con ninguna estructura definida.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Se unifica archivos ACX consolidados de cada año en un unico archivo EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir las rutas de las carpetas y el archivo de salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Files since 2018 until 2024\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "\n",
    "# Obtener la lista de archivos .TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Leer y concatenar todos los archivos .TXT\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',')\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Asignar encabezados\n",
    "columnas = [\n",
    "    \"Cod_EPS\", \"Fecha_Pago\", \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\", \"Tipo_Cotizante\",\n",
    "    \"Tp_DA\", \"No_DA\", \"IB_cotización\", \"Dias_Cotizados\", \"Valor_Cotización\",\n",
    "    \"F_P_Planilla_Pila\", \"Cod_OP\", \"No_Planilla\", \"Estado_Registro\", \"Codigo_Glosa\",\n",
    "    \"Total_D_Compensados\", \"Grupo_Edad\", \"Zona\", \"UPC_Reconocer\", \"Provisión_Incapacidades\",\n",
    "    \"Recursos_PyP\", \"Serial_BDUA\", \"Serial_HA\", \"Departamento\", \"Municipio\", \"Exoneración\",\n",
    "    \"Nombre_Archivo\"\n",
    "]\n",
    "if len(df_unificado.columns) == 28:\n",
    "    df_unificado.columns = columnas\n",
    "else:\n",
    "    raise ValueError(\"El DataFrame unificado no tiene 28 columnas.\")\n",
    "\n",
    "# Asegurar que 'Departamento' tenga 2 dígitos y 'Municipio' 3 dígitos\n",
    "df_unificado['Departamento'] = df_unificado['Departamento'].astype(str).str.zfill(2)\n",
    "df_unificado['Municipio'] = df_unificado['Municipio'].astype(str).str.zfill(3)\n",
    "# Guardar el DataFrame unificado en un archivo .TXT\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Automatico-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_33 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_44 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\"\n",
    "\n",
    "# Encabezados para archivos de 44 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"ENT_ID_ORIGEN\", \"NUM_SOLICITUD_TRASLADO\", \"TIPO_TRASLADO\", \n",
    "    \"TPS_IDN_CF_ID_2\", \"HST_IDN_NUMERO_CF_IDENTIFICACION_2\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"TIPO_COTIZANTE\", \n",
    "    \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_IDN_ID_APORTANTE\", \n",
    "    \"HST_IDN_No_IDENTIFICACION_APORTANTE\", \"COD_CIIU\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 44 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_44 = header_33 + [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\"]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_33 + folders_44:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_33:\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:  # folder in folders_46\n",
    "        if df_consolidated.shape[1] != 46:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 45 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_44\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "    \n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 47:\n",
    "        cols_to_drop = [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_33 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_44 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\"\n",
    "\n",
    "# Encabezados para archivos de 44 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"ENT_ID_ORIGEN\", \"NUM_SOLICITUD_TRASLADO\", \"TIPO_TRASLADO\", \n",
    "    \"TPS_IDN_CF_ID_2\", \"HST_IDN_NUMERO_CF_IDENTIFICACION_2\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"TIPO_COTIZANTE\", \n",
    "    \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_IDN_ID_APORTANTE\", \n",
    "    \"HST_IDN_No_IDENTIFICACION_APORTANTE\", \"COD_CIIU\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 44 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_44 = header_33 + [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\", \"GLOSA\"]\n",
    "header_33 = header_33 + [\"GLOSA\"]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_33 + folders_44:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .TXT en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_33:\n",
    "        if df_consolidated.shape[1] != 35:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 35 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:  # folder in folders_46\n",
    "        if df_consolidated.shape[1] != 47:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 47 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_44\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "    \n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\Files since 2018 until 2024\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R3\\All-AUTO-R3.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 48:\n",
    "        cols_to_drop = [\"Columna34\",\"Columna35\",\"Columna36\",\"Columna37\",\"Columna38\",\"Columna39\",\"Columna40\",\"Columna41\",\"Columna42\",\"Columna43\",\"Columna44\", \"Columna45\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2022-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2022-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_25 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_34 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_42 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_25 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_34 = header_25 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_42 = header_34 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "header_25 = header_25 + [\"GLOSA\"]\n",
    "header_34 = header_34 + [\"GLOSA\"]\n",
    "header_41 = header_42 + [\"GLOSA\"]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_25 + folders_34 + folders_42:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_25:\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    elif folder in folders_34:  # folder in folders_46\n",
    "        if df_consolidated.shape[1] != 35:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 35 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_34\n",
    "    else:  # folder in folders_43\n",
    "        if df_consolidated.shape[1] != 42:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 42 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_41\n",
    "\n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado exitosamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "header_43 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35, 42]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"GLOSA\"\n",
    "        # Note que en header_26, \"GLOSA\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"GLOSA\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    elif df.shape[1] == 35:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_43\n",
    "    \n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 43:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    # Asegurarse que la última columna sea \"GLOSA\" (según header_35)\n",
    "    df = df[header_35]\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "# Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "df_unificado[\"No_Glosas\"] = df_unificado[\"GLOSA\"].apply(contar_glosas)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 S1 automatico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2022-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2022-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_33 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_40 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_33 = header_24 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_40 = header_33 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_33 + folders_40:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_33:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:\n",
    "        if df_consolidated.shape[1] != 41:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 41 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_40\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado exitosamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_42 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Columna34\", \"Columna35\", \"Columna36\", \"Columna37\", \"Columna38\",\n",
    "    \"Columna39\", \"Columna40\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 42:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"Fecha_Efectiva\"\n",
    "        # Note que en header_26, \"Fecha_Efectiva\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"Fecha_Efectiva\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "        \n",
    "    # Asegurarse que la última columna sea \"Fecha_Efectiva\" (según header_35)\n",
    "    df = df[header_35]\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 MC Maestro de ingresos contributibo\n",
    "Se unifican los archivos MC maestro de ingresos contributivo, del 2018 al 2025.\n",
    "Capresoca EPS no tiene un registro preciso de los archivos que ha reportado a la ADRES, asi las cosas se procede a unificar los archivos validado y negados. Los archivos del enero 2018 a abril del 2023  la estructura del archivo contiene 23 columnas, apartir de Mayo 2023 a 2025 la estructura tiene 24 columnas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 MC Validado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2023-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2023-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_23 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023-1\"]\n",
    "folders_24 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_35 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "\n",
    "# Encabezados para archivos de 23 columnas\n",
    "header_23 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_24 = header_23 + [\"COD_IPS_OD\"]\n",
    "header_35 = header_24 + [\"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\",\n",
    "                         \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_23 + folders_24 + folders_35:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_23:\n",
    "        if df_consolidated.shape[1] != 24:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 24 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_23\n",
    "    elif folder in folders_24:  # folder in folders_24\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    else:  # folder in folders_24\n",
    "        if df_consolidated.shape[1] != 36:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 36 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_35\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 MC NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2023-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2023-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 23 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023-1\"]\n",
    "folders_25 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_36 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "\n",
    "# Encabezados para archivos de 23 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\",\n",
    "    \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_25 = [\"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "    \"GLOSA\"\n",
    "    ]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 23 más \"COD_IPS_OD\" al final)\n",
    "header_36 = [\"ENT_ID\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_COTIZANTE\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"CON_DISCAPACIDAD\",\n",
    "    \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"CND_AFL_FECHA_INICIO\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "    \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\",\n",
    "    \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_25 + folders_36:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "\n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_25:  # folder in folders_26\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    else:  # folder in folders_36\n",
    "        if df_consolidated.shape[1] != 37:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 37 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_36\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 MC.VAL Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "    # Determinar qué columnas eliminar en función del número de columnas\n",
    "    # Para archivos con 24 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\"]\n",
    "    # Para archivos con 25 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 24:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"]\n",
    "    elif ncols == 25:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 36:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "                        \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "                        \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 MC.NEG Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "    # Determinar qué columnas eliminar en función del número de columnas\n",
    "    # Para archivos con 25 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\"]\n",
    "    # Para archivos con 26 columnas:\n",
    "    #    eliminar: [\"CON_DISCAPACIDAD\", \"Columna15\", \"Columna20\", \"Columna21\", \"Columna22\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 25:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\"]\n",
    "    elif ncols == 26:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 37:\n",
    "        cols_to_drop = [\"TPS_COTIZANTE\", \"CON_DISCAPACIDAD\", \"TPS_IDN_ID_APORTANTE\", \"HST_IDN_NUMERO_APORTANTE\", \"COD_CIIU\", \"COD_IPS_P\", \"COD_IPS_OD\",\n",
    "                        \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TPS_MDL_SBS_ID\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "                        \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    # Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "    df_temp[\"No_Glosas\"] = df_temp[\"GLOSA\"].apply(contar_glosas)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "cols.remove(\"GLOSA\")\n",
    "cols.append(\"GLOSA\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Consolidar MC [NEG, VAL] en un unico Archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1252339945.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(pick_final_row)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas de entrada para el régimen contributivo (MC)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"GLOSA\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"Glosas\" y \"Glosa_Principal\" a partir de la columna \"GLOSA\"\n",
    "df_total[\"Glosas\"] = df_total[\"GLOSA\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"GLOSA\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 MS Maestro de ingresos subsidiado\n",
    "Se unifican los archivos MS maestro de ingresos Subsidiado, del 2018 al 2025.\n",
    "Capresoca EPS no tiene un registro preciso de los archivos que ha reportado a la ADRES, asi las cosas se procede a unificar los archivos validado y negados. Los archivos del enero 2018 a febrero del 2022 la estructura del archivo contiene 17 columnas, apartir de marzo 2022 a abril de 2023 la estructura tiene 24 columnas, y desde mayo del 2023 a 2025 la columna tienen 27 columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 MS Validado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2022-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2022-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2023-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2023-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_17 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_24 = [\"2022-2\", \"2023-1\"]\n",
    "folders_27 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_33 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_17 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\n",
    "    \"ZNS_ID\",\"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\"TPS_AFL_ID\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 27 columnas\n",
    "header_27 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 33 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\",\n",
    "    \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_17 + folders_24 + folders_27 + folders_33:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_17:\n",
    "        if df_consolidated.shape[1] != 18:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 18 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_17\n",
    "    elif folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_27:\n",
    "        if df_consolidated.shape[1] != 28:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 28 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_27\n",
    "    else:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 33:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 32 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 MS NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2022-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2022-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2023-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2023-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_18 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_25 = [\"2022-2\", \"2023-1\"]\n",
    "folders_29 = [\"2023-2\", \"2024\", \"2025-1\"]\n",
    "folders_33 = [\"2025-2\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_18 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\n",
    "    \"ZNS_ID\",\"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_25 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"TPS_MDL_SBS_ID\",\"COD_IPS_P\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\"TPS_AFL_ID\",\n",
    "    \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 27 columnas\n",
    "header_29 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 33 columnas\n",
    "header_33 = [\n",
    "    \"ENT_ID\",\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\",\"AFL_PRIMER_APELLIDO\",\"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\"AFL_SEGUNDO_NOMBRE\",\"AFL_FECHA_NACIMIENTO\",\"TPS_GNR_ID\",\"DPR_ID\",\"MNC_ID\",\"ZNS_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\"TPS_GRP_PBL_ID\",\"TPS_NVL_SSB_ID\",\"COD_IPS_P\",\"TPS_MDL_SBS_ID\",\"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\",\"CON_DISCAPACIDAD\",\"TPS_IDN_CF_ID\",\"HST_IDN_NUMERO_CF_IDENTIFICACION\",\"TPS_PRN_ID\",\n",
    "    \"TPS_AFL_ID\",\"TP_ETNIA\",\"COD_COM_INDIGENA\",\"COD_IPS_OD\", \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\",\n",
    "    \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GLOSA\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_18 + folders_25 + folders_29 + folders_33:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_18:\n",
    "        if df_consolidated.shape[1] != 19:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 19 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_18\n",
    "    elif folder in folders_25:\n",
    "        if df_consolidated.shape[1] != 26:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 26 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_25\n",
    "    elif folder in folders_29:\n",
    "        if df_consolidated.shape[1] != 29:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 29 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_29\n",
    "    else:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 MS.VAL Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 18:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\"]\n",
    "        new_cols = [\"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_IDN_CF_ID\",\n",
    "                \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 25:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\", \"CON_DISCAPACIDAD\"]\n",
    "        new_cols = [\"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 28:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 33:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\", \n",
    "                        \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "if \"Fecha_Proceso\" in cols:\n",
    "    cols.remove(\"Fecha_Proceso\")\n",
    "    cols.append(\"Fecha_Proceso\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 MS.NEG Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directorio de entrada y salida\n",
    "#input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "\n",
    "input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "\n",
    "# Listar todos los archivos .TXT en el directorio\n",
    "txt_files = [f for f in os.listdir(input_dir) if f.endswith('.TXT')]\n",
    "\n",
    "def contar_glosas(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return 0\n",
    "    # Separa por ; y cuenta los fragmentos no vacíos\n",
    "    glosa_parts = [seg for seg in glosa.split(\";\") if seg.strip() != \"\"]\n",
    "    return len(glosa_parts)\n",
    "\n",
    "dfs = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    # Leer el archivo: se asume que la primera fila es encabezado y se leen todas las columnas como str\n",
    "    df_temp = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", dtype=str)\n",
    "    \n",
    "\n",
    "    ncols = df_temp.shape[1]\n",
    "    if ncols == 19:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\"]\n",
    "        new_cols = [\"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"TPS_IDN_CF_ID\",\n",
    "                \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 26:\n",
    "        cols_to_drop = [\"TPS_MDL_SBS_ID\", \"COD_IPS_P\", \"CON_DISCAPACIDAD\"]\n",
    "        new_cols = [\"TP_ETNIA\"]\n",
    "        for col in new_cols:\n",
    "            df_temp[col] = \"\"\n",
    "    elif ncols == 29:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\"]\n",
    "    elif ncols == 34:\n",
    "        cols_to_drop = [\"COD_IPS_P\", \"TPS_MDL_SBS_ID\", \"CON_DISCAPACIDAD\", \"COD_COM_INDIGENA\", \"COD_IPS_OD\", \n",
    "                        \"PAIS_NACIMIENTO\",  \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "    else:\n",
    "        raise ValueError(f\"El archivo {file} tiene {ncols} columnas inesperadas.\")\n",
    "    \n",
    "    # Eliminar las columnas no deseadas\n",
    "    df_temp.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Agregar la nueva columna \"No_Glosas\" contando las glosas en la columna \"GLOSA\"\n",
    "    df_temp[\"No_Glosas\"] = df_temp[\"GLOSA\"].apply(contar_glosas)\n",
    "    \n",
    "    # Crear la nueva columna \"Fecha_Proceso\" a partir del valor de \"Nombre_Archivo\"\n",
    "    # Se extrae el nombre sin extensión y se obtienen los últimos 8 caracteres (DDMMAAAA)\n",
    "    # Luego se formatea a DD/MM/AAAA\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        # Tomar los últimos 8 caracteres\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df_temp[\"Fecha_Proceso\"] = df_temp[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    \n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Concatenar todos los DataFrames\n",
    "df_consolidado = pd.concat(dfs, ignore_index=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "if \"Fecha_Proceso\" in cols:\n",
    "    cols.remove(\"Fecha_Proceso\")\n",
    "    cols.append(\"Fecha_Proceso\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_consolidado[\"MNC_ID\"] = df_consolidado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_consolidado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "cols = list(df_consolidado.columns)\n",
    "cols.remove(\"GLOSA\")\n",
    "cols.append(\"GLOSA\")\n",
    "df_consolidado = df_consolidado[cols]\n",
    "\n",
    "# Guarda el DataFrame consolidado en el archivo de salida\n",
    "df_consolidado.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Consolidar MS [NEG, VAL] en un unico Archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\Consolidado_MS.TXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1087536913.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(pick_final_row)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Rutas de entrada (archivos ya consolidados y con encabezados)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\Consolidado_MS.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\Consolidado_MS.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"GLOSA\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"Glosas\" y \"Glosa_Principal\" a partir de la columna \"GLOSA\"\n",
    "df_total[\"Glosas\"] = df_total[\"GLOSA\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"GLOSA\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. NC \n",
    "## NC VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\\All-2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 NC  VAL Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\All NC VAL\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 NC NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\\All-2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\", \"Glosa\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 NC NEG Consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\All NC NEG\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\all-NC-NEG.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 NC consolidado Val y NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1128384090.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(pick_final_row)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\Consolidado_NC.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas de entrada para el régimen contributivo (MC)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC NEG\\all-NC-NEG.txt\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\Consolidado_NC.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"Glosa\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"Glosas\" y \"Glosa_Principal\" a partir de la columna \"Glosa\"\n",
    "df_total[\"Glosas\"] = df_total[\"Glosa\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"Glosas\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. NS VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\\All-2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Consolidar NS All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\\\all-NS-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. NS NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\\All-2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\"\n",
    "\n",
    "# Encabezados para archivos de 18 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\", \"Glosa\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .NEG en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.NEG')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3. NS NEG consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 NS NEG y VAL consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_25108\\1898494004.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(pick_final_row)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo consolidado guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\Consolidado_NS.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas de entrada para el régimen contributivo (MC)\n",
    "#val_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All_MC_VAL.TXT\"\n",
    "#neg_path = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC NEG\\All_MC_NEG.TXT\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\Consolidado_MC.TXT\"\n",
    "\n",
    "val_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\\\all-NS-VAL.txt\"\n",
    "neg_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\Consolidado_NS.TXT\"\n",
    "\n",
    "# 1) Leer DataFrames (asumiendo que ya tienen encabezado)\n",
    "df_val = pd.read_csv(val_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "df_neg = pd.read_csv(neg_path, encoding=\"ANSI\", sep=\",\", header=0, dtype=str)\n",
    "\n",
    "# 2) Asegurar que ambos DF tengan la columna 'Estado'\n",
    "if \"Estado\" not in df_val.columns:\n",
    "    df_val[\"Estado\"] = \"VAL\"\n",
    "if \"Estado\" not in df_neg.columns:\n",
    "    df_neg[\"Estado\"] = \"NEG\"\n",
    "\n",
    "# 3) En caso de que df_val no tenga las columnas \"GLOSA\" y \"No_Glosas\", se crean vacías\n",
    "for col in [\"GLOSA\", \"No_Glosas\"]:\n",
    "    if col not in df_val.columns:\n",
    "        df_val[col] = \"\"\n",
    "\n",
    "# 4) Unificar el orden de columnas tomando como referencia df_neg\n",
    "columnas_comunes = df_neg.columns.tolist()\n",
    "df_val = df_val.reindex(columns=columnas_comunes, fill_value=\"\")\n",
    "\n",
    "# 5) Concatenar ambos DataFrames\n",
    "df_total = pd.concat([df_val, df_neg], ignore_index=True)\n",
    "\n",
    "# 6) Convertir \"Fecha_Proceso\" a datetime y crear columna \"Mes\"\n",
    "df_total[\"Fecha_Proceso\"] = pd.to_datetime(df_total[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "df_total[\"Mes\"] = df_total[\"Fecha_Proceso\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# 7) Funciones para extraer códigos de glosa\n",
    "def extraer_codigos(glosa):\n",
    "    if pd.isna(glosa) or glosa.strip() == \"\":\n",
    "        return []\n",
    "    # Buscar todos los patrones \"GN\" seguido de 4 dígitos\n",
    "    return re.findall(r'(GN\\d{4})', glosa)\n",
    "\n",
    "def procesar_glosa(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return \";\".join(codigos)\n",
    "\n",
    "def glosa_principal(glosa):\n",
    "    codigos = extraer_codigos(glosa)\n",
    "    return codigos[0] if codigos else \"\"\n",
    "\n",
    "# 8) Crear nuevas columnas \"GLOSA\" y \"Glosa_Principal\" a partir de la columna \"Glosa\"\n",
    "df_total[\"Glosas\"] = df_total[\"Glosa\"].apply(procesar_glosa)\n",
    "df_total[\"Glosa_Principal\"] = df_total[\"Glosa\"].apply(glosa_principal)\n",
    "\n",
    "# 9) Función para seleccionar la fila \"ganadora\" del grupo\n",
    "def pick_final_row(grupo):\n",
    "    # Si hay al menos un registro VAL en el grupo, se toma la primera fila con estado VAL.\n",
    "    if (grupo[\"Estado\"] == \"VAL\").any():\n",
    "        return grupo[grupo[\"Estado\"] == \"VAL\"].iloc[0]\n",
    "    else:\n",
    "        return grupo.iloc[0]\n",
    "\n",
    "# 10) Agrupar por [TPS_IDN_ID, HST_IDN_NUMERO_IDENTIFICACION, Mes] y aplicar la función\n",
    "df_final = (\n",
    "    df_total\n",
    "    .groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Mes\"], as_index=False)\n",
    "    .apply(pick_final_row)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 11) Guardar el DataFrame final en el archivo de salida\n",
    "df_final.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "print(\"Archivo consolidado guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. S1.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2022-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2022-2.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2025-1.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\\All-2025-2.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "# Lista de carpetas con estructura de 24 columnas y 24 columnas respectivamente\n",
    "folders_24 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022-1\"]\n",
    "folders_33 = [\"2022-2\", \"2023\", \"2024\", \"2025-1\"]\n",
    "folders_40 = [\"2025-2\"]\n",
    "#_________________HOME__________________\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\MC\\MC VAL\\All\"\n",
    "#_________________OFICCE__________________\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\"\n",
    "\n",
    "# Encabezados para archivos de 24 columnas\n",
    "header_24 = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 24 columnas (el mismo que para 33 más \"Fecha_Efectiva\" al final)\n",
    "header_33 = header_24 + [\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\"\n",
    "    ]\n",
    "\n",
    "header_40 = header_33 + [\n",
    "    \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "    ]\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_24 + folders_33 + folders_40:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "\n",
    "    # Crear la nueva columna \"Fecha_Efectiva\" según las reglas definidas\n",
    "\n",
    "    def compute_fecha_efectiva(row):\n",
    "        # La columna de fecha está en formato dd/mm/yyyy\n",
    "        fecha_str = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "        fecha = datetime.datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "        # Asumimos que la columna que define el traslado se llama \"TIPO_TRASLADO\"\n",
    "        tipo = int(row[\"TIPO_TRASLADO\"])\n",
    "        if tipo in [0, 3, 5]:\n",
    "            return fecha_str\n",
    "        elif tipo == 1:\n",
    "            # Primer día del mes siguiente\n",
    "            new_date = (fecha + relativedelta(months=1)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 2:\n",
    "            # Primer día del mes subsiguiente (dos meses después)\n",
    "            new_date = (fecha + relativedelta(months=2)).replace(day=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        elif tipo == 4:\n",
    "            # Mover la fecha un mes manteniendo el mismo día\n",
    "            new_date = fecha + relativedelta(months=1)\n",
    "            return new_date.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            # Por defecto, se conserva la fecha original\n",
    "            return fecha_str\n",
    "\n",
    "    \n",
    "    # Actualizar los encabezados agregando \"Nombre_Archivo\" como la primera columna\n",
    "    if folder in folders_24:\n",
    "        if df_consolidated.shape[1] != 25:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 25 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_24\n",
    "    elif folder in folders_33:  # folder in folders_33\n",
    "        if df_consolidated.shape[1] != 34:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 34 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_33\n",
    "    else:\n",
    "        if df_consolidated.shape[1] != 41:\n",
    "            print(f\"Advertencia en {folder}: Se esperaban 41 columnas (incluyendo 'Nombre_Archivo') pero se encontraron {df_consolidated.shape[1]}.\")\n",
    "        df_consolidated.columns = [\"Nombre_Archivo\"] + header_40\n",
    "\n",
    "    df_consolidated[\"Fecha_Efectiva\"] = df_consolidated.apply(compute_fecha_efectiva, axis=1)\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado exitosamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All-S1-VAL.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Encabezados para archivos de 26 columnas (original)\n",
    "header_26 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_35 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "# Encabezados para archivos de 35 columnas (estructura completa)\n",
    "header_42 = [\n",
    "    \"Nombre_Archivo\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\",\n",
    "    \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\",\n",
    "    \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"Columna34\", \"Columna35\", \"Columna36\", \"Columna37\", \"Columna38\",\n",
    "    \"Columna39\", \"Columna40\", \"Fecha_Efectiva\"\n",
    "]\n",
    "\n",
    "\n",
    "# Lista de columnas faltantes en archivos de 26 (posición 25 a 33 en header_35)\n",
    "missing_cols = header_35[25:34]  # 9 columnas\n",
    "\n",
    "# Definir rutas de entrada y salida\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All-S1-VAL.txt\"\n",
    "\n",
    "# Obtener lista de archivos TXT en la carpeta de entrada\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Leer archivo sin encabezados, con encoding ANSI y tratar todos los campos como cadenas\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "\n",
    "    ncols = df.shape[1]\n",
    "    if ncols == 42:\n",
    "        cols_to_drop = [\"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"]\n",
    "        # Eliminar las columnas no deseadas\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    \n",
    "    # Agregar columna con el nombre del archivo, si aún no está incluida\n",
    "    if df.shape[1] not in [26, 35]:\n",
    "        raise ValueError(f\"El archivo {file} tiene {df.shape[1]} columnas, no se reconoce la estructura.\")\n",
    "    \n",
    "    # Si el archivo tiene 26 columnas, agregar las columnas faltantes con valor vacío\n",
    "    if df.shape[1] == 26:\n",
    "        # Asignar encabezados originales de 26 columnas\n",
    "        df.columns = header_26\n",
    "        # Crear un DataFrame temporal con las columnas faltantes, llenas de cadena vacía\n",
    "        df_missing = pd.DataFrame(\"\", index=df.index, columns=missing_cols)\n",
    "        # Concatenar el DataFrame original (hasta la columna \"TIPO_TRASLADO\") con las columnas faltantes y luego la columna \"Fecha_Efectiva\"\n",
    "        # Note que en header_26, \"Fecha_Efectiva\" es la última columna. Así que separamos el DataFrame en dos partes:\n",
    "        df_left = df.iloc[:, :25]  # columnas 0 a 24 (incluyendo \"TIPO_TRASLADO\")\n",
    "        df_fecha = df.iloc[:, 25:]  # columna \"Fecha_Efectiva\"\n",
    "        # Concatenar: [df_left, df_missing, df_fecha]\n",
    "        df = pd.concat([df_left, df_missing, df_fecha], axis=1)\n",
    "    else:\n",
    "        # Si el archivo tiene 35 columnas, asignar el header completo\n",
    "        df.columns = header_35\n",
    "        \n",
    "    # Asegurarse que la última columna sea \"Fecha_Efectiva\" (según header_35)\n",
    "    df = df[header_35]\n",
    "\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame unificado en un archivo TXT sin encabezado, con encoding ANSI y separador coma\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')\n",
    "\n",
    "print(\"Archivo guardado exitosamente en:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4 EPS025\n",
    "## R4 NEGADO EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .NEG por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\", \"GLOSA\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".NEG\")]\n",
    "        \n",
    "        # Verificar si hay archivos .NEG para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .NEG en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.NEG -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 NEGADO EPS025 CONSOLIDADO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 541\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 386\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 133\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 159\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 78\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 87\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 105\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 63\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 1552\n",
      "Registros en el consolidado final: 1552\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\\R4_neg_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"R4_neg_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Negado\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R4 VAL EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .VAL por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".VAL\")]\n",
    "        \n",
    "        # Verificar si hay archivos .VAL para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .VAL en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.VAL -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 VAL EPS025 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 9546\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 3345\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 3891\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 5767\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 4211\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 4490\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 4465\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 2656\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 38371\n",
      "Registros en el consolidado final: 38371\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\\R4_VAL_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"R4_VAL_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4 Validado\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R4 EPS025 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de Negados cargado. Registros: 1552\n",
      "Archivo de Validados cargado. Registros: 38371\n",
      "\n",
      "Alineando la estructura de las columnas...\n",
      "Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\n",
      "Concatenando los archivos...\n",
      "\n",
      "--- Realizando validaciones ---\n",
      "Suma de registros de origen: 39923\n",
      "Registros en el consolidado final: 39923\n",
      "¡Validación de registros exitosa! El número coincide.\n",
      "¡Validación de columnas exitosa! La estructura es correcta.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4_consolidado_total.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_validados_y_negados(ruta_principal):\n",
    "    \"\"\"\n",
    "    Unifica los archivos R4_VAL_consolidado.txt y R4_neg_consolidado.txt\n",
    "    en un único archivo final, manejando la diferencia de columnas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_principal (str): La ruta a la carpeta 'R4' que contiene las subcarpetas 'R4 Validado' y 'R4 Negado'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rutas a los archivos de origen\n",
    "    ruta_validados = os.path.join(ruta_principal, \"R4 Validado\", \"R4_VAL_consolidado.txt\")\n",
    "    ruta_negados = os.path.join(ruta_principal, \"R4 Negado\", \"R4_neg_consolidado.txt\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    ruta_salida = os.path.join(ruta_principal, \"R4_consolidado_total.txt\")\n",
    "\n",
    "    # Columnas esperadas para el archivo final (las del negado)\n",
    "    columnas_finales = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                        \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\",\n",
    "                        \"GLOSA\", \"NOMBRE_ARCHIVO\", \"FECHA_PROCESO\"]\n",
    "\n",
    "    # Iniciar contadores\n",
    "    num_registros_val = 0\n",
    "    num_registros_neg = 0\n",
    "    \n",
    "    try:\n",
    "        # --- Leer y preparar el archivo de Negados ---\n",
    "        df_negados = pd.read_csv(ruta_negados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_neg = len(df_negados)\n",
    "        print(f\"Archivo de Negados cargado. Registros: {num_registros_neg}\")\n",
    "        \n",
    "        # --- Leer y preparar el archivo de Validados ---\n",
    "        df_validados = pd.read_csv(ruta_validados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_val = len(df_validados)\n",
    "        print(f\"Archivo de Validados cargado. Registros: {num_registros_val}\")\n",
    "        \n",
    "        # --- Alinear la estructura de los DataFrames ---\n",
    "        print(\"\\nAlineando la estructura de las columnas...\")\n",
    "        \n",
    "        # Verificar si la columna 'GLOSA' ya existe en el DataFrame de validados\n",
    "        if 'GLOSA' not in df_validados.columns:\n",
    "            print(\"Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\")\n",
    "            df_validados['GLOSA'] = \"\"  # Agrega la columna con una cadena vacía\n",
    "        \n",
    "        # Reordenar las columnas del DataFrame de validados para que coincidan con el de negados\n",
    "        df_validados = df_validados[columnas_finales]\n",
    "        df_negados = df_negados[columnas_finales] # Para asegurar el orden en ambos\n",
    "\n",
    "        # Validar el número de columnas después del ajuste\n",
    "        if len(df_validados.columns) != len(columnas_finales) or len(df_negados.columns) != len(columnas_finales):\n",
    "            print(\"¡ERROR DE ALINEACIÓN! El número de columnas no coincide después del ajuste.\")\n",
    "            return\n",
    "\n",
    "        # --- Concatenar los DataFrames ---\n",
    "        print(\"Concatenando los archivos...\")\n",
    "        df_consolidado = pd.concat([df_negados, df_validados], ignore_index=True)\n",
    "        num_registros_consolidado = len(df_consolidado)\n",
    "\n",
    "        # --- Validaciones ---\n",
    "        total_registros_sumados = num_registros_neg + num_registros_val\n",
    "        \n",
    "        print(\"\\n--- Realizando validaciones ---\")\n",
    "        print(f\"Suma de registros de origen: {total_registros_sumados}\")\n",
    "        print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "        \n",
    "        # Validación de conteo de registros\n",
    "        if total_registros_sumados == num_registros_consolidado:\n",
    "            print(\"¡Validación de registros exitosa! El número coincide.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. Revisar los archivos de origen.\")\n",
    "            return\n",
    "            \n",
    "        # Validación de columnas finales\n",
    "        if list(df_consolidado.columns) == columnas_finales:\n",
    "            print(\"¡Validación de columnas exitosa! La estructura es correcta.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! Las columnas del DataFrame final no coinciden con la estructura esperada.\")\n",
    "            print(f\"Columnas obtenidas: {list(df_consolidado.columns)}\")\n",
    "            print(f\"Columnas esperadas: {columnas_finales}\")\n",
    "            return\n",
    "            \n",
    "        # --- Guardar el resultado si las validaciones son correctas ---\n",
    "        df_consolidado.to_csv(ruta_salida, index=False, sep=',', encoding='utf-8')\n",
    "        print(f\"\\nProceso completado. El consolidado final se ha guardado en: {ruta_salida}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Uno de los archivos no se encontró. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de unificación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "ruta_carpeta_r4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\"\n",
    "\n",
    "consolidar_validados_y_negados(ruta_carpeta_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 EPS025 \n",
    "## S4 Negados EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .NEG por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\", \"GLOSA\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".NEG\")]\n",
    "        \n",
    "        # Verificar si hay archivos .NEG para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .NEG en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.NEG -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 NEGADO EPS025 CONSOLIDADO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 291\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 194\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 168\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 306\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 107\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 182\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 1\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 4\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 1253\n",
      "Registros en el consolidado final: 1253\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\\S4_neg_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"S4_neg_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Negado\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 VAL EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .VAL por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".VAL\")]\n",
    "        \n",
    "        # Verificar si hay archivos .VAL para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .VAL en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.VAL -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 VAL EPS025 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 11615\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 2999\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 3600\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 6827\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 3852\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 5983\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 7308\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 2311\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 44495\n",
      "Registros en el consolidado final: 44495\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\\S4_VAL_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"S4_VAL_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4 Validado\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 EPS025 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de Negados cargado. Registros: 1253\n",
      "Archivo de Validados cargado. Registros: 44495\n",
      "\n",
      "Alineando la estructura de las columnas...\n",
      "Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\n",
      "Concatenando los archivos...\n",
      "\n",
      "--- Realizando validaciones ---\n",
      "Suma de registros de origen: 45748\n",
      "Registros en el consolidado final: 45748\n",
      "¡Validación de registros exitosa! El número coincide.\n",
      "¡Validación de columnas exitosa! La estructura es correcta.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4_consolidado_total.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_validados_y_negados(ruta_principal):\n",
    "    \"\"\"\n",
    "    Unifica los archivos S4_VAL_consolidado.txt y S4_neg_consolidado.txt\n",
    "    en un único archivo final, manejando la diferencia de columnas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_principal (str): La ruta a la carpeta 'S4' que contiene las subcarpetas 'S4 Validado' y 'S4 Negado'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rutas a los archivos de origen\n",
    "    ruta_validados = os.path.join(ruta_principal, \"S4 Validado\", \"S4_VAL_consolidado.txt\")\n",
    "    ruta_negados = os.path.join(ruta_principal, \"S4 Negado\", \"S4_neg_consolidado.txt\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    ruta_salida = os.path.join(ruta_principal, \"S4_consolidado_total.txt\")\n",
    "\n",
    "    # Columnas esperadas para el archivo final (las del negado)\n",
    "    columnas_finales = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                        \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\",\n",
    "                        \"GLOSA\", \"NOMBRE_ARCHIVO\", \"FECHA_PROCESO\"]\n",
    "\n",
    "    # Iniciar contadores\n",
    "    num_registros_val = 0\n",
    "    num_registros_neg = 0\n",
    "    \n",
    "    try:\n",
    "        # --- Leer y preparar el archivo de Negados ---\n",
    "        df_negados = pd.read_csv(ruta_negados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_neg = len(df_negados)\n",
    "        print(f\"Archivo de Negados cargado. Registros: {num_registros_neg}\")\n",
    "        \n",
    "        # --- Leer y preparar el archivo de Validados ---\n",
    "        df_validados = pd.read_csv(ruta_validados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_val = len(df_validados)\n",
    "        print(f\"Archivo de Validados cargado. Registros: {num_registros_val}\")\n",
    "        \n",
    "        # --- Alinear la estructura de los DataFrames ---\n",
    "        print(\"\\nAlineando la estructura de las columnas...\")\n",
    "        \n",
    "        # Verificar si la columna 'GLOSA' ya existe en el DataFrame de validados\n",
    "        if 'GLOSA' not in df_validados.columns:\n",
    "            print(\"Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\")\n",
    "            df_validados['GLOSA'] = \"\"  # Agrega la columna con una cadena vacía\n",
    "        \n",
    "        # Reordenar las columnas del DataFrame de validados para que coincidan con el de negados\n",
    "        df_validados = df_validados[columnas_finales]\n",
    "        df_negados = df_negados[columnas_finales] # Para asegurar el orden en ambos\n",
    "\n",
    "        # Validar el número de columnas después del ajuste\n",
    "        if len(df_validados.columns) != len(columnas_finales) or len(df_negados.columns) != len(columnas_finales):\n",
    "            print(\"¡ERROR DE ALINEACIÓN! El número de columnas no coincide después del ajuste.\")\n",
    "            return\n",
    "\n",
    "        # --- Concatenar los DataFrames ---\n",
    "        print(\"Concatenando los archivos...\")\n",
    "        df_consolidado = pd.concat([df_negados, df_validados], ignore_index=True)\n",
    "        num_registros_consolidado = len(df_consolidado)\n",
    "\n",
    "        # --- Validaciones ---\n",
    "        total_registros_sumados = num_registros_neg + num_registros_val\n",
    "        \n",
    "        print(\"\\n--- Realizando validaciones ---\")\n",
    "        print(f\"Suma de registros de origen: {total_registros_sumados}\")\n",
    "        print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "        \n",
    "        # Validación de conteo de registros\n",
    "        if total_registros_sumados == num_registros_consolidado:\n",
    "            print(\"¡Validación de registros exitosa! El número coincide.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. Revisar los archivos de origen.\")\n",
    "            return\n",
    "            \n",
    "        # Validación de columnas finales\n",
    "        if list(df_consolidado.columns) == columnas_finales:\n",
    "            print(\"¡Validación de columnas exitosa! La estructura es correcta.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! Las columnas del DataFrame final no coinciden con la estructura esperada.\")\n",
    "            print(f\"Columnas obtenidas: {list(df_consolidado.columns)}\")\n",
    "            print(f\"Columnas esperadas: {columnas_finales}\")\n",
    "            return\n",
    "            \n",
    "        # --- Guardar el resultado si las validaciones son correctas ---\n",
    "        df_consolidado.to_csv(ruta_salida, index=False, sep=',', encoding='utf-8')\n",
    "        print(f\"\\nProceso completado. El consolidado final se ha guardado en: {ruta_salida}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Uno de los archivos no se encontró. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de unificación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "ruta_carpeta_r4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\"\n",
    "\n",
    "consolidar_validados_y_negados(ruta_carpeta_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S5 TXT EPS025\n",
    "## S5 ANUAL EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .TXT por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_SOLICITANTE2\", \n",
    "                     \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".TXT\")]\n",
    "        \n",
    "        # Verificar si hay archivos .TXT para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .TXT en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.TXT -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .TXT con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S5 CONSOLIDADO EPS025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 9726\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 3170\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 3499\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 8156\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 9183\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 12224\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 7959\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 3573\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 57490\n",
      "Registros en el consolidado final: 57490\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\S5_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"S5_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NS MUNICIPIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2018.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2019.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2020.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2021.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2022.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2023.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2024.TXT\n",
      "Consolidado guardado: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\\All-2025.TXT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de carpetas con estructura de 17 columnas, 24 columnas y 27 columnas respectivamente\n",
    "folders_20 = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]\n",
    "\n",
    "#base_input_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\"\n",
    "#base_output_dir = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All\"\n",
    "\n",
    "base_input_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\"\n",
    "base_output_dir = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\"\n",
    "\n",
    "# Encabezados para archivos de 17 columnas\n",
    "header_20 = [\n",
    "    \"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\",\n",
    "    \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"\n",
    "]\n",
    "\n",
    "\n",
    "# Función para procesar cada carpeta\n",
    "for folder in folders_20:\n",
    "    input_folder = os.path.join(base_input_dir, folder)\n",
    "    # Lista de archivos .VAL en la carpeta\n",
    "    val_files = [f for f in os.listdir(input_folder) if f.endswith('.VAL')]\n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        # Omitir archivos vacíos (tamaño 0 bytes)\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Leer el archivo sin encabezado, con codificación ANSI y separador ,\n",
    "        # Se establece dtype=str para leer todas las columnas como cadena\n",
    "        df = pd.read_csv(file_path, encoding=\"ANSI\", sep=\",\", header=None, dtype=str)\n",
    "        # Omitir si el DataFrame quedó vacío\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Insertar una primera columna con el nombre del archivo de origen\n",
    "        df.insert(0, \"Nombre_Archivo\", file)\n",
    "        consolidated_dfs.append(df)\n",
    "    \n",
    "    # Si no se leyó ningún archivo, seguir con el siguiente folder\n",
    "    if not consolidated_dfs:\n",
    "        print(f\"Sin datos en la carpeta {folder}.\")\n",
    "        continue\n",
    "        \n",
    "    # Concatenar todos los DataFrames\n",
    "    df_consolidated = pd.concat(consolidated_dfs, ignore_index=True)\n",
    "    \n",
    "    df_consolidated.columns = [\"Nombre_Archivo\"] + header_20\n",
    "        \n",
    "    # Definir nombre de archivo de salida según la carpeta\n",
    "    output_file = os.path.join(base_output_dir, f\"All-{folder}.TXT\")\n",
    "    df_consolidated.to_csv(output_file, index=False, encoding=\"ANSI\", sep=\",\")\n",
    "    print(f\"Consolidado guardado: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidar NS MUNICIPIOS All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output paths\n",
    "#input_folder = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All\"\n",
    "#output_file = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\all-2023-2025.txt\"\n",
    "\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\All\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\all-NS-VAL.txt\"\n",
    "\n",
    "# Get list of TXT files in input folder\n",
    "txt_files = [f for f in os.listdir(input_folder) if f.endswith('.TXT')]\n",
    "\n",
    "# Read and concatenate all TXT files\n",
    "df_list = []\n",
    "for file in txt_files:\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    # Read with ANSI encoding and no header\n",
    "    df = pd.read_csv(file_path, encoding='ANSI', sep=',', dtype=str)\n",
    "    def extraer_fecha(nombre):\n",
    "        base, _ = os.path.splitext(nombre)\n",
    "        fecha_str = base[-8:]\n",
    "        if len(fecha_str) == 8 and fecha_str.isdigit():\n",
    "            return fecha_str[:2] + \"/\" + fecha_str[2:4] + \"/\" + fecha_str[4:]\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    df[\"Fecha_Proceso\"] = df[\"Nombre_Archivo\"].apply(extraer_fecha)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_unificado = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def unificar_columnas(row):\n",
    "    # Convertir \"DPR_ID\" a dos dígitos y \"MNC_ID\" a tres dígitos, luego concatenar\n",
    "    dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "    mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "    return dpr + mnc\n",
    "\n",
    "\n",
    "# Aplicar la función y almacenar el resultado en la columna \"MNC_ID\"\n",
    "df_unificado[\"MNC_ID\"] = df_unificado.apply(unificar_columnas, axis=1)\n",
    "# Eliminar la columna \"DPR_ID\"\n",
    "df_unificado.drop(columns=[\"DPR_ID\"], inplace=True)\n",
    "\n",
    "# Save unified DataFrame to TXT file with ANSI encoding and no header\n",
    "df_unificado.to_csv(output_file, index=False, encoding='ANSI', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4 EPSC25\n",
    "## R4 NEGADO EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .NEG por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\", \"GLOSA\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".NEG\")]\n",
    "        \n",
    "        # Verificar si hay archivos .NEG para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .NEG en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.NEG -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 NEGADO EPSC25 CONSOLIDADO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 862\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 432\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 20\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 56\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 49\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 31\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 0\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 0\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 1450\n",
      "Registros en el consolidado final: 1450\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\\R4_neg_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"R4_neg_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 NEG\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R4 VAL EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .VAL por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".VAL\")]\n",
    "        \n",
    "        # Verificar si hay archivos .VAL para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .VAL en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.VAL -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R4 VAL EPSC25 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 922\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 485\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 983\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 991\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 1091\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 1420\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 1095\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 753\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 7740\n",
      "Registros en el consolidado final: 7740\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\\R4_VAL_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"R4_VAL_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4 VAL\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R4 EPSC25 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de Negados cargado. Registros: 1450\n",
      "Archivo de Validados cargado. Registros: 7740\n",
      "\n",
      "Alineando la estructura de las columnas...\n",
      "Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\n",
      "Concatenando los archivos...\n",
      "\n",
      "--- Realizando validaciones ---\n",
      "Suma de registros de origen: 9190\n",
      "Registros en el consolidado final: 9190\n",
      "¡Validación de registros exitosa! El número coincide.\n",
      "¡Validación de columnas exitosa! La estructura es correcta.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\\R4_consolidado_total.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_validados_y_negados(ruta_principal):\n",
    "    \"\"\"\n",
    "    Unifica los archivos R4_VAL_consolidado.txt y R4_neg_consolidado.txt\n",
    "    en un único archivo final, manejando la diferencia de columnas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_principal (str): La ruta a la carpeta 'R4' que contiene las subcarpetas 'R4 VAL' y 'R4 NEG'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rutas a los archivos de origen\n",
    "    ruta_validados = os.path.join(ruta_principal, \"R4 VAL\", \"R4_VAL_consolidado.txt\")\n",
    "    ruta_negados = os.path.join(ruta_principal, \"R4 NEG\", \"R4_neg_consolidado.txt\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    ruta_salida = os.path.join(ruta_principal, \"R4_consolidado_total.txt\")\n",
    "\n",
    "    # Columnas esperadas para el archivo final (las del negado)\n",
    "    columnas_finales = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                        \"ID_NOM_REGIS_ARCHIVO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\",\n",
    "                        \"GLOSA\", \"NOMBRE_ARCHIVO\", \"FECHA_PROCESO\"]\n",
    "\n",
    "    # Iniciar contadores\n",
    "    num_registros_val = 0\n",
    "    num_registros_neg = 0\n",
    "    \n",
    "    try:\n",
    "        # --- Leer y preparar el archivo de Negados ---\n",
    "        df_negados = pd.read_csv(ruta_negados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_neg = len(df_negados)\n",
    "        print(f\"Archivo de Negados cargado. Registros: {num_registros_neg}\")\n",
    "        \n",
    "        # --- Leer y preparar el archivo de Validados ---\n",
    "        df_validados = pd.read_csv(ruta_validados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_val = len(df_validados)\n",
    "        print(f\"Archivo de Validados cargado. Registros: {num_registros_val}\")\n",
    "        \n",
    "        # --- Alinear la estructura de los DataFrames ---\n",
    "        print(\"\\nAlineando la estructura de las columnas...\")\n",
    "        \n",
    "        # Verificar si la columna 'GLOSA' ya existe en el DataFrame de validados\n",
    "        if 'GLOSA' not in df_validados.columns:\n",
    "            print(\"Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\")\n",
    "            df_validados['GLOSA'] = \"\"  # Agrega la columna con una cadena vacía\n",
    "        \n",
    "        # Reordenar las columnas del DataFrame de validados para que coincidan con el de negados\n",
    "        df_validados = df_validados[columnas_finales]\n",
    "        df_negados = df_negados[columnas_finales] # Para asegurar el orden en ambos\n",
    "\n",
    "        # Validar el número de columnas después del ajuste\n",
    "        if len(df_validados.columns) != len(columnas_finales) or len(df_negados.columns) != len(columnas_finales):\n",
    "            print(\"¡ERROR DE ALINEACIÓN! El número de columnas no coincide después del ajuste.\")\n",
    "            return\n",
    "\n",
    "        # --- Concatenar los DataFrames ---\n",
    "        print(\"Concatenando los archivos...\")\n",
    "        df_consolidado = pd.concat([df_negados, df_validados], ignore_index=True)\n",
    "        num_registros_consolidado = len(df_consolidado)\n",
    "\n",
    "        # --- Validaciones ---\n",
    "        total_registros_sumados = num_registros_neg + num_registros_val\n",
    "        \n",
    "        print(\"\\n--- Realizando validaciones ---\")\n",
    "        print(f\"Suma de registros de origen: {total_registros_sumados}\")\n",
    "        print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "        \n",
    "        # Validación de conteo de registros\n",
    "        if total_registros_sumados == num_registros_consolidado:\n",
    "            print(\"¡Validación de registros exitosa! El número coincide.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. Revisar los archivos de origen.\")\n",
    "            return\n",
    "            \n",
    "        # Validación de columnas finales\n",
    "        if list(df_consolidado.columns) == columnas_finales:\n",
    "            print(\"¡Validación de columnas exitosa! La estructura es correcta.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! Las columnas del DataFrame final no coinciden con la estructura esperada.\")\n",
    "            print(f\"Columnas obtenidas: {list(df_consolidado.columns)}\")\n",
    "            print(f\"Columnas esperadas: {columnas_finales}\")\n",
    "            return\n",
    "            \n",
    "        # --- Guardar el resultado si las validaciones son correctas ---\n",
    "        df_consolidado.to_csv(ruta_salida, index=False, sep=',', encoding='utf-8')\n",
    "        print(f\"\\nProceso completado. El consolidado final se ha guardado en: {ruta_salida}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Uno de los archivos no se encontró. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de unificación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "ruta_carpeta_r4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R4\"\n",
    "\n",
    "consolidar_validados_y_negados(ruta_carpeta_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4 EPSC25 \n",
    "## S4 Negados EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .NEG por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\", \"GLOSA\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".NEG\")]\n",
    "        \n",
    "        # Verificar si hay archivos .NEG para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .NEG en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.NEG -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 NEGADO EPSC25 CONSOLIDADO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 0\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 4\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 0\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 2\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 0\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 5\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 0\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 0\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 11\n",
      "Registros en el consolidado final: 11\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\\S4_neg_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"S4_neg_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 NEG\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 VAL EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2019.txt\n",
      "Procesando el año: 2020...\n",
      "Archivos del año 2020 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2020.txt\n",
      "Procesando el año: 2021...\n",
      "Archivos del año 2021 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2021.txt\n",
      "Procesando el año: 2022...\n",
      "Archivos del año 2022 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2022.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .VAL por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                     \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".VAL\")]\n",
    "        \n",
    "        # Verificar si hay archivos .VAL para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .VAL en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.VAL -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .NEG con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4 VAL EPSC25 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 8 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 369\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 33\n",
      "Archivo 'unificado_2020.txt' leído. Registros: 13\n",
      "Archivo 'unificado_2021.txt' leído. Registros: 55\n",
      "Archivo 'unificado_2022.txt' leído. Registros: 48\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 164\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 206\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 19\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 907\n",
      "Registros en el consolidado final: 907\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\\S4_VAL_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"S4_VAL_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4 VAL\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4 EPSC25 CONSOLIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de Negados cargado. Registros: 11\n",
      "Archivo de Validados cargado. Registros: 907\n",
      "\n",
      "Alineando la estructura de las columnas...\n",
      "Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\n",
      "Concatenando los archivos...\n",
      "\n",
      "--- Realizando validaciones ---\n",
      "Suma de registros de origen: 918\n",
      "Registros en el consolidado final: 918\n",
      "¡Validación de registros exitosa! El número coincide.\n",
      "¡Validación de columnas exitosa! La estructura es correcta.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\\S4_consolidado_total.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_validados_y_negados(ruta_principal):\n",
    "    \"\"\"\n",
    "    Unifica los archivos S4_VAL_consolidado.txt y S4_neg_consolidado.txt\n",
    "    en un único archivo final, manejando la diferencia de columnas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_principal (str): La ruta a la carpeta 'S4' que contiene las subcarpetas 'S4 VAL' y 'S4 NEG'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rutas a los archivos de origen\n",
    "    ruta_validados = os.path.join(ruta_principal, \"S4 VAL\", \"S4_VAL_consolidado.txt\")\n",
    "    ruta_negados = os.path.join(ruta_principal, \"S4 NEG\", \"S4_neg_consolidado.txt\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    ruta_salida = os.path.join(ruta_principal, \"S4_consolidado_total.txt\")\n",
    "\n",
    "    # Columnas esperadas para el archivo final (las del negado)\n",
    "    columnas_finales = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "                        \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\",\n",
    "                        \"GLOSA\", \"NOMBRE_ARCHIVO\", \"FECHA_PROCESO\"]\n",
    "\n",
    "    # Iniciar contadores\n",
    "    num_registros_val = 0\n",
    "    num_registros_neg = 0\n",
    "    \n",
    "    try:\n",
    "        # --- Leer y preparar el archivo de Negados ---\n",
    "        df_negados = pd.read_csv(ruta_negados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_neg = len(df_negados)\n",
    "        print(f\"Archivo de Negados cargado. Registros: {num_registros_neg}\")\n",
    "        \n",
    "        # --- Leer y preparar el archivo de Validados ---\n",
    "        df_validados = pd.read_csv(ruta_validados, sep=',', encoding='utf-8', dtype=str)\n",
    "        num_registros_val = len(df_validados)\n",
    "        print(f\"Archivo de Validados cargado. Registros: {num_registros_val}\")\n",
    "        \n",
    "        # --- Alinear la estructura de los DataFrames ---\n",
    "        print(\"\\nAlineando la estructura de las columnas...\")\n",
    "        \n",
    "        # Verificar si la columna 'GLOSA' ya existe en el DataFrame de validados\n",
    "        if 'GLOSA' not in df_validados.columns:\n",
    "            print(\"Columna 'GLOSA' no encontrada en el archivo de validados. Agregándola con valores vacíos.\")\n",
    "            df_validados['GLOSA'] = \"\"  # Agrega la columna con una cadena vacía\n",
    "        \n",
    "        # Reordenar las columnas del DataFrame de validados para que coincidan con el de negados\n",
    "        df_validados = df_validados[columnas_finales]\n",
    "        df_negados = df_negados[columnas_finales] # Para asegurar el orden en ambos\n",
    "\n",
    "        # Validar el número de columnas después del ajuste\n",
    "        if len(df_validados.columns) != len(columnas_finales) or len(df_negados.columns) != len(columnas_finales):\n",
    "            print(\"¡ERROR DE ALINEACIÓN! El número de columnas no coincide después del ajuste.\")\n",
    "            return\n",
    "\n",
    "        # --- Concatenar los DataFrames ---\n",
    "        print(\"Concatenando los archivos...\")\n",
    "        df_consolidado = pd.concat([df_negados, df_validados], ignore_index=True)\n",
    "        num_registros_consolidado = len(df_consolidado)\n",
    "\n",
    "        # --- Validaciones ---\n",
    "        total_registros_sumados = num_registros_neg + num_registros_val\n",
    "        \n",
    "        print(\"\\n--- Realizando validaciones ---\")\n",
    "        print(f\"Suma de registros de origen: {total_registros_sumados}\")\n",
    "        print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "        \n",
    "        # Validación de conteo de registros\n",
    "        if total_registros_sumados == num_registros_consolidado:\n",
    "            print(\"¡Validación de registros exitosa! El número coincide.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. Revisar los archivos de origen.\")\n",
    "            return\n",
    "            \n",
    "        # Validación de columnas finales\n",
    "        if list(df_consolidado.columns) == columnas_finales:\n",
    "            print(\"¡Validación de columnas exitosa! La estructura es correcta.\")\n",
    "        else:\n",
    "            print(\"¡ERROR DE VALIDACIÓN! Las columnas del DataFrame final no coinciden con la estructura esperada.\")\n",
    "            print(f\"Columnas obtenidas: {list(df_consolidado.columns)}\")\n",
    "            print(f\"Columnas esperadas: {columnas_finales}\")\n",
    "            return\n",
    "            \n",
    "        # --- Guardar el resultado si las validaciones son correctas ---\n",
    "        df_consolidado.to_csv(ruta_salida, index=False, sep=',', encoding='utf-8')\n",
    "        print(f\"\\nProceso completado. El consolidado final se ha guardado en: {ruta_salida}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Uno de los archivos no se encontró. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de unificación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "ruta_carpeta_r4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\S4\"\n",
    "\n",
    "consolidar_validados_y_negados(ruta_carpeta_r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R5 TXT EPSC25\n",
    "## R5 ANUAL EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018...\n",
      "Archivos del año 2018 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\\unificado_2018.txt\n",
      "Procesando el año: 2019...\n",
      "Archivos del año 2019 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\\unificado_2019.txt\n",
      "Procesando el año: 2023...\n",
      "Archivos del año 2023 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\\unificado_2023.txt\n",
      "Procesando el año: 2024...\n",
      "Archivos del año 2024 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\\unificado_2024.txt\n",
      "Procesando el año: 2025...\n",
      "Archivos del año 2025 unificados correctamente en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\\unificado_2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\"\n",
    "output_file = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\All\"\n",
    "\n",
    "def unificar_archivos_por_anio(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Unifica archivos .NEG por año en un solo archivo .txt.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta principal que contiene las carpetas por año.\n",
    "        output_folder (str): La ruta donde se guardarán los archivos unificados por año.\n",
    "    \"\"\"\n",
    "    # Lista de nombres de columnas requeridas\n",
    "    columnas_base = [\"AFL_ID\", \"ENT_ID_SOLICITANTE\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_SOLICITANTE2\", \"COD_MUNICIPIO\", \n",
    "                     \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"CONSECUTIGO\", \"RESPUESTA\", \"CAUSAL_RESPUESTA\", \"FECHA_PROBABLE_TRASLADO\"]\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Listar las carpetas de los años\n",
    "    carpetas_anio = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "\n",
    "    # Excluir la carpeta \"All\" de la unificación\n",
    "    if \"All\" in carpetas_anio:\n",
    "        carpetas_anio.remove(\"All\")\n",
    "\n",
    "    for anio in carpetas_anio:\n",
    "        print(f\"Procesando el año: {anio}...\")\n",
    "        ruta_anio = os.path.join(input_folder, anio)\n",
    "        archivos_neg_anio = [f for f in os.listdir(ruta_anio) if f.endswith(\".TXT\")]\n",
    "        \n",
    "        # Verificar si hay archivos .TXT para procesar en el año\n",
    "        if not archivos_neg_anio:\n",
    "            print(f\"No se encontraron archivos .TXT en la carpeta del año {anio}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        lista_dfs = []\n",
    "        for nombre_archivo in archivos_neg_anio:\n",
    "            ruta_archivo = os.path.join(ruta_anio, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Extraer la fecha del nombre del archivo (DDMMAAAA)\n",
    "                # Ejemplo: R4EPS02505012024.TXT -> DDMMAAAA = 05012024\n",
    "                # Se asume que el formato de la fecha es constante.\n",
    "                partes_nombre = nombre_archivo.split(\".\")\n",
    "                fecha_str = partes_nombre[0][-8:]\n",
    "                fecha_proceso = datetime.strptime(fecha_str, \"%d%m%Y\").strftime(\"%d/%m/%Y\")\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error al extraer la fecha del archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Leer el archivo .TXT con pandas\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', header=None, names=columnas_base, dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Agregar las nuevas columnas\n",
    "                df_temp[\"NOMBRE_ARCHIVO\"] = nombre_archivo\n",
    "                df_temp[\"FECHA_PROCESO\"] = fecha_proceso\n",
    "                \n",
    "                lista_dfs.append(df_temp)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo {nombre_archivo}. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Concatenar todos los DataFrames del año en uno solo\n",
    "        if lista_dfs:\n",
    "            df_final_anio = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "            # Definir la ruta de salida para el archivo unificado\n",
    "            output_file = os.path.join(output_folder, f\"unificado_{anio}.txt\")\n",
    "            \n",
    "            # Guardar el DataFrame unificado en el archivo .txt\n",
    "            df_final_anio.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "            print(f\"Archivos del año {anio} unificados correctamente en: {output_file}\")\n",
    "        else:\n",
    "            print(f\"No se pudieron procesar archivos válidos para el año {anio}. No se creará el archivo de salida.\")\n",
    "            \n",
    "# --- Uso de la función ---\n",
    "\n",
    "unificar_archivos_por_anio(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R5 CONSOLIDADO EPSC25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la consolidación de 5 archivos...\n",
      "Archivo 'unificado_2018.txt' leído. Registros: 26\n",
      "Archivo 'unificado_2019.txt' leído. Registros: 41\n",
      "Archivo 'unificado_2023.txt' leído. Registros: 43\n",
      "Archivo 'unificado_2024.txt' leído. Registros: 75\n",
      "Archivo 'unificado_2025.txt' leído. Registros: 31\n",
      "\n",
      "--- Realizando validación de registros ---\n",
      "Suma de registros de todos los archivos: 216\n",
      "Registros en el consolidado final: 216\n",
      "¡Validación exitosa! El número de registros coincide.\n",
      "\n",
      "Proceso completado. El consolidado final se ha guardado en: C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\\R5_consolidado.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidar_archivos_y_validar(input_folder):\n",
    "    \"\"\"\n",
    "    Consolida todos los archivos .txt de un directorio en un único archivo,\n",
    "    y valida el número total de registros.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): La ruta a la carpeta que contiene los archivos .txt a unificar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ruta de la carpeta donde están los archivos unificados por año\n",
    "    ruta_consolidado_anual = os.path.join(input_folder, \"All\")\n",
    "    \n",
    "    # Ruta de salida para el consolidado final\n",
    "    output_file = os.path.join(input_folder, \"R5_consolidado.txt\")\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    lista_dfs = []\n",
    "    \n",
    "    # Variable para llevar el conteo total de registros\n",
    "    total_registros_sumados = 0\n",
    "\n",
    "    try:\n",
    "        # Obtener la lista de archivos .txt en la carpeta \"All\"\n",
    "        archivos_txt = [f for f in os.listdir(ruta_consolidado_anual) if f.endswith(\".txt\")]\n",
    "        \n",
    "        if not archivos_txt:\n",
    "            print(f\"No se encontraron archivos .txt en la carpeta: {ruta_consolidado_anual}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Iniciando la consolidación de {len(archivos_txt)} archivos...\")\n",
    "        \n",
    "        # Iterar sobre cada archivo para leerlo y procesarlo\n",
    "        for nombre_archivo in archivos_txt:\n",
    "            ruta_archivo = os.path.join(ruta_consolidado_anual, nombre_archivo)\n",
    "            \n",
    "            try:\n",
    "                # Leer el archivo con pandas. Ahora tiene encabezado y separador \",\"\n",
    "                # dtype=str para mantener todos los datos como texto\n",
    "                df_temp = pd.read_csv(ruta_archivo, sep=',', encoding='utf-8', dtype=str, on_bad_lines='skip')\n",
    "                \n",
    "                # Obtener el número de registros (filas) de este archivo\n",
    "                num_registros_archivo = len(df_temp)\n",
    "                print(f\"Archivo '{nombre_archivo}' leído. Registros: {num_registros_archivo}\")\n",
    "                \n",
    "                # Sumar los registros para la validación final\n",
    "                total_registros_sumados += num_registros_archivo\n",
    "                \n",
    "                # Añadir el DataFrame a la lista\n",
    "                lista_dfs.append(df_temp)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al leer el archivo '{nombre_archivo}'. Se omitirá este archivo. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Consolidar todos los DataFrames en uno solo\n",
    "        if lista_dfs:\n",
    "            consolidado_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "            num_registros_consolidado = len(consolidado_final)\n",
    "\n",
    "            # --- Validación de la suma de registros ---\n",
    "            print(\"\\n--- Realizando validación de registros ---\")\n",
    "            print(f\"Suma de registros de todos los archivos: {total_registros_sumados}\")\n",
    "            print(f\"Registros en el consolidado final: {num_registros_consolidado}\")\n",
    "            \n",
    "            if total_registros_sumados == num_registros_consolidado:\n",
    "                print(\"¡Validación exitosa! El número de registros coincide.\")\n",
    "                \n",
    "                # Guardar el consolidado final\n",
    "                consolidado_final.to_csv(output_file, index=False, sep=',', encoding='utf-8')\n",
    "                print(f\"\\nProceso completado. El consolidado final se ha guardado en: {output_file}\")\n",
    "            else:\n",
    "                print(\"¡ERROR DE VALIDACIÓN! El número de registros no coincide. El archivo final NO se ha guardado.\")\n",
    "                print(\"Por favor, revise los archivos de origen.\")\n",
    "        else:\n",
    "            print(\"No se encontraron archivos válidos para consolidar. No se creará el archivo final.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: La carpeta '{ruta_consolidado_anual}' no existe. Por favor, verifique la ruta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error inesperado durante el proceso de consolidación. Error: {e}\")\n",
    "\n",
    "# --- Uso de la función ---\n",
    "# La ruta de entrada es la carpeta padre que contiene la subcarpeta \"All\"\n",
    "input_folder_path = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\R5\\R5 TXT\"\n",
    "\n",
    "consolidar_archivos_y_validar(input_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
