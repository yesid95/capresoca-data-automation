{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Para trabajar con dataframes\n",
    "import openpyxl  # Para manipular archivos Excel\n",
    "from pathlib import Path  # Para manejar rutas del sistema\n",
    "from datetime import datetime  # Para trabajar con fechas\n",
    "import matplotlib.pyplot as plt  # Para crear gráficos\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows  # Para convertir dataframes a filas de Excel\n",
    "from openpyxl.chart import BarChart, Reference  # Para crear gráficos en Excel\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_MS_ADRES_EPS025 = r\"\\\\servernas\\AYC2\\ASEGURAMIENTO\\ASEGURAMIENTO\\PROCESO_ASEGURAMIENTO\\REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\CONSOLIDADOS\\04_Abril\\20250424-CONSOLIDADOS\\EPS025MS0024042025.TXT\"\n",
    "MS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validación Archivos Maestro_2025_04_29.csv\"\n",
    "R_NS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\03_Marzo\\21\\SIE_NSEPS02521032025.txt\"\n",
    "R_NS_Glosa = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\03_Marzo\\21\\yesid 21-03-2025.xlsx\"\n",
    "R_IPS = r\"\\\\Servernas\\AYC2\\ASEGURAMIENTO\\ASEGURAMIENTO\\PROCESO_ASEGURAMIENTO\\REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\REPORTE RESOLUCION_0762_2023\\IPS_CODIGO.txt\"\n",
    "\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\04_Abril\\29\\Salida.TXT\"\n",
    "R_Salida_Excel = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\04_Abril\\29\\Salida.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms_SIE = pd.read_csv(MS_SIE, sep=';', encoding='ansi', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms_EPS025 = pd.read_csv(R_MS_ADRES_EPS025, sep=',', encoding='ansi', header=None, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_30688\\2100155474.py:15: ParserWarning: Both a converter and dtype were specified for column Columna16 - only the converter will be used.\n",
      "  Df_NS_VAL = pd.read_excel(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def conv_col16(x):\n",
    "    # Si el valor es nulo, retorna cadena vacía\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    # Si es un objeto datetime (o Timestamp), formatea a dd/mm/yyyy\n",
    "    if isinstance(x, (pd.Timestamp, datetime.datetime)):\n",
    "        return x.strftime(\"%d/%m/%Y\")\n",
    "    # En caso contrario, lo convierte a string sin modificar\n",
    "    return str(x)\n",
    "\n",
    "# Cargar la hoja \"NS_Enviar\" del archivo Excel, forzando que Columna16 se lea con el converter\n",
    "Df_NS_VAL = pd.read_excel(\n",
    "    R_NS_Glosa,\n",
    "    sheet_name=\"NS_Enviar\",\n",
    "    dtype=str,  # Trata todas las columnas como str por defecto\n",
    "    header=0,   # Primera fila como encabezado\n",
    "    converters={\"Columna16\": conv_col16}  # Forzar Columna16 con la función definida\n",
    ")\n",
    "\n",
    "# Asegurar que \"Columna10\" sea de 2 dígitos\n",
    "Df_NS_VAL[\"Columna10\"] = Df_NS_VAL[\"Columna10\"].str.zfill(2)\n",
    "\n",
    "# Convertir la columna \"Columna9\" a tipo fecha en formato dd/mm/yyyy\n",
    "Df_NS_VAL[\"Columna9\"] = pd.to_datetime(\n",
    "    Df_NS_VAL[\"Columna9\"], \n",
    "    format='%Y-%m-%d %H:%M:%S', \n",
    "    errors='coerce'\n",
    ").dt.strftime('%d/%m/%Y')\n",
    "\n",
    "Df_NS_VAL[\"Columna13\"] = pd.to_datetime(\n",
    "    Df_NS_VAL[\"Columna13\"], \n",
    "    format='%Y-%m-%d %H:%M:%S', \n",
    "    errors='coerce'\n",
    ").dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Asegurar que \"Columna11\" sea de 3 dígitos\n",
    "Df_NS_VAL[\"Columna11\"] = Df_NS_VAL[\"Columna11\"].str.zfill(3)\n",
    "\n",
    "# Agregar una nueva columna \"origen\" con el valor \"Glosa\" para todos los registros\n",
    "Df_NS_VAL[\"origen\"] = \"Glosa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar la hoja \"NS_Noenviar\" del archivo Excel\n",
    "Df_NS_NEG = pd.read_excel(\n",
    "    R_NS_Glosa,\n",
    "    sheet_name=\"NS_Noenviar\",\n",
    "    dtype=str,  # Todas las columnas como tipo str\n",
    "    header=0  # Primera fila como encabezado\n",
    ")\n",
    "\n",
    "# Asegurar que \"Columna10\" sea de 2 dígitos\n",
    "Df_NS_NEG[\"Columna10\"] = Df_NS_NEG[\"Columna10\"].str.zfill(2)\n",
    "# Convertir la columna \"Columna9\" a tipo fecha en formato dd/mm/yyyy\n",
    "Df_NS_NEG[\"Columna9\"] = pd.to_datetime(Df_NS_NEG[\"Columna9\"], format='%Y-%m-%d %H:%M:%S', errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "Df_NS_NEG[\"Columna13\"] = pd.to_datetime(Df_NS_NEG[\"Columna13\"], format='%Y-%m-%d %H:%M:%S', errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "# Asegurar que \"Columna11\" sea de 3 dígitos\n",
    "Df_NS_NEG[\"Columna11\"] = Df_NS_NEG[\"Columna11\"].str.zfill(3)\n",
    "Df_NS_NEG[\"origen\"] = \"No_Enviar_glosa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo .TXT en un DataFrame\n",
    "Df_NS_SIE = pd.read_csv(\n",
    "    R_NS_SIE,\n",
    "    encoding='ansi',\n",
    "    header=None,\n",
    "    delimiter=',',\n",
    "    dtype=str,  # Todas las columnas como tipo str\n",
    "    keep_default_na=False  # Evitar conversiones automáticas a NaN\n",
    ")\n",
    "\n",
    "# Convertir la columna 8 a tipo fecha en formato dd/mm/yyyy\n",
    "Df_NS_SIE[8] = pd.to_datetime(Df_NS_SIE[8], format='%d/%m/%Y', errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "Df_NS_SIE[12] = pd.to_datetime(Df_NS_SIE[12], format='%d/%m/%Y', errors='coerce').dt.strftime('%d/%m/%Y')\n",
    "# Agregar una nueva columna llamada \"origen\" con el valor \"Glosa\" para todos los registros\n",
    "Df_NS_SIE[\"origen\"] = \"SIE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Columna1 Columna2 Columna3         Columna4   Columna5       Columna6  \\\n",
      "0          1   EPS025       CN   24119311215391       COBA           CRUZ   \n",
      "1          2   EPS025       CN   24111411214588      PEREZ        SANCHEZ   \n",
      "2          3   EPS025       CN   24118511216313     BLANCO         VARGAS   \n",
      "3          4   EPS025       TI       1119666210      SEGUA        GUANARO   \n",
      "4          5   EPS025       PE  838688025081979     GARCIA        GIMENEZ   \n",
      "5          6   EPS025       TI       1119510485      PEREZ         PRIETO   \n",
      "6          7   EPS025       CC       1118146693      JAIME          SALON   \n",
      "7          8   EPS025       CN   25037610069451   ESPINOSA     MENDIVELSO   \n",
      "8          9   EPS025       RC       1115865687     PUERTA      CRISTIANO   \n",
      "9         10   EPS025       RC       1115921009     CASTRO           DIAZ   \n",
      "10        13   EPS025       TI       1116040693    TARACHE        ANZUETA   \n",
      "11        24   EPS025       RC       1115692308    HURTADO         MILLAN   \n",
      "12        26   EPS025       CN   24117411203018       SOSA        MARQUEZ   \n",
      "13        27   EPS025       CN   24103031170413      SIGUA          GOMEZ   \n",
      "138        1   EPS025       AS       85230I0080     HOROPA    GUACARAPARE   \n",
      "139        2   EPS025       AS       85250I0043   CAMBIARI            NaN   \n",
      "140        3   EPS025       CC         41615427       VERA  DE VALENZUELA   \n",
      "141        4   EPS025       CC         47439906     VARGAS        MARQUEZ   \n",
      "142        5   EPS025       CC          4300709  RODRIGUEZ         CHAVES   \n",
      "143        6   EPS025       CC         23936722   MARTINEZ           CRUZ   \n",
      "144        7   EPS025       CC         23724405      TOVAR        CUELLAR   \n",
      "145        8   EPS025       CC         23751608     VARELA     DE ALFONSO   \n",
      "146        9   EPS025       PT          3745661     BRACHO          MUÑOZ   \n",
      "147       10   EPS025       PT          7952473      SILVA      SOLORZANO   \n",
      "148       11   EPS025       CC         23789114      MARIN            NaN   \n",
      "149       12   EPS025       CC         47428705      BELLO          GALAN   \n",
      "150       13   EPS025       CC         23789759    BENITEZ          ABRIL   \n",
      "151       14   EPS025       CC         23943128   CARRILLO     DE BARINAS   \n",
      "152       15   EPS025       CC         23709683      SIGUA       CHAPARRO   \n",
      "153       16   EPS025       CC         74849386       SUEZ         FLOREZ   \n",
      "154       17   EPS025       CC         47440795   LIBERATO       CHAPARRO   \n",
      "155       18   EPS025       PT          6659953   QUINTANA         VARELA   \n",
      "156       19   EPS025       PT          7777713   GONZALEZ         SOLANO   \n",
      "157       20   EPS025       CC          7360172      OJEDA            NaN   \n",
      "158       21   EPS025       CC          9528921     VARGAS       CARDENAS   \n",
      "159       22   EPS025       PT          5622937  RODRIGUEZ         GARCIA   \n",
      "173       12   EPS025       CN   24117411203018       SOSA        MARQUEZ   \n",
      "176       15   EPS025       CN   24103031170413      SIGUA          GOMEZ   \n",
      "179       18   EPS025       CN   25039810069067     GAMBOA           VEGA   \n",
      "182       21   EPS025      RC        1115921009     CASTRO           DIAZ   \n",
      "\n",
      "       Columna7    Columna8    Columna9 Columna10  ... Columna12   Columna13  \\\n",
      "0       HIJO DE         NaN  23/11/2024        85  ...       N01  14/03/2025   \n",
      "1       HIJO DE         NaN  22/11/2024        85  ...       N01  14/03/2025   \n",
      "2       HIJO DE         NaN  23/11/2024        85  ...       N01  14/03/2025   \n",
      "3    LEOVIGILDO         NaN  01/02/2005        85  ...       N01  07/03/2025   \n",
      "4        YESIKA       LUISA  25/08/1979        85  ...       N01  03/03/2025   \n",
      "5         LAURA       SOFIA  03/01/2006        85  ...       N01  06/03/2025   \n",
      "6          JOSE     JOAQUIN  08/04/1959        85  ...       N01  20/02/2025   \n",
      "7       HIJO DE         NaN  01/03/2025        85  ...       N01  27/03/2025   \n",
      "8         ZAIRA    EVANYELI  19/07/2016        85  ...       N01  27/02/2025   \n",
      "9        EITHAN      JAVIER  27/01/2025        85  ...       N01  20/02/2025   \n",
      "10       ANDREA   KATHERINE  06/01/2007        85  ...       N01  04/02/2025   \n",
      "11        AMARI     LUCIANA  12/01/2018        85  ...       N01  13/03/2025   \n",
      "12      HIJO DE         NaN  20/12/2024        85  ...       N01  01/01/2025   \n",
      "13      HIJO DE         NaN  20/11/2024        85  ...       N01  01/01/2025   \n",
      "138      CARMEN       LUCIA  22/04/1974        85  ...       N01  20/03/2025   \n",
      "139   CARMELITA         NaN  18/07/1928        85  ...       N01  20/03/2025   \n",
      "140       MARIA     OTALVIS  14/02/1951        85  ...       N01  20/03/2025   \n",
      "141      VIANEY   CONSTANZA  07/10/1981        85  ...       N01  20/03/2025   \n",
      "142     GONZALO         NaN  28/06/1948        85  ...       N01  20/03/2025   \n",
      "143       MARIA   ENRIQUETA  01/11/1946        85  ...       N01  20/03/2025   \n",
      "144        DORA         NaN  12/04/1945        85  ...       N01  20/03/2025   \n",
      "145       MARIA       ELVIA  18/08/1948        85  ...       N01  20/03/2025   \n",
      "146    FRANGELY         NaN  27/07/1996        85  ...       N01  20/03/2025   \n",
      "147  WILLIANNYS    GERAMIAS  15/01/2015        85  ...       N01  20/03/2025   \n",
      "148      ALCIRA         NaN  29/09/1956        85  ...       N01  20/03/2025   \n",
      "149      TERESA         NaN  28/02/1968        85  ...       N01  20/03/2025   \n",
      "150      BLANCA      RUBITH  31/01/1962        85  ...       N01  20/03/2025   \n",
      "151       MARIA  DEL CARMEN  17/12/1953        85  ...       N01  20/03/2025   \n",
      "152        NERY         NaN  10/09/1964        85  ...       N01  20/03/2025   \n",
      "153      VICTOR      FERNEL  18/05/1981        85  ...       N01  20/03/2025   \n",
      "154       NIDIA      MARLEY  20/01/1983        85  ...       N01  20/03/2025   \n",
      "155    JESUANNI       SUSEJ  04/07/2007        85  ...       N01  20/03/2025   \n",
      "156      DAILIN  DEL CARMEN  14/11/2016        85  ...       N01  20/03/2025   \n",
      "157        LUIS       EMIRO  20/06/1954        85  ...       N01  20/03/2025   \n",
      "158      ISRAEL         NaN  25/12/1954        85  ...       N01  20/03/2025   \n",
      "159        JOSE      RAFAEL  16/05/1947        85  ...       N01  20/03/2025   \n",
      "173     HIJO DE         NaN  20/12/2024        85  ...       N01  03/12/2024   \n",
      "176     HIJO DE         NaN  20/11/2024        85  ...       N01  01/12/2024   \n",
      "179     HIJO DE         NaN  01/03/2025        85  ...       N01  04/03/2025   \n",
      "182      EITHAN      JAVIER  27/01/2025        85  ...       N01  04/02/2025   \n",
      "\n",
      "    Columna14   Columna15   Columna16 Columna17 Columna18 Columna19 Columna20  \\\n",
      "0          RC  1115871297  23/11/2024         0       NaN       NaN       NaN   \n",
      "1          RC  1222146798  22/11/2024         0       NaN       NaN       NaN   \n",
      "2          RC  1115920980  23/11/2024         0       NaN       NaN       NaN   \n",
      "3          CC  1119666210  01/02/2005         0       NaN       NaN       NaN   \n",
      "4          PT     4546997  25/08/1979         0       NaN       NaN       NaN   \n",
      "5          CC  1119510485  03/01/2006         0       NaN       NaN       NaN   \n",
      "6          CC  1118146867  08/04/1959         1       NaN       NaN       NaN   \n",
      "7          RC  1118100673  01/03/2025         0       NaN       NaN       NaN   \n",
      "8          TI  1115865687  01/07/2016         0       NaN       NaN       NaN   \n",
      "9          RC  1115921012  27/01/2025         1       NaN       NaN       NaN   \n",
      "10         CC   116040693  06/01/2007         0       NaN       NaN       NaN   \n",
      "11         TI  1115692308  12/01/2018         0       NaN       NaN       NaN   \n",
      "12         RC  1057615298  20/12/2024         0       NaN       NaN       NaN   \n",
      "13         RC  1115871291  20/11/2024         0       NaN       NaN       NaN   \n",
      "138        AS  85230I0080  23/04/1974         1       NaN       NaN         1   \n",
      "139        AS  85250I0043  19/07/1928         1       NaN       NaN       NaN   \n",
      "140        CC    41615427  15/02/1951         1       NaN       NaN       NaN   \n",
      "141        CC    47439906  08/10/1981         1       NaN       NaN       NaN   \n",
      "142        CC     4300709  29/06/1948         1       NaN       NaN       NaN   \n",
      "143        CC    23936722  02/11/1946         1       NaN       NaN       NaN   \n",
      "144        CC    23724405  13/04/1945         1       NaN       NaN       NaN   \n",
      "145        CC    23751608  19/08/1948         1       NaN       NaN       NaN   \n",
      "146        PT     3745661  28/07/1996         1       NaN       NaN       NaN   \n",
      "147        PT     7952473  16/01/2015         1       NaN       NaN       NaN   \n",
      "148        CC    23789114  30/09/1956         1       NaN       NaN       NaN   \n",
      "149        CC    47428705  29/02/1968         1       NaN       NaN       NaN   \n",
      "150        CC    23789759  01/02/1962         1       NaN       NaN       NaN   \n",
      "151        CC    23943128  18/12/1953         1       NaN       NaN       NaN   \n",
      "152        CC    23709683  11/09/1964         1       NaN       NaN       NaN   \n",
      "153        CC    74849386  19/05/1981         1       NaN       NaN       NaN   \n",
      "154        CC    47440795  21/01/1983         1       NaN       NaN       NaN   \n",
      "155        PT     6659953  05/07/2007         1       NaN       NaN       NaN   \n",
      "156        PT     7777713  15/11/2016         1       NaN       NaN       NaN   \n",
      "157        CC     7360172  21/06/1954         1       NaN       NaN       NaN   \n",
      "158        CC     9528921  26/12/1954         1       NaN       NaN       NaN   \n",
      "159        PT     5622937  17/05/1947         1       NaN       NaN       NaN   \n",
      "173        RC  1057615298  20/12/2024         0       NaN       NaN       NaN   \n",
      "176        RC  1115871291  20/11/2024         0       NaN       NaN       NaN   \n",
      "179        RC  1222147061  01/03/2025         0       NaN       NaN       NaN   \n",
      "182        RC  1115921012  27/01/2025         1       NaN       NaN       NaN   \n",
      "\n",
      "    origen  \n",
      "0    Glosa  \n",
      "1    Glosa  \n",
      "2    Glosa  \n",
      "3    Glosa  \n",
      "4    Glosa  \n",
      "5    Glosa  \n",
      "6    Glosa  \n",
      "7    Glosa  \n",
      "8    Glosa  \n",
      "9    Glosa  \n",
      "10   Glosa  \n",
      "11   Glosa  \n",
      "12   Glosa  \n",
      "13   Glosa  \n",
      "138  Glosa  \n",
      "139  Glosa  \n",
      "140  Glosa  \n",
      "141  Glosa  \n",
      "142  Glosa  \n",
      "143  Glosa  \n",
      "144  Glosa  \n",
      "145  Glosa  \n",
      "146  Glosa  \n",
      "147  Glosa  \n",
      "148  Glosa  \n",
      "149  Glosa  \n",
      "150  Glosa  \n",
      "151  Glosa  \n",
      "152  Glosa  \n",
      "153  Glosa  \n",
      "154  Glosa  \n",
      "155  Glosa  \n",
      "156  Glosa  \n",
      "157  Glosa  \n",
      "158  Glosa  \n",
      "159  Glosa  \n",
      "173  Glosa  \n",
      "176  Glosa  \n",
      "179  Glosa  \n",
      "182  Glosa  \n",
      "\n",
      "[40 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtrar los registros donde la columna en la posición 11 sea igual a \"N01\"\n",
    "registros_n01 = Df_NS_VAL[Df_NS_VAL.iloc[:, 11] == \"N01\"]\n",
    "\n",
    "# Imprimir los registros filtrados\n",
    "print(registros_n01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenación completada con éxito.\n",
      "Número total de registros en Df_NS_VAL: 702\n"
     ]
    }
   ],
   "source": [
    "# 1. Verificar que ambos DF tengan la misma cantidad de columnas\n",
    "if Df_NS_SIE.shape[1] != Df_NS_VAL.shape[1]:\n",
    "    raise ValueError(\"Los DataFrames tienen distinto número de columnas. No se pueden concatenar directamente.\")\n",
    "\n",
    "# 2. Convertir todo a texto en ambos DF\n",
    "# Se rellena NaN con cadena vacía para evitar que se propaguen como nulos\n",
    "for col in Df_NS_VAL.columns:\n",
    "    Df_NS_VAL[col] = Df_NS_VAL[col].fillna('').astype(str)\n",
    "\n",
    "for col_index in range(Df_NS_SIE.shape[1]):\n",
    "    Df_NS_SIE.iloc[:, col_index] = Df_NS_SIE.iloc[:, col_index].fillna('').astype(str)\n",
    "\n",
    "# 3. Renombrar las columnas de Df_NS_SIE para que sean idénticas a las de Df_NS_VAL\n",
    "Df_NS_SIE.columns = Df_NS_VAL.columns\n",
    "\n",
    "# 4. Concatenar\n",
    "Df_NS_VAL = pd.concat([Df_NS_VAL, Df_NS_SIE], ignore_index=True)\n",
    "\n",
    "print(\"Concatenación completada con éxito.\")\n",
    "print(\"Número total de registros en Df_NS_VAL:\", len(Df_NS_VAL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Columna1 Columna2 Columna3         Columna4  Columna5  Columna6  \\\n",
      "0          1   EPS025       CN   24119311215391      COBA      CRUZ   \n",
      "1          2   EPS025       CN   24111411214588     PEREZ   SANCHEZ   \n",
      "2          3   EPS025       CN   24118511216313    BLANCO    VARGAS   \n",
      "3          4   EPS025       TI       1119666210     SEGUA   GUANARO   \n",
      "4          5   EPS025       PE  838688025081979    GARCIA   GIMENEZ   \n",
      "..       ...      ...      ...              ...       ...       ...   \n",
      "592      408   EPS025       RC       1115867456    POVEDA    DEDIOS   \n",
      "599      415   EPS025       RC       1118125389     GAMEZ    MATEUS   \n",
      "611      427   EPS025       TI       1118539734   VALLEJO   RIVEROS   \n",
      "626      442   EPS025       CN   25022010054010  GONZALEZ  PE ALOZA   \n",
      "639      455   EPS025       CN   25037010072762    ROMERO    PINEDA   \n",
      "\n",
      "       Columna7 Columna8    Columna9 Columna10  ... Columna12   Columna13  \\\n",
      "0       HIJO DE           23/11/2024        85  ...       N01  14/03/2025   \n",
      "1       HIJO DE           22/11/2024        85  ...       N01  14/03/2025   \n",
      "2       HIJO DE           23/11/2024        85  ...       N01  14/03/2025   \n",
      "3    LEOVIGILDO           01/02/2005        85  ...       N01  07/03/2025   \n",
      "4        YESIKA    LUISA  25/08/1979        85  ...       N01  03/03/2025   \n",
      "..          ...      ...         ...       ...  ...       ...         ...   \n",
      "592       ANGIE    LUCIA  06/03/2018        85  ...       N01  20/03/2025   \n",
      "599      VALERY  MICHELL  07/08/2016        85  ...       N01  20/03/2025   \n",
      "611       JOHAN  SNEYDER  15/12/2006        85  ...       N01  20/03/2025   \n",
      "626     HIJO DE           16/02/2025        85  ...       N01  20/03/2025   \n",
      "639     HIJO DE           04/03/2025        85  ...       N01  10/03/2025   \n",
      "\n",
      "    Columna14   Columna15   Columna16 Columna17 Columna18 Columna19 Columna20  \\\n",
      "0          RC  1115871297  23/11/2024         0                                 \n",
      "1          RC  1222146798  22/11/2024         0                                 \n",
      "2          RC  1115920980  23/11/2024         0                                 \n",
      "3          CC  1119666210  01/02/2005         0                                 \n",
      "4          PT     4546997  25/08/1979         0                                 \n",
      "..        ...         ...         ...       ...       ...       ...       ...   \n",
      "592        TI  1115867456  06/03/2018         0                                 \n",
      "599        TI  1118125389  07/08/2016         0                                 \n",
      "611        CC  1118539734  15/12/2006         0                                 \n",
      "626        RC  1115871388  16/02/2025         0                                 \n",
      "639        RC  1115871384  04/03/2025         0                                 \n",
      "\n",
      "    origen  \n",
      "0    Glosa  \n",
      "1    Glosa  \n",
      "2    Glosa  \n",
      "3    Glosa  \n",
      "4    Glosa  \n",
      "..     ...  \n",
      "592    SIE  \n",
      "599    SIE  \n",
      "611    SIE  \n",
      "626    SIE  \n",
      "639    SIE  \n",
      "\n",
      "[64 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtrar los registros donde la columna en la posición 11 sea igual a \"N01\"\n",
    "registros_n01 = Df_NS_VAL[Df_NS_VAL.iloc[:, 11] == \"N01\"]\n",
    "\n",
    "# Imprimir los registros filtrados\n",
    "print(registros_n01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N01' 'N02' 'N03' 'N17' 'N21' 'N25' 'N39' 'N09' 'N04' 'N19' 'N14' 'N36'\n",
      " 'N43' 'N46' 'N32' 'N12']\n"
     ]
    }
   ],
   "source": [
    "valores_unicos = Df_NS_VAL.iloc[:, 11].unique()\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros donde la columna en la posición 11 sea igual a ['N43', 'N46']\n",
    "registros_a_mover = Df_NS_VAL[Df_NS_VAL.iloc[:, 11].isin(['N12', 'N43', 'N46'])]\n",
    "\n",
    "# Eliminar esos registros de Df_NS_VAL\n",
    "Df_NS_VAL = Df_NS_VAL[~Df_NS_VAL.iloc[:, 11].isin(['N12', 'N43', 'N46'])]\n",
    "\n",
    "# Agregar los registros filtrados a Df_NS_NEG\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, registros_a_mover], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N01' 'N02' 'N03' 'N17' 'N21' 'N25' 'N39' 'N09' 'N04' 'N19' 'N14' 'N36'\n",
      " 'N32']\n"
     ]
    }
   ],
   "source": [
    "valores_unicos = Df_NS_VAL.iloc[:, 11].unique()\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1) Crear un diccionario a partir de df_maestro ===\n",
    "#    - key   -> (valorCol4, valorCol5)\n",
    "#    - value -> el contenido de la columna [1], donde puede estar \"EPSC25\"\n",
    "maestro_dict = dict(zip(\n",
    "    df_maestro.iloc[:, [4, 5]].apply(tuple, axis=1),  # Genera tuplas (col4, col5)\n",
    "    df_maestro.iloc[:, 1]                             # Valor = contenido col1 (donde aparece \"EPSC25\")\n",
    "))\n",
    "\n",
    "# === 2) Definir una función que, para cada fila de Df_NS_VAL, decida la acción ===\n",
    "def decidir_origen(row):\n",
    "    \"\"\"\n",
    "    row es una fila (Series) de Df_NS_VAL.\n",
    "    - Las columnas [2, 3] se usarán para armar la clave (key).\n",
    "    - La columna [11] se usa para ver si es N02 o N03.\n",
    "    Devuelve un string que indica la acción:\n",
    "       \"EPSC25\"   -> Mover a Df_NS_NEG con origen \"NS-EPSC25\"\n",
    "       \"NO_APARE\" -> Mover a Df_NS_NEG con origen \"No Aparecen MS ADRES\"\n",
    "       \"KEEP\"     -> Mantener en Df_NS_VAL (no se mueve)\n",
    "    \"\"\"\n",
    "    # Tomamos las columnas [2, 3] como clave\n",
    "    key = (row.iloc[2], row.iloc[3])\n",
    "    # Tomamos la columna [11] para ver si es \"N02\" o \"N03\"\n",
    "    valor_col11 = row.iloc[11]\n",
    "\n",
    "    if key in maestro_dict:\n",
    "        # El registro (col2, col3) aparece en df_maestro\n",
    "        if maestro_dict[key] == \"EPSC25\":\n",
    "            # Sí está en df_maestro y además la columna [1] = \"EPSC25\"\n",
    "            return \"EPSC25\"\n",
    "        else:\n",
    "            # Aparece en df_maestro pero no es \"EPSC25\"\n",
    "            return \"KEEP\"\n",
    "    else:\n",
    "        # No aparece en df_maestro\n",
    "        # Si la columna [11] es N02 o N03, se queda en Df_NS_VAL\n",
    "        if valor_col11 in [\"N02\", \"N03\"]:\n",
    "            return \"KEEP\"\n",
    "        else:\n",
    "            return \"NO_APARE\"\n",
    "\n",
    "# === 3) Aplicar la función a cada fila para decidir el destino de cada registro ===\n",
    "decisiones = Df_NS_VAL.apply(decidir_origen, axis=1)\n",
    "\n",
    "# === 4) Crear DataFrames intermedios para los que se mueven a Df_NS_NEG ===\n",
    "df_move_EPSC25 = Df_NS_VAL[decisiones == \"EPSC25\"].copy()\n",
    "df_move_EPSC25[\"origen\"] = \"NS-EPSC25\"\n",
    "\n",
    "df_move_no_aparecen = Df_NS_VAL[decisiones == \"NO_APARE\"].copy()\n",
    "df_move_no_aparecen[\"origen\"] = \"No Aparecen MS ADRES\"\n",
    "\n",
    "# === 5) Concatenar todo en Df_NS_NEG y eliminar los que se mueven de Df_NS_VAL ===\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_move_EPSC25, df_move_no_aparecen], ignore_index=True)\n",
    "\n",
    "# Mantener en Df_NS_VAL solo los que tienen \"KEEP\"\n",
    "Df_NS_VAL = Df_NS_VAL[decisiones == \"KEEP\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 validar N01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_30688\\3458659767.py:8: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, dayfirst=True, infer_datetime_format=True, errors='raise')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Función para formatear la fecha de la columna 15\n",
    "def format_date(val):\n",
    "    s = str(val).strip().strip(\"'\")  # Elimina espacios y comillas simples\n",
    "    try:\n",
    "        # Intentamos convertir usando infer_datetime_format y dayfirst\n",
    "        dt = pd.to_datetime(s, dayfirst=True, infer_datetime_format=True, errors='raise')\n",
    "        return dt.strftime('%d/%m/%Y')\n",
    "    except Exception:\n",
    "        # Si falla la conversión, devolvemos el valor original\n",
    "        return s\n",
    "\n",
    "# Función que procesa una fila: formatea la fecha de col15 y valida col16\n",
    "def process_row(row):\n",
    "    # Convertir la columna 15 (índice 15) a fecha formateada\n",
    "    row.iloc[15] = format_date(row.iloc[15])\n",
    "    \n",
    "    # Validar la columna 16 (índice 16): si no es \"0\", \"1\" o \"2\", o está vacío, asignar \"0\"\n",
    "    val16 = str(row.iloc[16]).strip()\n",
    "    if val16 not in [\"0\", \"1\", \"2\"]:\n",
    "        row.iloc[16] = \"0\"\n",
    "    return row\n",
    "\n",
    "# 1. Filtrar filas donde la columna [11] es \"N01\"\n",
    "mask_n01 = (Df_NS_VAL.iloc[:, 11] == \"N01\")\n",
    "df_n01 = Df_NS_VAL.loc[mask_n01].copy()\n",
    "\n",
    "# 2. Aplicar la función de procesamiento a cada fila de df_n01\n",
    "df_n01 = df_n01.apply(process_row, axis=1)\n",
    "\n",
    "# 3. Reemplazar las filas procesadas en Df_NS_VAL\n",
    "Df_NS_VAL.loc[mask_n01] = df_n01\n",
    "\n",
    "print(\"Proceso completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 validar N02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso para registros 'N02' completado.\n",
      "Registros movidos a Df_NS_NEG: 2\n",
      "Registros restantes en Df_NS_VAL: 551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_30688\\2244516542.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  n01_tuples = set(Df_NS_VAL.loc[n01_mask, Df_NS_VAL.columns[[13,14]]].apply(lambda row: (row[0], row[1]), axis=1))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- PREPARACIÓN DE DATOS ---\n",
    "# 1. Crear un conjunto y diccionario a partir de df_maestro\n",
    "# El ID en df_maestro está en las columnas [4, 5] y el par de validación está en las columnas [8, 9]\n",
    "maestro_ids = df_maestro.iloc[:, [4,5]].apply(tuple, axis=1)\n",
    "maestro_dict = dict(zip(\n",
    "    maestro_ids,\n",
    "    df_maestro.iloc[:, 8:10].apply(tuple, axis=1)  # Asume que las columnas 8 y 9 son adyacentes (8:10)\n",
    "))\n",
    "\n",
    "# 2. Extraer el conjunto de pares (Columna13, Columna14) de los registros \"N01\" en Df_NS_VAL\n",
    "n01_mask = (Df_NS_VAL.iloc[:, 11] == \"N01\")\n",
    "n01_tuples = set(Df_NS_VAL.loc[n01_mask, Df_NS_VAL.columns[[13,14]]].apply(lambda row: (row[0], row[1]), axis=1))\n",
    "\n",
    "# --- PROCESAMIENTO DE REGISTROS \"N02\" ---\n",
    "# Filtrar registros de Df_NS_VAL donde Columna11 es \"N02\"\n",
    "mask_n02 = (Df_NS_VAL.iloc[:, 11] == \"N02\")\n",
    "df_n02 = Df_NS_VAL.loc[mask_n02].copy()\n",
    "\n",
    "def process_N02(row):\n",
    "    # Primera validación: si Columna13 == Columna6 y Columna14 == Columna7, se mueve con \"Sin cambios de nombre\"\n",
    "    if row.iloc[13] == row.iloc[6] and row.iloc[14] == row.iloc[7]:\n",
    "        return \"mover_sin_cambios\"\n",
    "    else:\n",
    "        # Construir el ID a partir de Columnas2 y 3\n",
    "        id_val = (row.iloc[2], row.iloc[3])\n",
    "        # Validación A: El registro existe en df_maestro (ID en columnas [4,5])\n",
    "        if id_val in maestro_dict:\n",
    "            # Se valida que el par (Columna13, Columna14) de Df_NS_VAL sea igual al par (Columna8, Columna9) en df_maestro\n",
    "            if (row.iloc[13], row.iloc[14]) == maestro_dict[id_val]:\n",
    "                return \"mover_sin_cambios\"\n",
    "            else:\n",
    "                return \"mantener\"\n",
    "        else:\n",
    "            # Validación B: Para registros no existentes en df_maestro,\n",
    "            # se verifica si el ID (Columnas2 y 3) aparece en los registros \"N01\" (usando los pares en [13,14])\n",
    "            if id_val in n01_tuples:\n",
    "                return \"mantener\"\n",
    "            else:\n",
    "                return \"mover_no_existe\"\n",
    "\n",
    "# Aplicar la función a cada fila de df_n02\n",
    "df_n02['decision'] = df_n02.apply(process_N02, axis=1)\n",
    "\n",
    "# --- SEPARAR REGISTROS SEGÚN DECISIÓN ---\n",
    "# Registros a mover con \"Sin cambios de nombre\"\n",
    "move_sin_cambios = df_n02[df_n02['decision'] == \"mover_sin_cambios\"].copy()\n",
    "move_sin_cambios[\"origen\"] = \"Sin cambios de nombre\"\n",
    "\n",
    "# Registros a mover con \"No existe ADRES\"\n",
    "move_no_existe = df_n02[df_n02['decision'] == \"mover_no_existe\"].copy()\n",
    "move_no_existe[\"origen\"] = \"No existe ADRES\"\n",
    "\n",
    "# Combinar ambos grupos a mover\n",
    "df_to_move = pd.concat([move_sin_cambios, move_no_existe], ignore_index=True)\n",
    "\n",
    "# Actualizar Df_NS_NEG (suponiendo que ya existe y es un DataFrame previamente definido)\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar de Df_NS_VAL los registros que se han movido (aquellos que no se mantienen)\n",
    "Df_NS_VAL = Df_NS_VAL.drop(df_n02[df_n02['decision'] != \"mantener\"].index)\n",
    "\n",
    "print(\"Proceso para registros 'N02' completado.\")\n",
    "print(\"Registros movidos a Df_NS_NEG:\", len(df_to_move))\n",
    "print(\"Registros restantes en Df_NS_VAL:\", len(Df_NS_VAL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 N03 Apellidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso para registros 'N03' completado.\n",
      "Registros movidos a Df_NS_NEG: 0\n",
      "Registros restantes en Df_NS_VAL: 551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_30688\\287058666.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  n01_tuples = set(Df_NS_VAL.loc[n01_mask, Df_NS_VAL.columns[[13,14]]].apply(lambda row: (row[0], row[1]), axis=1))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- PREPARACIÓN DE DATOS ---\n",
    "# 1. Crear un conjunto y diccionario a partir de df_maestro\n",
    "# El ID en df_maestro está en las columnas [4, 5] y el par de validación está en las columnas [6, 7]\n",
    "maestro_ids = df_maestro.iloc[:, [4,5]].apply(tuple, axis=1)\n",
    "maestro_dict = dict(zip(\n",
    "    maestro_ids,\n",
    "    df_maestro.iloc[:, 6:8].apply(tuple, axis=1)  # Asume que las columnas 6 y 8 son adyacentes (6:8)\n",
    "))\n",
    "\n",
    "# 2. Extraer el conjunto de pares (Columna13, Columna14) de los registros \"N01\" en Df_NS_VAL\n",
    "n01_mask = (Df_NS_VAL.iloc[:, 11] == \"N01\")\n",
    "n01_tuples = set(Df_NS_VAL.loc[n01_mask, Df_NS_VAL.columns[[13,14]]].apply(lambda row: (row[0], row[1]), axis=1))\n",
    "\n",
    "# --- PROCESAMIENTO DE REGISTROS \"N03\" ---\n",
    "# Filtrar registros de Df_NS_VAL donde Columna11 es \"N03\"\n",
    "mask_n03 = (Df_NS_VAL.iloc[:, 11] == \"N03\")\n",
    "df_n03 = Df_NS_VAL.loc[mask_n02].copy()\n",
    "\n",
    "def process_N03(row):\n",
    "    # Primera validación: si Columna13 == Columna6 y Columna14 == Columna7, se mueve con \"Sin cambios de nombre\"\n",
    "    if row.iloc[13] == row.iloc[4] and row.iloc[14] == row.iloc[5]:\n",
    "        return \"mover_sin_cambios\"\n",
    "    else:\n",
    "        # Construir el ID a partir de Columnas2 y 3\n",
    "        id_val = (row.iloc[2], row.iloc[3])\n",
    "        # Validación A: El registro existe en df_maestro (ID en columnas [4,5])\n",
    "        if id_val in maestro_dict:\n",
    "            # Se valida que el par (Columna13, Columna14) de Df_NS_VAL sea igual al par (Columna8, Columna9) en df_maestro\n",
    "            if (row.iloc[13], row.iloc[14]) == maestro_dict[id_val]:\n",
    "                return \"mover_sin_cambios\"\n",
    "            else:\n",
    "                return \"mantener\"\n",
    "        else:\n",
    "            # Validación B: Para registros no existentes en df_maestro,\n",
    "            # se verifica si el ID (Columnas2 y 3) aparece en los registros \"N01\" (usando los pares en [13,14])\n",
    "            if id_val in n01_tuples:\n",
    "                return \"mantener\"\n",
    "            else:\n",
    "                return \"mover_no_existe\"\n",
    "\n",
    "# Aplicar la función a cada fila de df_n03\n",
    "df_n03['decision'] = df_n03.apply(process_N03, axis=1)\n",
    "\n",
    "# --- SEPARAR REGISTROS SEGÚN DECISIÓN ---\n",
    "# Registros a mover con \"Sin cambios de Apellidos\"\n",
    "move_sin_cambios = df_n03[df_n03['decision'] == \"mover_sin_cambios\"].copy()\n",
    "move_sin_cambios[\"origen\"] = \"Sin cambios de Apellidos\"\n",
    "\n",
    "# Registros a mover con \"No existe ADRES\"\n",
    "move_no_existe = df_n03[df_n03['decision'] == \"mover_no_existe\"].copy()\n",
    "move_no_existe[\"origen\"] = \"No existe ADRES\"\n",
    "\n",
    "# Combinar ambos grupos a mover\n",
    "df_to_move = pd.concat([move_sin_cambios, move_no_existe], ignore_index=True)\n",
    "\n",
    "# Actualizar Df_NS_NEG (suponiendo que ya existe y es un DataFrame previamente definido)\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_to_move], ignore_index=True)\n",
    "\n",
    "# Eliminar de Df_NS_VAL los registros que se han movido (aquellos que no se mantienen)\n",
    "Df_NS_VAL = Df_NS_VAL.drop(df_n03[df_n03['decision'] != \"mantener\"].index)\n",
    "\n",
    "print(\"Proceso para registros 'N03' completado.\")\n",
    "print(\"Registros movidos a Df_NS_NEG:\", len(df_to_move))\n",
    "print(\"Registros restantes en Df_NS_VAL:\", len(Df_NS_VAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 N04 Cambio de municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso para N04 y N02 completado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESAMIENTO PARA REGISTROS \"N04\"\n",
    "# -----------------------------\n",
    "\n",
    "# Filtrar registros N04 en Df_NS_VAL (columna 11 == \"N04\")\n",
    "mask_N04 = (Df_NS_VAL.iloc[:, 11] == \"N04\")\n",
    "df_N04 = Df_NS_VAL.loc[mask_N04].copy()\n",
    "\n",
    "def process_N04(row):\n",
    "    # Convertir columna 13 y 14 a numérico y luego formatear:\n",
    "    try:\n",
    "        num13 = pd.to_numeric(row.iloc[13], errors='coerce')\n",
    "        num14 = pd.to_numeric(row.iloc[14], errors='coerce')\n",
    "    except Exception:\n",
    "        num13, num14 = None, None\n",
    "\n",
    "    # Formatear a cadenas con dígitos fijos:\n",
    "    str13 = str(int(num13)).zfill(2) if pd.notna(num13) else \"\"\n",
    "    str14 = str(int(num14)).zfill(3) if pd.notna(num14) else \"\"\n",
    "    # Actualizar las columnas formateadas en la fila\n",
    "    row.iloc[13] = str13\n",
    "    row.iloc[14] = str14\n",
    "\n",
    "    # Validar: si la columna 13 es diferente de \"85\", se mueve a otro departamento.\n",
    "    if str13 != \"85\":\n",
    "        return \"mover_N04_otro_departamento\"\n",
    "    else:\n",
    "        # Si col13 es \"85\", se busca en df_maestro por id.\n",
    "        id_val = (row.iloc[2], row.iloc[3])\n",
    "        df_match = df_maestro[(df_maestro.iloc[:, 4] == id_val[0]) & (df_maestro.iloc[:, 5] == id_val[1])]\n",
    "        if not df_match.empty:\n",
    "            match = df_match.iloc[0]\n",
    "            # Extraer y formatear las columnas 18 y 19 de df_maestro\n",
    "            try:\n",
    "                m18 = pd.to_numeric(match.iloc[18], errors='coerce')\n",
    "                m19 = pd.to_numeric(match.iloc[19], errors='coerce')\n",
    "            except Exception:\n",
    "                m18, m19 = None, None\n",
    "            str18 = str(int(m18)).zfill(2) if pd.notna(m18) else \"\"\n",
    "            str19 = str(int(m19)).zfill(3) if pd.notna(m19) else \"\"\n",
    "            # Comparar (col13, col14) de Df_NS_VAL con (col18, col19) de df_maestro\n",
    "            if str13 == str18 and str14 == str19:\n",
    "                return \"municipio_igual_adres\"\n",
    "            else:\n",
    "                return \"mantener\"\n",
    "        else:\n",
    "            return \"mantener\"\n",
    "\n",
    "# Aplicar la función de procesamiento a cada fila de df_N04\n",
    "df_N04[\"decision\"] = df_N04.apply(process_N04, axis=1)\n",
    "\n",
    "# Separar registros que se moverán:\n",
    "df_N04_move = df_N04[df_N04[\"decision\"].isin([\"mover_N04_otro_departamento\", \"municipio_igual_adres\"])].copy()\n",
    "df_N04_move.loc[df_N04_move[\"decision\"] == \"mover_N04_otro_departamento\", \"origen\"] = \"N04 a otro departamento\"\n",
    "df_N04_move.loc[df_N04_move[\"decision\"] == \"municipio_igual_adres\", \"origen\"] = \"municipio igual a ADRES\"\n",
    "\n",
    "# Mover registros a Df_NS_NEG y eliminarlos de Df_NS_VAL\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_N04_move], ignore_index=True)\n",
    "Df_NS_VAL = Df_NS_VAL.drop(df_N04_move.index)\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESAMIENTO PARA REGISTROS \"N02\"\n",
    "# -----------------------------\n",
    "\n",
    "# Filtrar registros N02 en Df_NS_VAL (columna 11 == \"N02\")\n",
    "mask_N02 = (Df_NS_VAL.iloc[:, 11] == \"N02\")\n",
    "df_N02 = Df_NS_VAL.loc[mask_N02].copy()\n",
    "\n",
    "def process_N02(row):\n",
    "    # Convertir y formatear columnas 9 y 10 (a 2 y 3 dígitos respectivamente)\n",
    "    try:\n",
    "        num9 = pd.to_numeric(row.iloc[9], errors='coerce')\n",
    "        num10 = pd.to_numeric(row.iloc[10], errors='coerce')\n",
    "    except Exception:\n",
    "        num9, num10 = None, None\n",
    "    str9 = str(int(num9)).zfill(2) if pd.notna(num9) else \"\"\n",
    "    str10 = str(int(num10)).zfill(3) if pd.notna(num10) else \"\"\n",
    "    \n",
    "    # Convertir y formatear columnas 13 y 14 de la misma forma\n",
    "    try:\n",
    "        num13 = pd.to_numeric(row.iloc[13], errors='coerce')\n",
    "        num14 = pd.to_numeric(row.iloc[14], errors='coerce')\n",
    "    except Exception:\n",
    "        num13, num14 = None, None\n",
    "    str13 = str(int(num13)).zfill(2) if pd.notna(num13) else \"\"\n",
    "    str14 = str(int(num14)).zfill(3) if pd.notna(num14) else \"\"\n",
    "    \n",
    "    # Actualizar la fila\n",
    "    row.iloc[9] = str9\n",
    "    row.iloc[10] = str10\n",
    "    row.iloc[13] = str13\n",
    "    row.iloc[14] = str14\n",
    "    \n",
    "    # Validar: si (col9, col10) son iguales a (col13, col14), se mueve a Df_NS_NEG\n",
    "    if str9 == str13 and str10 == str14:\n",
    "        return \"sin_cambios\"\n",
    "    else:\n",
    "        return \"mantener\"\n",
    "\n",
    "df_N02[\"decision\"] = df_N02.apply(process_N02, axis=1)\n",
    "\n",
    "# Separar registros de N02 que se moverán:\n",
    "df_N02_move = df_N02[df_N02[\"decision\"] == \"sin_cambios\"].copy()\n",
    "df_N02_move[\"origen\"] = \"sin cambios\"\n",
    "\n",
    "# Mover estos registros a Df_NS_NEG y eliminarlos de Df_NS_VAL\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_N02_move], ignore_index=True)\n",
    "Df_NS_VAL = Df_NS_VAL.drop(df_N02_move.index)\n",
    "\n",
    "print(\"Proceso para N04 y N02 completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 N09 Retiro por muerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso para N09 completado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Filtrar registros \"N09\" en Df_NS_VAL (columna 11 == \"N09\")\n",
    "mask_N09 = (Df_NS_VAL.iloc[:, 11] == \"N09\")\n",
    "df_N09 = Df_NS_VAL.loc[mask_N09].copy()\n",
    "\n",
    "# 2. Crear un diccionario a partir de df_maestro para mapear el ID (de columnas [4,5]) al valor de la columna [28]\n",
    "# Convertimos el ID a una tupla, y asumimos que los valores en columna 28 se pueden comparar como cadena.\n",
    "maestro_ids = df_maestro.iloc[:, [4, 5]].apply(tuple, axis=1)\n",
    "maestro_col28 = df_maestro.iloc[:, 28].astype(str).str.strip()  # Limpiamos espacios\n",
    "maestro_dict_col28 = dict(zip(maestro_ids, maestro_col28))\n",
    "\n",
    "# 3. Función para procesar cada fila \"N09\"\n",
    "def process_N09(row):\n",
    "    # Formar el ID a partir de las columnas 2 y 3 de Df_NS_VAL\n",
    "    id_val = (row.iloc[2], row.iloc[3])\n",
    "    # Si el ID existe en el diccionario de df_maestro\n",
    "    if id_val in maestro_dict_col28:\n",
    "        # Si el valor en la columna 28 de df_maestro es \"AF\", marcar el registro\n",
    "        if maestro_dict_col28[id_val] == \"AF\":\n",
    "            return \"mismo_estado_adres\"\n",
    "    return \"mantener\"\n",
    "\n",
    "# Aplicar la función a los registros N09\n",
    "df_N09[\"decision\"] = df_N09.apply(process_N09, axis=1)\n",
    "\n",
    "# 4. Separar los registros a mover\n",
    "df_N09_move = df_N09[df_N09[\"decision\"] == \"mismo_estado_adres\"].copy()\n",
    "df_N09_move[\"origen\"] = \"Mismo estado ADRES\"\n",
    "\n",
    "# 5. Mover estos registros a Df_NS_NEG y eliminarlos de Df_NS_VAL\n",
    "Df_NS_NEG = pd.concat([Df_NS_NEG, df_N09_move], ignore_index=True)\n",
    "Df_NS_VAL = Df_NS_VAL.drop(df_N09_move.index)\n",
    "\n",
    "print(\"Proceso para N09 completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46 N46 Reporte de datos de contacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176000\n",
      "Index(['tipo_documento', 'numero_identificacion', 'primer_apellido',\n",
      "       'segundo_apellido', 'primer_nombre', 'segundo_nombre',\n",
      "       'fecha_nacimiento', 'genero', 'municipio', 'direccion', 'celular',\n",
      "       'telefono_1', 'telefono_2', 'correo_electronico'],\n",
      "      dtype='object')\n",
      "223834\n",
      "223834\n"
     ]
    }
   ],
   "source": [
    "# Copia sólo de ciertas columnas\n",
    "N46 = df_ms_SIE[[\n",
    " 'tipo_documento', 'numero_identificacion', 'primer_apellido',\n",
    " 'segundo_apellido', 'primer_nombre', 'segundo_nombre',\n",
    " 'fecha_nacimiento', 'genero', 'municipio',\n",
    " 'direccion', 'celular', 'telefono_1', 'telefono_2',\n",
    " 'correo_electronico'\n",
    "]].copy()\n",
    "print(df_ms_EPS025.shape[0])\n",
    "print(N46.columns)\n",
    "print(df_ms_SIE.shape[0])\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tipo_documento', 'numero_identificacion', 'primer_apellido',\n",
      "       'segundo_apellido', 'primer_nombre', 'segundo_nombre',\n",
      "       'fecha_nacimiento', 'genero', 'municipio', 'direccion', 'celular',\n",
      "       'telefono_1', 'telefono_2', 'correo_electronico', 'Muni_Naci', 'Dep',\n",
      "       'Mun', 'Estado_ADRES'],\n",
      "      dtype='object')\n",
      "['AC' nan 'AF' 'RE' 'SM' 'SD']\n"
     ]
    }
   ],
   "source": [
    "# 1. Extrae los nombres de tus columnas clave en N46\n",
    "key0, key1 = N46.columns[0], N46.columns[1]\n",
    "\n",
    "# 2. Construye un df temporal con sólo las 4 columnas que necesitas y renómbralas\n",
    "df_tmp = df_ms_EPS025.iloc[:, [4, 5, 13, 23, 24, 33]].copy()\n",
    "df_tmp.columns = [key0, key1, \"Muni_Naci\", \"Dep\", \"Mun\", \"Estado_ADRES\"]\n",
    "\n",
    "# 3. Mézclalo con N46 sobre esas claves, añadiendo Muni_Naci y Estado_ADRES\n",
    "N46 = N46.merge(df_tmp, on=[key0, key1], how=\"left\")\n",
    "print(N46.columns)\n",
    "print(N46[\"Estado_ADRES\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223834\n",
      "149498\n",
      "95505\n"
     ]
    }
   ],
   "source": [
    "print(N46.shape[0])\n",
    "# Define los valores a eliminar\n",
    "vals = ['AF', 'RE', 'SM', 'SD']\n",
    "\n",
    "# Crea una máscara: True para las filas que *sí* queremos quitar\n",
    "mask = N46['Estado_ADRES'].isna() | N46['Estado_ADRES'].isin(vals)\n",
    "\n",
    "# Queda solo lo que NO cumple esa condición\n",
    "N46 = N46[~mask].reset_index(drop=True)\n",
    "print(N46.shape[0])\n",
    "\n",
    "# Primero, elimina NaN\n",
    "N46 = N46[N46['correo_electronico'].notna()]\n",
    "# Luego, elimina cadenas vacías\n",
    "N46 = N46[N46['correo_electronico'] != \"\"].reset_index(drop=True)\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95505\n",
      "30761\n"
     ]
    }
   ],
   "source": [
    "import re  # Para validación de correos electrónicos\n",
    "N46['correo_electronico'] = N46['correo_electronico'].apply(\n",
    "    lambda email: \"\" if (\n",
    "        re.search(r'sat', email, re.IGNORECASE) or \n",
    "        re.search(r'SAT', email, re.IGNORECASE) or \n",
    "        re.search(r'TRASLADO', email, re.IGNORECASE) or \n",
    "        re.search(r'ACTUALIZAR', email, re.IGNORECASE) or \n",
    "        re.search(r'actualiza', email, re.IGNORECASE) or \n",
    "        re.search(r'actualizar', email, re.IGNORECASE) or \n",
    "        re.search(r'tiene', email, re.IGNORECASE) or \n",
    "        re.search(r'TIENE', email, re.IGNORECASE) or \n",
    "        re.search(r'afilia', email, re.IGNORECASE) or \n",
    "        re.search(r'traslado', email, re.IGNORECASE) or \n",
    "        email.split('@')[0].isdigit()\n",
    "    ) else email\n",
    ")\n",
    "print(N46.shape[0])\n",
    "\n",
    "# Primero, elimina NaN\n",
    "N46 = N46[N46['correo_electronico'].notna()]\n",
    "# Luego, elimina cadenas vacías\n",
    "N46 = N46[N46['correo_electronico'] != \"\"].reset_index(drop=True)\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30761\n",
      "30566\n"
     ]
    }
   ],
   "source": [
    "for col in ['celular', 'telefono_1', 'telefono_2']:\n",
    "    N46[col] = (\n",
    "        N46[col]\n",
    "          .astype(str)\n",
    "          .str.replace(r'[-\\s]', '', regex=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def is_valid_email(email):\n",
    "    if pd.isnull(email) or email.strip() == \"\":\n",
    "        return False\n",
    "    # Patrón básico de email\n",
    "    regex = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
    "    if not re.match(regex, email):\n",
    "        return False\n",
    "    # Validar casos específicos de correos no deseados\n",
    "    email_lower = email.lower()\n",
    "    if email_lower in [\"actualizar@actualizar.com\", \"notiene@gamil.com\", \"00@hotmail.com\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Aplicar la validación en la columna 'correo_electronico' y reemplazar los no válidos por cadena vacía\n",
    "N46['correo_electronico'] = N46['correo_electronico'].apply(lambda x: x if is_valid_email(x) else '')\n",
    "\n",
    "print(N46.shape[0])\n",
    "\n",
    "# Primero, elimina NaN\n",
    "N46 = N46[N46['correo_electronico'].notna()]\n",
    "# Luego, elimina cadenas vacías\n",
    "N46 = N46[N46['correo_electronico'] != \"\"].reset_index(drop=True)\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30566\n",
      "Filas sin ningún valor numérico en las 3 columnas:\n",
      "         celular telefono_1 telefono_2\n",
      "161          nan        nan        nan\n",
      "183          nan        nan        nan\n",
      "185          nan        nan        nan\n",
      "258          nan        nan        nan\n",
      "648          nan        nan        nan\n",
      "...          ...        ...        ...\n",
      "25599        nan        nan        nan\n",
      "26179        nan        nan        nan\n",
      "26220        nan        nan        nan\n",
      "26492  NOREFIERE     NOREFI        nan\n",
      "27009        nan        nan        nan\n",
      "\n",
      "[215 rows x 3 columns]\n",
      "30351\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(N46.shape[0])\n",
    "# columnas a validar\n",
    "phones = ['celular', 'telefono_1', 'telefono_2']\n",
    "\n",
    "# 1) Genero un DataFrame booleano donde True = convertible a número\n",
    "is_numeric = N46[phones].apply(lambda col: pd.to_numeric(col, errors='coerce').notna())\n",
    "\n",
    "# 2) Identifico las filas donde **ninguna** de las 3 columnas es numérica\n",
    "mask_no_number = ~is_numeric.any(axis=1)\n",
    "\n",
    "# 3) (Opcional) Imprimo esas filas para revisarlas\n",
    "print(\"Filas sin ningún valor numérico en las 3 columnas:\")\n",
    "print(N46.loc[mask_no_number, phones])\n",
    "\n",
    "# 4) Elimino esas filas del DataFrame\n",
    "N46 = N46.loc[~mask_no_number].reset_index(drop=True)\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30351\n",
      "30313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_5632\\2976976887.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  N46[phones] = N46[phones].applymap(clean_phone)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "phones = ['celular', 'telefono_1', 'telefono_2']\n",
    "print(N46.shape[0])\n",
    "def clean_phone(x):\n",
    "    s = str(x)\n",
    "    return s if re.match(r'^3\\d{9}$', s) else ''\n",
    "\n",
    "N46[phones] = N46[phones].applymap(clean_phone)\n",
    "\n",
    "# luego el mismo filtrado\n",
    "mask = N46[phones].ne('').any(axis=1)\n",
    "N46 = N46[mask].reset_index(drop=True)\n",
    "print(N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celular         108\n",
      "telefono_1     4539\n",
      "telefono_2    19820\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "phones = ['celular','telefono_1','telefono_2']\n",
    "\n",
    "# Esto construye una Serie con el conteo de vacíos ('' o NaN) por columna\n",
    "empty_counts = (N46[phones].isna() | N46[phones].eq('')).sum()\n",
    "\n",
    "print(empty_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teléfonos vacíos: 0  /  30313\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Asegúrate de que las tres columnas ya estén limpias ('3xxxxxxxxx' o '')\n",
    "#    y sustituyes '' → NaN para que bfill funcione correctamente:\n",
    "N46[['celular','telefono_1','telefono_2']] = (\n",
    "    N46[['celular','telefono_1','telefono_2']]\n",
    "      .replace('', np.nan)\n",
    ")\n",
    "\n",
    "# 2) Construye la columna única, haciendo back-fill por fila y tomando la primera\n",
    "N46['telefono'] = (\n",
    "    N46[['celular','telefono_1','telefono_2']]\n",
    "      .bfill(axis=1)    # rellena NaN con el siguiente valor no-NaN de derecha a izquierda\n",
    "      .iloc[:, 0]       # toma la primera columna resultante (primer valor válido)\n",
    ")\n",
    "\n",
    "# 3) Ahora puedes eliminar las tres originales\n",
    "N46 = N46.drop(columns=['celular','telefono_1','telefono_2'])\n",
    "\n",
    "# Comprueba que no hay vacíos en tu nueva columna:\n",
    "print(\"Teléfonos vacíos:\", N46['telefono'].isna().sum(), \" / \", N46.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  direccion_limpia\n",
      "0       CL ESTE 20\n",
      "1         CR 59 42\n",
      "2      CR 19 10 02\n",
      "3          CL 9 47\n",
      "4        CR 6 8 56\n",
      "5         13 28 29\n",
      "6      CL 36 13 15\n",
      "7         36 6 101\n",
      "8         TV 26 22\n",
      "9         TV 26 22\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "import pandas as pd\n",
    "\n",
    "# Supongamos que N46 ya existe en nuestro entorno\n",
    "# Aquí definimos las listas de nombres completos y sus abreviaturas\n",
    "full_names = [\n",
    "    \"Avenida calle\",\"Administración\",\"Adelante\",\"Aeropuerto\",\"Agencia\",\"Agrupación\",\n",
    "    \"Avenida carrera\",\"Altillo\",\"Al lado\",\"Almacén\",\"Apartamento\",\"Apartado\",\"Atrás\",\n",
    "    \"Autopista\",\"Avenida\",\"Anillo vial\",\"Bodega\",\"Bloque\",\"Boulevard\",\"Barrio\",\n",
    "    \"Corregimiento\",\"Casa\",\"Caserío\",\"Centro comercial\",\"Ciudadela\",\"Célula\",\"Centro\",\n",
    "    \"Circular\",\"Calle\",\"Callejón\",\"Camino\",\"Comunidad\",\"Conjunto residencial\",\"Conjunto\",\n",
    "    \"Carrera\",\"Carretera\",\"Circunvalar\",\"Consultorio\",\"Diagonal\",\"Depósito\",\"Departamento\",\n",
    "    \"Depósito sótano\",\"Edificio\",\"Entrada\",\"Escalera\",\"Esquina\",\"Este\",\"Etapa\",\"Exterior\",\n",
    "    \"Finca\",\"Garaje\",\"Garaje sótano\",\"Glorieta\",\"Hacienda\",\"Hangar\",\"Interior\",\n",
    "    \"Inspección de Policía\",\"Inspección Departamental\",\"Inspección Municipal\",\"Kilómetro\",\n",
    "    \"Local\",\"Local mezzanine\",\"Lote\",\"Módulo\",\"Mojón\",\"Muelle\",\"Mezzanine\",\"Manzana\",\n",
    "    \"Vías de nombre común\",\"Norte\",\"Oriente\",\"Occidente\",\"Oeste\",\"Oficina\",\"Piso\",\"Parcela\",\n",
    "    \"Parque\",\"Predio\",\"Penthouse\",\"Pasaje\",\"Planta\",\"Puente\",\"Portería\",\"Poste\",\"Parqueadero\",\n",
    "    \"Paraje\",\"Paseo\",\"Puesto\",\"Park Way\",\"Rancheria\",\"Round Point\",\"Salón\",\"Salón comunal\",\n",
    "    \"Salida\",\"Sector\",\"Solar\",\"Súper manzana\",\"Semisótano\",\"Sótano\",\"Suite\",\"Sur\",\"Terminal\",\n",
    "    \"Terraplén\",\"Torre\",\"Transversal\",\"Terraza\",\"Unidad\",\"Unidad residencial\",\"Urbanización\",\n",
    "    \"Vereda\",\"Variante\",\"Zona franca\",\"Zona\"\n",
    "]\n",
    "abbrs = [\n",
    "    \"AC\",\"AD\",\"ADL\",\"AER\",\"AG\",\"AGP\",\"AK\",\"AL\",\"ALD\",\"ALM\",\"AP\",\"APTDO\",\"ATR\",\"AUT\",\"AV\",\n",
    "    \"AVIAL\",\"BG\",\"BL\",\"BLV\",\"BRR\",\"C\",\"CA\",\"CAS\",\"CC\",\"CD\",\"CEL\",\"CEN\",\"CIR\",\"CL\",\"CLJ\",\n",
    "    \"CN\",\"CO\",\"CON\",\"CONJ\",\"CR\",\"CRT\",\"CRV\",\"CS\",\"DG\",\"DP\",\"DPTO\",\"DS\",\"ED\",\"EN\",\"ES\",\n",
    "    \"ESQ\",\"ESTE\",\"ET\",\"EX\",\"FCA\",\"GJ\",\"GS\",\"GT\",\"HC\",\"HG\",\"IN\",\"IP\",\"IPD\",\"IPM\",\"KM\",\"LC\",\n",
    "    \"LM\",\"LT\",\"MD\",\"MJ\",\"MLL\",\"MN\",\"MZ\",\"NOMBRE VIA\",\"NORTE\",\"O\",\"OCC\",\"OESTE\",\"OF\",\"P\",\n",
    "    \"PA\",\"PAR\",\"PD\",\"PH\",\"PJ\",\"PL\",\"PN\",\"POR\",\"POS\",\"PQ\",\"PRJ\",\"PS\",\"PT\",\"PW\",\"RA\",\"RP\",\n",
    "    \"SA\",\"SC\",\"SD\",\"SEC\",\"SL\",\"SM\",\"SS\",\"ST\",\"SUITE\",\"SUR\",\"TER\",\"TERPLN\",\"TO\",\"TV\",\"TZ\",\n",
    "    \"UN\",\"UR\",\"URB\",\"VRD\",\"VTE\",\"ZF\",\"ZN\"\n",
    "]\n",
    "\n",
    "# Creamos un diccionario de mapeo\n",
    "mapping = dict(zip([n.upper() for n in full_names], abbrs))\n",
    "\n",
    "def abbreviate_address(addr):\n",
    "    if not isinstance(addr, str):\n",
    "        return \"\"\n",
    "    # Limpieza de símbolos\n",
    "    s = re.sub(r'[#\\-]', ' ', addr).upper()\n",
    "    tokens = s.split()\n",
    "    result = []\n",
    "    for tok in tokens:\n",
    "        # Intentamos hallar similitud de al menos 80%\n",
    "        match = get_close_matches(tok, mapping.keys(), n=1, cutoff=0.8)\n",
    "        if match:\n",
    "            result.append(mapping[match[0]])\n",
    "        elif tok.isdigit():\n",
    "            result.append(tok)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Aplicar la función a la columna 'direccion'\n",
    "N46['direccion_limpia'] = N46['direccion'].apply(abbreviate_address)\n",
    "\n",
    "# Eliminar la columna original si ya no se necesita\n",
    "N46.drop(columns=['direccion'], inplace=True)\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "print(N46[['direccion_limpia']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# guardar Datafarmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo libro de trabajo\n",
    "wb = Workbook()\n",
    "\n",
    "# Guardar Df_NS_NEG en una hoja llamada \"NS_NEG\"\n",
    "ws_neg = wb.active\n",
    "ws_neg.title = \"NS_NEG\"\n",
    "for row in dataframe_to_rows(Df_NS_NEG, index=False, header=True):\n",
    "    ws_neg.append([f\"'{cell}\" for cell in row])\n",
    "\n",
    "# Guardar Df_NS_VAL en una hoja llamada \"NS_VAL\"\n",
    "ws_val = wb.create_sheet(title=\"NS_VAL\")\n",
    "for row in dataframe_to_rows(Df_NS_VAL, index=False, header=True):\n",
    "    ws_val.append([f\"'{cell}\" for cell in row])\n",
    "\n",
    "# Guardar el archivo Excel\n",
    "wb.save(R_Salida_Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "N46.to_csv(R_Salida, sep=',', index=False, encoding='ansi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
