{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04d64e3",
   "metadata": {},
   "source": [
    "# 1. MODULO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa el módulo 'os' para interactuar con el sistema de archivos y rutas del sistema operativo\n",
    "import os\n",
    "\n",
    "# Importa 'pandas' como 'pd', una biblioteca potente para manipulación y análisis de datos mediante DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Importa el módulo 'datetime' para trabajar con fechas y horas de manera eficiente\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdabc2c",
   "metadata": {},
   "source": [
    "# 2. Rutas Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1507be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_S4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S4\\S4_consolidado_total.txt\"\n",
    "R_R4 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\R4\\R4_consolidado_total.txt\"\n",
    "R_S5 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\S5_consolidado.txt\"\n",
    "R_S1_AUTO = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "R_MS = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Validados\\All_MS_VAL.TXT\"\n",
    "R_NS = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS validado\\all-NS-VAL.txt\"\n",
    "R_NS_Municipios = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Novedades municipios\\all-NS-VAL.txt\"\n",
    "R_Maestro_ADRES = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\MS_Unif_Maximo.txt\"\n",
    "\n",
    "ano = 2025\n",
    "Regimen = \"EPS025\"\n",
    "#Regimen = \"EPSC25\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec82bc",
   "metadata": {},
   "source": [
    "# 3. Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad250f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo .TXT con pandas\n",
    "df_S4 = pd.read_csv(R_S4, sep=',', encoding='utf-8',dtype=str)\n",
    "df_R4 = pd.read_csv(R_R4, sep=',', encoding='utf-8',dtype=str)\n",
    "df_S5 = pd.read_csv(R_S5, sep=',', encoding='utf-8',dtype=str)\n",
    "df_S1_AUTO = pd.read_csv(R_S1_AUTO, sep=',', encoding='ansi',dtype=str)\n",
    "df_MS = pd.read_csv(R_MS, sep=',', encoding='ansi',dtype=str)\n",
    "df_NS = pd.read_csv(R_NS, sep=',', encoding='ansi',dtype=str)\n",
    "df_NS_Municipios = pd.read_csv(R_NS_Municipios, sep=',', encoding='ansi',dtype=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844012f",
   "metadata": {},
   "source": [
    "## 3.1. Unificación de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b43390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estado inicial\n",
    "print(\"=== ESTADO INICIAL ===\")\n",
    "print(f\"df_NS: {df_NS.shape[0]} registros\")\n",
    "print(f\"df_NS_Municipios: {df_NS_Municipios.shape[0]} registros\")\n",
    "print(f\"Total esperado: {df_NS.shape[0] + df_NS_Municipios.shape[0]} registros\")\n",
    "\n",
    "# Verificar que tengan las mismas columnas\n",
    "print(f\"\\ndf_NS tiene {df_NS.shape[1]} columnas\")\n",
    "print(f\"df_NS_Municipios tiene {df_NS_Municipios.shape[1]} columnas\")\n",
    "\n",
    "# Verificar si las columnas son exactamente las mismas\n",
    "columnas_iguales = list(df_NS.columns) == list(df_NS_Municipios.columns)\n",
    "print(f\"¿Columnas idénticas?: {columnas_iguales}\")\n",
    "\n",
    "if not columnas_iguales:\n",
    "    print(\"⚠️ ADVERTENCIA: Las columnas no son idénticas\")\n",
    "    print(\"Columnas en df_NS:\", list(df_NS.columns))\n",
    "    print(\"Columnas en df_NS_Municipios:\", list(df_NS_Municipios.columns))\n",
    "    \n",
    "    # Mostrar diferencias\n",
    "    cols_df_NS = set(df_NS.columns)\n",
    "    cols_df_NS_Municipios = set(df_NS_Municipios.columns)\n",
    "    \n",
    "    if cols_df_NS != cols_df_NS_Municipios:\n",
    "        print(\"Columnas solo en df_NS:\", cols_df_NS - cols_df_NS_Municipios)\n",
    "        print(\"Columnas solo en df_NS_Municipios:\", cols_df_NS_Municipios - cols_df_NS)\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "print(\"\\n=== CONCATENANDO DATAFRAMES ===\")\n",
    "df_NS_unificado = pd.concat([df_NS, df_NS_Municipios], ignore_index=True)\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"=== RESULTADO ===\")\n",
    "print(f\"DataFrame unificado: {df_NS_unificado.shape[0]} registros\")\n",
    "print(f\"Columnas en resultado: {df_NS_unificado.shape[1]}\")\n",
    "\n",
    "# Validación de integridad\n",
    "registros_originales = df_NS.shape[0] + df_NS_Municipios.shape[0]\n",
    "registros_resultado = df_NS_unificado.shape[0]\n",
    "\n",
    "print(f\"\\n=== VALIDACIÓN ===\")\n",
    "print(f\"Registros originales (suma): {registros_originales}\")\n",
    "print(f\"Registros en resultado: {registros_resultado}\")\n",
    "print(f\"Diferencia: {registros_resultado - registros_originales}\")\n",
    "\n",
    "if registros_originales == registros_resultado:\n",
    "    print(\"✅ VALIDACIÓN EXITOSA: La suma de registros coincide\")\n",
    "else:\n",
    "    print(\"❌ ERROR: La suma de registros NO coincide\")\n",
    "    print(\"Revisar si hay algún problema en la concatenación\")\n",
    "\n",
    "# Información adicional del DataFrame unificado\n",
    "print(f\"\\n=== INFORMACIÓN ADICIONAL ===\")\n",
    "print(f\"Índices van de 0 a {df_NS_unificado.index.max()}\")\n",
    "print(f\"¿Hay valores nulos?: {df_NS_unificado.isnull().any().any()}\")\n",
    "\n",
    "# Opcional: Mostrar primeras y últimas filas para verificar\n",
    "print(\"\\nPrimeras 3 filas del resultado:\")\n",
    "print(df_NS_unificado.head(3))\n",
    "print(\"\\nÚltimas 3 filas del resultado:\")\n",
    "print(df_NS_unificado.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcac50",
   "metadata": {},
   "source": [
    "# 4. Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26d462",
   "metadata": {},
   "source": [
    "## 4.1. df_S4\n",
    "### 4.1.1. Año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5973d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna FECHA_PROCESO a datetime y filtrar por año 2025\n",
    "df_S4['FECHA_PROCESO'] = pd.to_datetime(df_S4['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "df_S4 = df_S4[df_S4['FECHA_PROCESO'].dt.year == ano]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eedaf89",
   "metadata": {},
   "source": [
    "### 4.1.2. Unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estado inicial\n",
    "print(\"=== ESTADO INICIAL ===\")\n",
    "print(f\"Total de registros: {len(df_S4)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_S4['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros duplicados: {len(df_S4) - df_S4['AFL_ID'].nunique()}\")\n",
    "\n",
    "# Verificar duplicados por AFL_ID\n",
    "duplicados_inicial = df_S4[df_S4.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Registros en grupos duplicados: {len(duplicados_inicial)}\")\n",
    "\n",
    "# Convertir FECHA_PROCESO a datetime si no está ya convertida\n",
    "if df_S4['FECHA_PROCESO'].dtype == 'object':\n",
    "    df_S4['FECHA_PROCESO'] = pd.to_datetime(df_S4['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "\n",
    "# Convertir RESPUESTA a numérico para asegurar comparaciones correctas\n",
    "df_S4['RESPUESTA'] = pd.to_numeric(df_S4['RESPUESTA'])\n",
    "\n",
    "def limpiar_duplicados_s4(df):\n",
    "    \"\"\"\n",
    "    Limpia duplicados según los criterios especificados:\n",
    "    1. Si hay respuesta=1 y respuesta=0 para el mismo AFL_ID, mantener respuesta=1\n",
    "    2. Si todos son respuesta=0, mantener el más reciente (fecha más nueva)\n",
    "    3. Si todos son respuesta=1, mantener el más reciente (fecha más nueva)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar por AFL_ID, RESPUESTA (descendente), y FECHA_PROCESO (descendente)\n",
    "    df_sorted = df.sort_values(['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO'], \n",
    "                              ascending=[True, False, False])\n",
    "    \n",
    "    # Eliminar duplicados manteniendo el primer registro después del ordenamiento\n",
    "    # Esto garantiza que:\n",
    "    # - Si hay respuesta=1, se mantiene (por orden descendente de RESPUESTA)\n",
    "    # - Si hay múltiples con la misma respuesta, se mantiene el más reciente\n",
    "    df_clean = df_sorted.drop_duplicates(subset=['AFL_ID'], keep='first')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Aplicar la limpieza\n",
    "df_S4_clean = limpiar_duplicados_s4(df_S4)\n",
    "\n",
    "# Verificar estado final\n",
    "print(\"\\n=== ESTADO FINAL ===\")\n",
    "print(f\"Total de registros: {len(df_S4_clean)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_S4_clean['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros eliminados: {len(df_S4) - len(df_S4_clean)}\")\n",
    "\n",
    "# Verificar si quedan duplicados\n",
    "duplicados_final = df_S4_clean[df_S4_clean.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Duplicados restantes: {len(duplicados_final)}\")\n",
    "\n",
    "if len(duplicados_final) > 0:\n",
    "    print(\"⚠️  ADVERTENCIA: Aún existen duplicados!\")\n",
    "    print(duplicados_final[['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].head(10))\n",
    "else:\n",
    "    print(\"✅ No hay duplicados restantes\")\n",
    "\n",
    "# Análisis detallado de los casos procesados\n",
    "print(\"\\n=== ANÁLISIS DE CASOS ===\")\n",
    "\n",
    "# Contar casos por tipo de respuesta\n",
    "respuesta_counts = df_S4_clean['RESPUESTA'].value_counts().sort_index()\n",
    "print(\"Distribución de respuestas en resultado final:\")\n",
    "for respuesta, count in respuesta_counts.items():\n",
    "    print(f\"  RESPUESTA {respuesta}: {count} registros\")\n",
    "\n",
    "# Verificar algunos casos específicos para validación manual\n",
    "print(\"\\n=== VALIDACIÓN DE ALGUNOS CASOS ===\")\n",
    "# Tomar algunos AFL_ID que tenían duplicados y mostrar cómo se resolvieron\n",
    "if len(duplicados_inicial) > 0:\n",
    "    sample_ids = duplicados_inicial['AFL_ID'].unique()[:5]\n",
    "    \n",
    "    for afl_id in sample_ids:\n",
    "        print(f\"\\nAFL_ID: {afl_id}\")\n",
    "        casos_originales = df_S4[df_S4['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].sort_values('FECHA_PROCESO')\n",
    "        caso_final = df_S4_clean[df_S4_clean['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']]\n",
    "        \n",
    "        print(\"  Casos originales:\")\n",
    "        for _, row in casos_originales.iterrows():\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "        \n",
    "        print(\"  Caso mantenido:\")\n",
    "        if len(caso_final) > 0:\n",
    "            row = caso_final.iloc[0]\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# Actualizar el DataFrame original\n",
    "df_S4 = df_S4_clean.copy()\n",
    "\n",
    "print(f\"\\n✅ Proceso completado. DataFrame limpio asignado a df_S4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83366671",
   "metadata": {},
   "source": [
    "## 4.2 df_R4\n",
    "### 4.2.1 año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna FECHA_PROCESO a datetime y filtrar por año 2025\n",
    "df_R4['FECHA_PROCESO'] = pd.to_datetime(df_R4['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "df_R4 = df_R4[df_R4['FECHA_PROCESO'].dt.year == ano]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a7122",
   "metadata": {},
   "source": [
    "### 4.2.2. Unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estado inicial\n",
    "print(\"=== ESTADO INICIAL ===\")\n",
    "print(f\"Total de registros: {len(df_R4)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_R4['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros duplicados: {len(df_R4) - df_R4['AFL_ID'].nunique()}\")\n",
    "\n",
    "# Verificar duplicados por AFL_ID\n",
    "duplicados_inicial = df_R4[df_R4.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Registros en grupos duplicados: {len(duplicados_inicial)}\")\n",
    "\n",
    "# Convertir FECHA_PROCESO a datetime si no está ya convertida\n",
    "if df_R4['FECHA_PROCESO'].dtype == 'object':\n",
    "    df_R4['FECHA_PROCESO'] = pd.to_datetime(df_R4['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "\n",
    "# Convertir RESPUESTA a numérico para asegurar comparaciones correctas\n",
    "df_R4['RESPUESTA'] = pd.to_numeric(df_R4['RESPUESTA'])\n",
    "\n",
    "def limpiar_duplicados_s4(df):\n",
    "    \"\"\"\n",
    "    Limpia duplicados según los criterios especificados:\n",
    "    1. Si hay respuesta=1 y respuesta=0 para el mismo AFL_ID, mantener respuesta=1\n",
    "    2. Si todos son respuesta=0, mantener el más reciente (fecha más nueva)\n",
    "    3. Si todos son respuesta=1, mantener el más reciente (fecha más nueva)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar por AFL_ID, RESPUESTA (descendente), y FECHA_PROCESO (descendente)\n",
    "    df_sorted = df.sort_values(['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO'], \n",
    "                              ascending=[True, False, False])\n",
    "    \n",
    "    # Eliminar duplicados manteniendo el primer registro después del ordenamiento\n",
    "    # Esto garantiza que:\n",
    "    # - Si hay respuesta=1, se mantiene (por orden descendente de RESPUESTA)\n",
    "    # - Si hay múltiples con la misma respuesta, se mantiene el más reciente\n",
    "    df_clean = df_sorted.drop_duplicates(subset=['AFL_ID'], keep='first')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Aplicar la limpieza\n",
    "df_R4_clean = limpiar_duplicados_s4(df_R4)\n",
    "\n",
    "# Verificar estado final\n",
    "print(\"\\n=== ESTADO FINAL ===\")\n",
    "print(f\"Total de registros: {len(df_R4_clean)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_R4_clean['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros eliminados: {len(df_R4) - len(df_R4_clean)}\")\n",
    "\n",
    "# Verificar si quedan duplicados\n",
    "duplicados_final = df_R4_clean[df_R4_clean.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Duplicados restantes: {len(duplicados_final)}\")\n",
    "\n",
    "if len(duplicados_final) > 0:\n",
    "    print(\"⚠️  ADVERTENCIA: Aún existen duplicados!\")\n",
    "    print(duplicados_final[['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].head(10))\n",
    "else:\n",
    "    print(\"✅ No hay duplicados restantes\")\n",
    "\n",
    "# Análisis detallado de los casos procesados\n",
    "print(\"\\n=== ANÁLISIS DE CASOS ===\")\n",
    "\n",
    "# Contar casos por tipo de respuesta\n",
    "respuesta_counts = df_R4_clean['RESPUESTA'].value_counts().sort_index()\n",
    "print(\"Distribución de respuestas en resultado final:\")\n",
    "for respuesta, count in respuesta_counts.items():\n",
    "    print(f\"  RESPUESTA {respuesta}: {count} registros\")\n",
    "\n",
    "# Verificar algunos casos específicos para validación manual\n",
    "print(\"\\n=== VALIDACIÓN DE ALGUNOS CASOS ===\")\n",
    "# Tomar algunos AFL_ID que tenían duplicados y mostrar cómo se resolvieron\n",
    "if len(duplicados_inicial) > 0:\n",
    "    sample_ids = duplicados_inicial['AFL_ID'].unique()[:5]\n",
    "    \n",
    "    for afl_id in sample_ids:\n",
    "        print(f\"\\nAFL_ID: {afl_id}\")\n",
    "        casos_originales = df_R4[df_R4['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].sort_values('FECHA_PROCESO')\n",
    "        caso_final = df_R4_clean[df_R4_clean['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']]\n",
    "        \n",
    "        print(\"  Casos originales:\")\n",
    "        for _, row in casos_originales.iterrows():\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "        \n",
    "        print(\"  Caso mantenido:\")\n",
    "        if len(caso_final) > 0:\n",
    "            row = caso_final.iloc[0]\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# Actualizar el DataFrame original\n",
    "df_R4 = df_R4_clean.copy()\n",
    "\n",
    "print(f\"\\n✅ Proceso completado. DataFrame limpio asignado a df_R4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ce0f9",
   "metadata": {},
   "source": [
    "## 4.3. df_S5\n",
    "### 4.3.1 año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna FECHA_PROCESO a datetime y filtrar por año\n",
    "df_S5['FECHA_PROCESO'] = pd.to_datetime(df_S5['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "df_S5 = df_S5[df_S5['FECHA_PROCESO'].dt.year == ano]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3b4e6",
   "metadata": {},
   "source": [
    "### 4.3.2. Unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estado inicial\n",
    "print(\"=== ESTADO INICIAL ===\")\n",
    "print(f\"Total de registros: {len(df_S5)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_S5['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros duplicados: {len(df_S5) - df_S5['AFL_ID'].nunique()}\")\n",
    "\n",
    "# Verificar duplicados por AFL_ID\n",
    "duplicados_inicial = df_S5[df_S5.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Registros en grupos duplicados: {len(duplicados_inicial)}\")\n",
    "\n",
    "# Convertir FECHA_PROCESO a datetime si no está ya convertida\n",
    "if df_S5['FECHA_PROCESO'].dtype == 'object':\n",
    "    df_S5['FECHA_PROCESO'] = pd.to_datetime(df_S5['FECHA_PROCESO'], format='%d/%m/%Y')\n",
    "\n",
    "# Convertir RESPUESTA a numérico para asegurar comparaciones correctas\n",
    "df_S5['RESPUESTA'] = pd.to_numeric(df_S5['RESPUESTA'])\n",
    "\n",
    "def limpiar_duplicados_s4(df):\n",
    "    \"\"\"\n",
    "    Limpia duplicados según los criterios especificados:\n",
    "    1. Si hay respuesta=1 y respuesta=0 para el mismo AFL_ID, mantener respuesta=1\n",
    "    2. Si todos son respuesta=0, mantener el más reciente (fecha más nueva)\n",
    "    3. Si todos son respuesta=1, mantener el más reciente (fecha más nueva)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar por AFL_ID, RESPUESTA (descendente), y FECHA_PROCESO (descendente)\n",
    "    df_sorted = df.sort_values(['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO'], \n",
    "                              ascending=[True, False, False])\n",
    "    \n",
    "    # Eliminar duplicados manteniendo el primer registro después del ordenamiento\n",
    "    # Esto garantiza que:\n",
    "    # - Si hay respuesta=1, se mantiene (por orden descendente de RESPUESTA)\n",
    "    # - Si hay múltiples con la misma respuesta, se mantiene el más reciente\n",
    "    df_clean = df_sorted.drop_duplicates(subset=['AFL_ID'], keep='first')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Aplicar la limpieza\n",
    "df_S5_clean = limpiar_duplicados_s4(df_S5)\n",
    "\n",
    "# Verificar estado final\n",
    "print(\"\\n=== ESTADO FINAL ===\")\n",
    "print(f\"Total de registros: {len(df_S5_clean)}\")\n",
    "print(f\"Registros únicos por AFL_ID: {df_S5_clean['AFL_ID'].nunique()}\")\n",
    "print(f\"Registros eliminados: {len(df_S5) - len(df_S5_clean)}\")\n",
    "\n",
    "# Verificar si quedan duplicados\n",
    "duplicados_final = df_S5_clean[df_S5_clean.duplicated(subset=['AFL_ID'], keep=False)]\n",
    "print(f\"Duplicados restantes: {len(duplicados_final)}\")\n",
    "\n",
    "if len(duplicados_final) > 0:\n",
    "    print(\"⚠️  ADVERTENCIA: Aún existen duplicados!\")\n",
    "    print(duplicados_final[['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].head(10))\n",
    "else:\n",
    "    print(\"✅ No hay duplicados restantes\")\n",
    "\n",
    "# Análisis detallado de los casos procesados\n",
    "print(\"\\n=== ANÁLISIS DE CASOS ===\")\n",
    "\n",
    "# Contar casos por tipo de respuesta\n",
    "respuesta_counts = df_S5_clean['RESPUESTA'].value_counts().sort_index()\n",
    "print(\"Distribución de respuestas en resultado final:\")\n",
    "for respuesta, count in respuesta_counts.items():\n",
    "    print(f\"  RESPUESTA {respuesta}: {count} registros\")\n",
    "\n",
    "# Verificar algunos casos específicos para validación manual\n",
    "print(\"\\n=== VALIDACIÓN DE ALGUNOS CASOS ===\")\n",
    "# Tomar algunos AFL_ID que tenían duplicados y mostrar cómo se resolvieron\n",
    "if len(duplicados_inicial) > 0:\n",
    "    sample_ids = duplicados_inicial['AFL_ID'].unique()[:5]\n",
    "    \n",
    "    for afl_id in sample_ids:\n",
    "        print(f\"\\nAFL_ID: {afl_id}\")\n",
    "        casos_originales = df_S5[df_S5['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']].sort_values('FECHA_PROCESO')\n",
    "        caso_final = df_S5_clean[df_S5_clean['AFL_ID'] == afl_id][['AFL_ID', 'RESPUESTA', 'FECHA_PROCESO']]\n",
    "        \n",
    "        print(\"  Casos originales:\")\n",
    "        for _, row in casos_originales.iterrows():\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "        \n",
    "        print(\"  Caso mantenido:\")\n",
    "        if len(caso_final) > 0:\n",
    "            row = caso_final.iloc[0]\n",
    "            print(f\"    RESPUESTA: {row['RESPUESTA']}, FECHA: {row['FECHA_PROCESO'].strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# Actualizar el DataFrame original\n",
    "df_S5 = df_S5_clean.copy()\n",
    "\n",
    "print(f\"\\n✅ Proceso completado. DataFrame limpio asignado a df_S5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad034c8",
   "metadata": {},
   "source": [
    "## 4.4. df_S1_AUTO\n",
    "### 4.4.1. año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a515bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna Fecha_Efectiva a datetime y filtrar por año\n",
    "print(f\"Antes de filtrado:\\n {df_S1_AUTO.shape}\")\n",
    "df_S1_AUTO['Fecha_Efectiva'] = pd.to_datetime(df_S1_AUTO['Fecha_Efectiva'], format='%d/%m/%Y')\n",
    "df_S1_AUTO = df_S1_AUTO[df_S1_AUTO['Fecha_Efectiva'].dt.year == ano]\n",
    "print(f\"Despues de filtrado:\\n {df_S1_AUTO.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4c681",
   "metadata": {},
   "source": [
    "### 4.4.2. Unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Antes de filtrado:\\n {df_S1_AUTO.shape}\")\n",
    "df_S1_AUTO = df_S1_AUTO[df_S1_AUTO['ENT_ID_ORIGEN'] != 'EPSC25']\n",
    "print(f\"Despues de filtrado:\\n {df_S1_AUTO.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917135d7",
   "metadata": {},
   "source": [
    "## 4.5. df_MS\n",
    "### 4.5.1. año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna CND_AFL_FECHA_INICIO a datetime y filtrar por año\n",
    "print(f\"Antes de filtrado:\\n {df_MS.shape}\")\n",
    "df_MS['CND_AFL_FECHA_INICIO'] = pd.to_datetime(df_MS['CND_AFL_FECHA_INICIO'], format='%d/%m/%Y')\n",
    "df_MS = df_MS[df_MS['CND_AFL_FECHA_INICIO'].dt.year == ano]\n",
    "print(f\"Despues de filtrado:\\n {df_MS.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bcabbd",
   "metadata": {},
   "source": [
    "## 4.6. df_NS_unificado\n",
    "### 4.6.1. año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153870bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna FECHA_NOVEDAD a datetime y filtrar por año\n",
    "print(f\"Antes de filtrado:\\n {df_NS_unificado.shape}\")\n",
    "df_NS_unificado['FECHA_NOVEDAD'] = pd.to_datetime(df_NS_unificado['FECHA_NOVEDAD'], format='%d/%m/%Y')\n",
    "df_NS_unificado = df_NS_unificado[df_NS_unificado['FECHA_NOVEDAD'].dt.year == ano]\n",
    "print(f\"Despues de filtrado:\\n {df_NS_unificado.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcace7d4",
   "metadata": {},
   "source": [
    "### 4.6.2. Unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd88a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Antes de filtrado:\\n {df_NS_unificado.shape}\")\n",
    "# Usando | (OR) para combinar condiciones\n",
    "df_NS_unificado = df_NS_unificado[\n",
    "    (df_NS_unificado['NOVEDAD'] == 'N09') | \n",
    "    (df_NS_unificado['NOVEDAD'] == 'N14') | \n",
    "    (df_NS_unificado['NOVEDAD'] == 'N13') | \n",
    "    (df_NS_unificado['NOVEDAD'] == 'N31')\n",
    "]\n",
    "print(f\"Despues de filtrado:\\n {df_NS_unificado.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237d467",
   "metadata": {},
   "source": [
    "# 5. Dataframes Estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b53410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame vacío con las columnas especificadas\n",
    "df_Estadisticas = pd.DataFrame(columns=[\n",
    "    'FECHA_PROCESO', \n",
    "    'NOMBRE_ARCHIVO', \n",
    "    'AFL_ID', \n",
    "    'TPS_IDN_ID', \n",
    "    'HST_IDN_NUMERO_IDENTIFICACION', \n",
    "    'RESPUESTA', \n",
    "    'PROCESO'\n",
    "])\n",
    "\n",
    "# Verificar la creación\n",
    "print(\"DataFrame creado:\")\n",
    "print(f\"Forma: {df_Estadisticas.shape}\")\n",
    "print(f\"Columnas: {list(df_Estadisticas.columns)}\")\n",
    "print(f\"¿Está vacío?: {df_Estadisticas.empty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa664cb",
   "metadata": {},
   "source": [
    "## 5.1. df_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6630f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que las columnas existen en df_S4\n",
    "columnas_requeridas = [\n",
    "    'FECHA_PROCESO', \n",
    "    'NOMBRE_ARCHIVO', \n",
    "    'AFL_ID', \n",
    "    'TPS_IDN_ID', \n",
    "    'HST_IDN_NUMERO_IDENTIFICACION', \n",
    "    'RESPUESTA'\n",
    "]\n",
    "\n",
    "print(\"=== VERIFICACIÓN DE COLUMNAS ===\")\n",
    "columnas_faltantes = [col for col in columnas_requeridas if col not in df_S4.columns]\n",
    "\n",
    "if columnas_faltantes:\n",
    "    print(f\"❌ Columnas faltantes en df_S4: {columnas_faltantes}\")\n",
    "    print(f\"Columnas disponibles en df_S4: {list(df_S4.columns)}\")\n",
    "else:\n",
    "    print(\"✅ Todas las columnas requeridas están disponibles\")\n",
    "    \n",
    "    # Verificar el tipo de datos de FECHA_PROCESO\n",
    "    print(f\"\\nTipo de datos de FECHA_PROCESO en df_S4: {df_S4['FECHA_PROCESO'].dtype}\")\n",
    "    \n",
    "    # Crear df_Estadisticas manteniendo los tipos de datos originales\n",
    "    df_Estadisticas = df_S4[columnas_requeridas].copy()\n",
    "    \n",
    "    # Asegurar que FECHA_PROCESO mantenga el formato datetime\n",
    "    if df_Estadisticas['FECHA_PROCESO'].dtype == 'object':\n",
    "        print(\"⚠️ Convirtiendo FECHA_PROCESO a datetime...\")\n",
    "        df_Estadisticas['FECHA_PROCESO'] = pd.to_datetime(df_Estadisticas['FECHA_PROCESO'])\n",
    "    \n",
    "    # Agregar columna PROCESO vacía\n",
    "    df_Estadisticas['PROCESO'] = ''\n",
    "    \n",
    "    print(f\"\\n=== RESULTADO ===\")\n",
    "    print(f\"df_Estadisticas creado con {df_Estadisticas.shape[0]} registros y {df_Estadisticas.shape[1]} columnas\")\n",
    "    print(f\"Columnas: {list(df_Estadisticas.columns)}\")\n",
    "    \n",
    "    # Verificar tipos de datos en el resultado\n",
    "    print(f\"\\n=== TIPOS DE DATOS ===\")\n",
    "    for col in df_Estadisticas.columns:\n",
    "        print(f\"{col}: {df_Estadisticas[col].dtype}\")\n",
    "    \n",
    "    # Verificar que PROCESO está vacía\n",
    "    valores_proceso = df_Estadisticas['PROCESO'].unique()\n",
    "    print(f\"\\nValores únicos en PROCESO: {valores_proceso}\")\n",
    "    \n",
    "    # Verificar formato de FECHA_PROCESO\n",
    "    print(f\"\\nEjemplos de FECHA_PROCESO:\")\n",
    "    print(df_Estadisticas['FECHA_PROCESO'].head(3).values)\n",
    "    \n",
    "    print(\"\\nPrimeras 3 filas:\")\n",
    "    print(df_Estadisticas.head(3))\n",
    "    \n",
    "    # Información adicional sobre fechas\n",
    "    if df_Estadisticas['FECHA_PROCESO'].dtype.name.startswith('datetime'):\n",
    "        print(f\"\\n=== INFORMACIÓN DE FECHAS ===\")\n",
    "        print(f\"Fecha mínima: {df_Estadisticas['FECHA_PROCESO'].min()}\")\n",
    "        print(f\"Fecha máxima: {df_Estadisticas['FECHA_PROCESO'].max()}\")\n",
    "        print(f\"Rango de fechas: {df_Estadisticas['FECHA_PROCESO'].nunique()} fechas únicas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbee44",
   "metadata": {},
   "source": [
    "## 5.2. df_R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que las columnas existen en df_R4\n",
    "columnas_requeridas = [\n",
    "    'FECHA_PROCESO', \n",
    "    'NOMBRE_ARCHIVO', \n",
    "    'AFL_ID', \n",
    "    'TPS_IDN_ID', \n",
    "    'HST_IDN_NUMERO_IDENTIFICACION', \n",
    "    'RESPUESTA'\n",
    "]\n",
    "\n",
    "print(\"=== VERIFICACIÓN DE COLUMNAS EN df_R4 ===\")\n",
    "columnas_faltantes = [col for col in columnas_requeridas if col not in df_R4.columns]\n",
    "\n",
    "if columnas_faltantes:\n",
    "    print(f\"❌ Columnas faltantes en df_R4: {columnas_faltantes}\")\n",
    "    print(f\"Columnas disponibles en df_R4: {list(df_R4.columns)}\")\n",
    "else:\n",
    "    print(\"✅ Todas las columnas requeridas están disponibles en df_R4\")\n",
    "    \n",
    "    # Guardar el estado actual de df_Estadisticas\n",
    "    registros_previos = df_Estadisticas.shape[0]\n",
    "    print(f\"\\nRegistros actuales en df_Estadisticas: {registros_previos}\")\n",
    "    \n",
    "    # Verificar el tipo de datos de FECHA_PROCESO en df_R4\n",
    "    print(f\"Tipo de datos de FECHA_PROCESO en df_R4: {df_R4['FECHA_PROCESO'].dtype}\")\n",
    "    \n",
    "    # Crear DataFrame temporal con datos de df_R4\n",
    "    df_R4_temp = df_R4[columnas_requeridas].copy()\n",
    "    \n",
    "    # Asegurar que FECHA_PROCESO mantenga el formato datetime en df_R4_temp\n",
    "    if df_R4_temp['FECHA_PROCESO'].dtype == 'object':\n",
    "        print(\"⚠️ Convirtiendo FECHA_PROCESO de df_R4 a datetime...\")\n",
    "        df_R4_temp['FECHA_PROCESO'] = pd.to_datetime(df_R4_temp['FECHA_PROCESO'])\n",
    "    \n",
    "    # Agregar columna PROCESO para df_R4 (puedes cambiar 'R4' por el valor que necesites)\n",
    "    df_R4_temp['PROCESO'] = 'R4'\n",
    "    \n",
    "    # Concatenar con df_Estadisticas existente\n",
    "    df_Estadisticas = pd.concat([df_Estadisticas, df_R4_temp], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n=== RESULTADO DESPUÉS DE AGREGAR df_R4 ===\")\n",
    "    print(f\"Registros de df_S4: {registros_previos}\")\n",
    "    print(f\"Registros de df_R4: {df_R4_temp.shape[0]}\")\n",
    "    print(f\"Total esperado: {registros_previos + df_R4_temp.shape[0]}\")\n",
    "    print(f\"Total actual en df_Estadisticas: {df_Estadisticas.shape[0]}\")\n",
    "    \n",
    "    # Validar la concatenación\n",
    "    if df_Estadisticas.shape[0] == registros_previos + df_R4_temp.shape[0]:\n",
    "        print(\"✅ Concatenación exitosa - Sin pérdida de registros\")\n",
    "    else:\n",
    "        print(\"❌ Problema en la concatenación - Revisar datos\")\n",
    "    \n",
    "    # Verificar distribución por PROCESO\n",
    "    print(f\"\\n=== DISTRIBUCIÓN POR PROCESO ===\")\n",
    "    distribucion_proceso = df_Estadisticas['PROCESO'].value_counts()\n",
    "    print(distribucion_proceso)\n",
    "    \n",
    "    # Verificar tipos de datos finales\n",
    "    print(f\"\\n=== TIPOS DE DATOS FINALES ===\")\n",
    "    for col in df_Estadisticas.columns:\n",
    "        print(f\"{col}: {df_Estadisticas[col].dtype}\")\n",
    "    \n",
    "    # Verificar formato de FECHA_PROCESO\n",
    "    print(f\"\\nEjemplos de FECHA_PROCESO después de concatenar:\")\n",
    "    print(df_Estadisticas['FECHA_PROCESO'].head(3).values)\n",
    "    print(df_Estadisticas['FECHA_PROCESO'].tail(3).values)\n",
    "    \n",
    "    print(\"\\nPrimeras 3 filas (df_S4):\")\n",
    "    print(df_Estadisticas.head(3))\n",
    "    print(\"\\nÚltimas 3 filas (df_R4):\")\n",
    "    print(df_Estadisticas.tail(3))\n",
    "    \n",
    "    # Información adicional sobre fechas\n",
    "    if df_Estadisticas['FECHA_PROCESO'].dtype.name.startswith('datetime'):\n",
    "        print(f\"\\n=== INFORMACIÓN DE FECHAS CONSOLIDADA ===\")\n",
    "        print(f\"Fecha mínima: {df_Estadisticas['FECHA_PROCESO'].min()}\")\n",
    "        print(f\"Fecha máxima: {df_Estadisticas['FECHA_PROCESO'].max()}\")\n",
    "        print(f\"Rango de fechas: {df_Estadisticas['FECHA_PROCESO'].nunique()} fechas únicas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
