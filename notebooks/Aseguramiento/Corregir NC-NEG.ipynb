{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52061c12",
   "metadata": {},
   "source": [
    "# 1. Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ece43",
   "metadata": {},
   "source": [
    "# 2. Rutas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta Entrada\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0019052025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0019052025.TXT\"\n",
    "R_NC_NEG = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\2025\\NSEPS02516052025.NEG\"\n",
    "R_NC_Anterior = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\16\\NSEPS02516052025.txt\"\n",
    "# Ruta Salida\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2401c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"22/05/2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d22915",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID_ADRES\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5851b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe NS anterior\n",
    "new_columns = [\"NUM_SOLICITUD_NOVEDAD\", \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\", \"MNC_ID\", \"NOVEDAD\", \"FECHA_NOVEDAD\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"COD_3_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\", \"COD_7_NOVEDAD\"]\n",
    "Df_NS_Anterior = pd.read_csv(R_NC_Anterior, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NS_Anterior.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_NS_Anterior.shape[0])\n",
    "\n",
    "# Dataframe NS Glosado\n",
    "new_columns.append(\"Glosa\")\n",
    "Df_NC_NEG = pd.read_csv(R_NC_NEG, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_NC_NEG.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_NC_NEG.shape[0])\n",
    "\n",
    "# Agregar columna \"Enviar\" con un valor inicial vacío\n",
    "Df_NC_NEG['Enviar'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1275f",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0167e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# 1) Tu merge original (trae ENT_ID_ADRES y TPS_EST_AFL_ID_from_adres de ADRES)\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer  = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "Df_NC_NEG = Df_NC_NEG.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"TPS_EST_AFL_ID\":\"TPS_EST_AFL_ID_from_adres\"})\n",
    "\n",
    "# 2) Preparamos un mini-dataframe sólo con las filas N01 que sí sí cruzaron con ADRES\n",
    "mapa_n01 = (\n",
    "    Df_NC_NEG.loc[Df_NC_NEG[\"NOVEDAD\"] == \"N01\", \n",
    "                  [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"ENT_ID_ADRES\", \"TPS_EST_AFL_ID_from_adres\"]]\n",
    "    .copy()  # Asegurarse de trabajar con una copia del DataFrame\n",
    "    .rename(columns={\n",
    "        \"COD_1_NOVEDAD\": \"TPS_IDN_ID\",\n",
    "        \"COD_2_NOVEDAD\": \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "        \"ENT_ID_ADRES\": \"ENT_ID_from_self\",\n",
    "        \"TPS_EST_AFL_ID_from_adres\": \"TPS_EST_AFL_ID_from_self\"\n",
    "    })\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# 3) Merge “secundario” contra esas N01 para rellenar vacíos\n",
    "Df_NC_NEG = Df_NC_NEG.merge(\n",
    "    mapa_n01,\n",
    "    on=[\"TPS_IDN_ID\",\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4) Para los que quedaron con ENT_ID_ADRES NaN, y son evoluciones (p.ej. N02, N03, …),\n",
    "#    rellenamos con el valor traído de la fila N01\n",
    "mask = Df_NC_NEG[\"ENT_ID_ADRES\"].isna() & Df_NC_NEG[\"NOVEDAD\"].str.startswith(\"N0\")\n",
    "Df_NC_NEG.loc[mask, \"ENT_ID_ADRES\"]                   = Df_NC_NEG.loc[mask, \"ENT_ID_from_self\"]\n",
    "Df_NC_NEG.loc[mask, \"TPS_EST_AFL_ID_from_adres\"] = Df_NC_NEG.loc[mask, \"TPS_EST_AFL_ID_from_self\"]\n",
    "\n",
    "# 5) Limpiamos las columnas auxiliares\n",
    "Df_NC_NEG.drop(columns=[\"ENT_ID_from_self\",\"TPS_EST_AFL_ID_from_self\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_NC_NEG:\", Df_NC_NEG.shape[0])\n",
    "\n",
    "# 1. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_NC_NEG[\"ENT_ID_ADRES\"] == \"EPSC25\")\n",
    ")\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_EPSC25 = Df_NC_NEG.loc[mask].copy()\n",
    "\n",
    "# 4. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    Df_NC_NEG[\"ENT_ID_ADRES\"].isna()                 # NaN\n",
    "    | (Df_NC_NEG[\"ENT_ID_ADRES\"].astype(str).str.strip() == \"\")  # \"\" o \"   \"\n",
    ")\n",
    "# 5. Extraemos los registros a enviar\n",
    "DF_No_Enviar = Df_NC_NEG.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_NC_NEG = Df_NC_NEG.loc[~mask].copy()\n",
    "\n",
    "print(\"Número de registros en DF_EPSC25:\", DF_EPSC25.shape[0])\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_NC_NEG:\", Df_NC_NEG.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb895175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir los valores únicos\n",
    "print(Df_NC_NEG['NOVEDAD'].unique())\n",
    "print(Df_NC_NEG['NOVEDAD'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_NC_NEG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
