{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librerias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Librerías para el manejo de DataFrames\n",
    "import os # Librerías para manejar rutas y archivos del sistema\n",
    "import datetime # Librería para manejar fechas y horas\n",
    "import openpyxl # Librería para leer y escribir archivos Excel\n",
    "import xlsxwriter # Librería para crear archivos Excel con tablas y gráficos\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAlida oficce\n",
    "FECHA_Salida = \"02-12-2025\"\n",
    "SIE_S1 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\02\\SIE_S1EPS02502122025.txt\"\n",
    "S_Excel = fr\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\02\\Dataframe Pila {FECHA_Salida}.xlsx\"\n",
    "\n",
    "# Entrada Oficce\n",
    "R_R1 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "R_S1 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "R_NC = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "R_Relaciones_Laborales = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\relaciones laborales\\Reporte_Afiliados Contributivo Relaciones Laborales_2025_12_02.csv\"\n",
    "R_MS_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validación Archivos Maestro_2025_12_02.csv\"\n",
    "R_MS_ADRES = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0027112025.TXT\"\n",
    "R_Pila_SIE_I = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Pila_SIE\\Pila I\"\n",
    "R_Pila_SIE_IP = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Pila_SIE\\Pila IP\"\n",
    "R_ABX = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "R_ACX = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "R_UGPP = r\"\\\\Servernas\\AYC2\\(01. ASEGURAMIENTO)\\01. ASEGURAMIENTO\\01. REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\REPORTE RESOLUCION_0762_2023\\12_Diciembre\\UGPP Septiembre.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mora = \"30/08/2025\"\n",
    "Dia = \"01/09/2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAlida oficce\n",
    "#FECHA_Salida = \"11-11-2025\"\n",
    "#SIE_S1 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\11_Noviembre\\11\\SIE_S1EPS02511112025.txt\"\n",
    "#S_Excel = fr\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\11_Noviembre\\11\\Dataframe Pila {FECHA_Salida}.xlsx\"\n",
    "\n",
    "# Entrada Oficce\n",
    "#R_R1 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "#R_S1 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\AUTOMATICO R1\\All-AUTO-R1.txt\"\n",
    "#R_NC = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Procesos BDUA EPS\\NC\\NC VAL\\all-NC-VAL.txt\"\n",
    "#R_S3 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "#R_Relaciones_Laborales = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\relaciones laborales\\Reporte_Afiliados Contributivo Relaciones Laborales_2025_11_11.csv\"\n",
    "#R_MS_SIE = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validación Archivos Maestro_2025_11_11.csv\"\n",
    "#R_MS_ADRES = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0010112025.TXT\"\n",
    "#R_Pila_SIE_I = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Pila_SIE\\Pila I\"\n",
    "#R_Pila_SIE_IP = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Pila_SIE\\Pila IP\"\n",
    "#R_ABX = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ABX\\Merg_ABX_2018_2025.TXT\"\n",
    "#R_ACX = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Compensación\\ACX\\Merg_ACX_2018_2025.TXT\"\n",
    "#R_UGPP = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\11_Noviembre\\11\\UGPP Junio.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Carga de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_13576\\956002780.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  Df_Pila_IP = Df_Pila_IP.applymap(lambda x: 1 if x == \"X\" else (0 if pd.isna(x) or x == \"\" else x))\n"
     ]
    }
   ],
   "source": [
    "Df_MS_ADRES = pd.read_csv(R_MS_ADRES, sep=\",\", encoding='ANSI', dtype=str, header=None)\n",
    "Df_R1_Auto = pd.read_csv(R_R1, sep=\",\", encoding='ANSI', dtype=str)\n",
    "Df_NC = pd.read_csv(R_NC, sep=\",\", encoding='ANSI', dtype=str)\n",
    "Df_S1 = pd.read_csv(R_S1, sep=\",\", encoding='ANSI', dtype=str)\n",
    "Df_Relacion_LAboral = pd.read_csv(R_Relaciones_Laborales, sep=\";\", encoding='ANSI', dtype=str)\n",
    "Df_ABX = pd.read_csv(R_ABX, sep=\",\", encoding='ANSI', dtype=str)\n",
    "Df_ACX = pd.read_csv(R_ACX, sep=\",\", encoding='ANSI', dtype=str)\n",
    "\n",
    "Df_UGPP = pd.read_excel(R_UGPP, sheet_name=\"2001 Desagregado de Cartera\", header=0, dtype=str)\n",
    "Df_UGPP = Df_UGPP[[\"TIPO DE DOCUMENTO COTIZANTE\", \"NÚMERO DE DOCUMENTO DEL COTIZANTE\"]]\n",
    "\n",
    "# Buscar todos los archivos .TXT en la carpeta R_Pila_SIE_I\n",
    "txt_files = glob.glob(os.path.join(R_Pila_SIE_I, \"*.txt\"))\n",
    "# Leer cada archivo y almacenarlo en una lista de DataFrames\n",
    "dfs = [pd.read_csv(file, delimiter=\"|\", encoding=\"ANSI\", dtype=str) for file in txt_files]\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "Df_Pila_I = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Buscar todos los archivos .TXT en la carpeta R_Pila_SIE_IP\n",
    "txt_files = glob.glob(os.path.join(R_Pila_SIE_IP, \"*.txt\"))\n",
    "# Leer cada archivo y almacenarlo en una lista de DataFrames\n",
    "dfs = [pd.read_csv(file, delimiter=\"|\", encoding=\"ANSI\", header=None, dtype=str) for file in txt_files]\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "Df_Pila_IP = pd.concat(dfs, ignore_index=True)\n",
    "Df_Pila_IP = Df_Pila_IP.applymap(lambda x: 1 if x == \"X\" else (0 if pd.isna(x) or x == \"\" else x))\n",
    "\n",
    "# Asignar los encabezados de Df_Pila_I a Df_Pila_IP para que tengan las mismas columnas\n",
    "Df_Pila_IP.columns = Df_Pila_I.columns\n",
    "# Concatenar Df_Pila_I y Df_Pila_IP, colocando los registros de Df_Pila_IP debajo de los de Df_Pila_I\n",
    "Df_Pila_SIE = pd.concat([Df_Pila_I, Df_Pila_IP], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Pila_SIE = Df_Pila_SIE[Df_Pila_SIE[\"Correcciones\"] != \"A\"]\n",
    "Df_ABX = Df_ABX[Df_ABX[\"Glosas\"].isna() | (Df_ABX[\"Glosas\"] == \"\")]\n",
    "Df_ACX = Df_ACX[Df_ACX[\"Codigo_Glosa\"].isna() | (Df_ACX[\"Codigo_Glosa\"] == \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo_documento                     object\n",
      "numero_identificacion              object\n",
      "tipo_documento_aportante           object\n",
      "numero_identificacion_aportante    object\n",
      "razon_social                       object\n",
      "fecha_ingreso                      object\n",
      "Unnamed: 6                         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Df_Relacion_LAboral.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = [\n",
    "    \"N° Identificación Aportante\", \"Perido Pago\", \"Tipo Documento Cotizante\",\n",
    "    \"N° Identificación Cotizante\", \"ING\", \"RET\", \"Días Cotizados\",\n",
    "    \"Número Planilla\", \"Fecha Planilla\"\n",
    "]\n",
    "Df_Pila_SIE = Df_Pila_SIE[cols_keep]\n",
    "\n",
    "cols_keep = [\n",
    "    \"Periodo_Compensado\", \"Tp_Do\", \"No_Do\",\n",
    "    \"Dias_Compensados\", \"Serial_BDUA\"\n",
    "]\n",
    "Df_ABX = Df_ABX[cols_keep]\n",
    "\n",
    "cols_keep = [\n",
    "    \"Periodo_Compensado\", \"Tp_DC\", \"No_DC\",\n",
    "    \"Dias_Cotizados\", \"Serial_BDUA\"\n",
    "]\n",
    "Df_ACX = Df_ACX[cols_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar los encabezados de Df_ACX a Df_ABX para que tengan las mismas columnas\n",
    "Df_ABX.columns = Df_ACX.columns\n",
    "# Concatenar Df_ACX y Df_ABX, colocando los registros de Df_ABX debajo de los de Df_ACX\n",
    "Df_ABX_ACX = pd.concat([Df_ACX, Df_ABX], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Df_Pila_I\n",
    "del Df_Pila_IP\n",
    "del Df_ABX\n",
    "del Df_ACX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [1, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 31, 32, 35, 36, 37, 38, 39, 40]\n",
    "Df_MS_ADRES = Df_MS_ADRES.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del filtro: 1724014\n",
      "Despues del filtro: 95330\n",
      "Número de registros únicos: 83719\n"
     ]
    }
   ],
   "source": [
    "print(f\"Antes del filtro: {len(Df_ABX_ACX)}\")\n",
    "\n",
    "# Extraer únicamente el patrón \"mm/yyyy\" de la columna \"Periodo_Compensado\"\n",
    "Df_ABX_ACX[\"Periodo_Compensado\"] = Df_ABX_ACX[\"Periodo_Compensado\"].str.extract(r\"(\\d{2}/\\d{4})\")[0]\n",
    "\n",
    "# Convertir la columna \"Periodo_Compensado\" a datetime asumiendo el día 1 para todas las fechas\n",
    "Df_ABX_ACX[\"Periodo_Compensado\"] = pd.to_datetime(Df_ABX_ACX[\"Periodo_Compensado\"], format=\"%m/%Y\")\n",
    "\n",
    "# Para cada grupo definido por [\"Serial_BDUA\"],\n",
    "# obtener la fecha máxima y filtrar solo los registros que tengan esa fecha.\n",
    "max_per_group = Df_ABX_ACX.groupby([\"Serial_BDUA\"])[\"Periodo_Compensado\"].transform(\"max\")\n",
    "Df_ABX_ACX = Df_ABX_ACX[Df_ABX_ACX[\"Periodo_Compensado\"] == max_per_group].copy()\n",
    "\n",
    "# Dar formato de \"dd/mm/yyyy\" a la columna \"Periodo_Compensado\" (queda como string)\n",
    "Df_ABX_ACX[\"Periodo_Compensado\"] = Df_ABX_ACX[\"Periodo_Compensado\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "print(f\"Despues del filtro: {len(Df_ABX_ACX)}\")\n",
    "\n",
    "\n",
    "# Convertir la columna \"Dias_Cotizados\" a numérica para poder sumar\n",
    "Df_ABX_ACX[\"Dias_Cotizados\"] = pd.to_numeric(Df_ABX_ACX[\"Dias_Cotizados\"], errors=\"coerce\")\n",
    "# Sumar los [\"Serial_BDUA\"]\n",
    "Df_ABX_ACX[\"Dias_Cotizados\"] = Df_ABX_ACX.groupby(\n",
    "    [\"Serial_BDUA\"]\n",
    ")[\"Dias_Cotizados\"].transform(\"sum\")\n",
    "\n",
    "\n",
    "# Dejar un solo registro por cada grupo de las columnas indicadas\n",
    "Df_ABX_ACX = Df_ABX_ACX.drop_duplicates(\n",
    "    subset=[\"Serial_BDUA\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "print(f\"Número de registros únicos: {len(Df_ABX_ACX)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"fecha_ingreso\" a datetime utilizando el formato \"%d/%m/%Y\"\n",
    "Df_Relacion_LAboral[\"fecha_ingreso\"] = pd.to_datetime(Df_Relacion_LAboral[\"fecha_ingreso\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# Para cada grupo definido por [\"tipo_documento\", \"numero_identificacion\"], \n",
    "# obtener el índice del registro con la fecha más reciente en \"fecha_ingreso\"\n",
    "idx_max = Df_Relacion_LAboral.groupby([\"tipo_documento\", \"numero_identificacion\"])[\"fecha_ingreso\"].idxmax()\n",
    "\n",
    "# Filtrar el DataFrame para quedarnos solo con los registros de fecha máxima en cada grupo\n",
    "Df_Relacion_LAboral = Df_Relacion_LAboral.loc[idx_max].copy()\n",
    "\n",
    "# Mantener la columna \"fecha_ingreso\" en formato \"dd/mm/yyyy\"\n",
    "Df_Relacion_LAboral[\"fecha_ingreso\"] = Df_Relacion_LAboral[\"fecha_ingreso\"].dt.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"Fecha_Proceso\" a datetime utilizando el formato \"%d/%m/%Y\"\n",
    "Df_R1_Auto[\"Fecha_Proceso\"] = pd.to_datetime(Df_R1_Auto[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Para cada grupo definido por [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"], \n",
    "# obtener el índice del registro con la fecha más reciente en \"Fecha_Proceso\"\n",
    "idx_max = Df_R1_Auto.groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"Fecha_Proceso\"].idxmax()\n",
    "\n",
    "# Filtrar el DataFrame para quedarnos solo con los registros de fecha máxima en cada grupo\n",
    "Df_R1_Auto = Df_R1_Auto.loc[idx_max].copy()\n",
    "\n",
    "# Mantener la columna \"Fecha_Proceso\" en formato \"dd/mm/yyyy\"\n",
    "Df_R1_Auto[\"Fecha_Proceso\"] = Df_R1_Auto[\"Fecha_Proceso\"].dt.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"FECHA_NOVEDAD\" a datetime utilizando el formato \"%d/%m/%Y\"\n",
    "Df_NC[\"FECHA_NOVEDAD\"] = pd.to_datetime(Df_NC[\"FECHA_NOVEDAD\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Para cada grupo definido por [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"], \n",
    "# obtener el índice del registro con la fecha más reciente en \"FECHA_NOVEDAD\"\n",
    "idx_max = Df_NC.groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"FECHA_NOVEDAD\"].idxmax()\n",
    "\n",
    "# Filtrar el DataFrame para quedarnos solo con los registros de fecha máxima en cada grupo\n",
    "Df_NC = Df_NC.loc[idx_max].copy()\n",
    "\n",
    "# Mantener la columna \"FECHA_NOVEDAD\" en formato \"dd/mm/yyyy\"\n",
    "Df_NC[\"FECHA_NOVEDAD\"] = Df_NC[\"FECHA_NOVEDAD\"].dt.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"Fecha_Proceso\" a datetime utilizando el formato \"%d/%m/%Y\"\n",
    "Df_S1[\"Fecha_Proceso\"] = pd.to_datetime(Df_S1[\"Fecha_Proceso\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Para cada grupo definido por [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"], \n",
    "# obtener el índice del registro con la fecha más reciente en \"Fecha_Proceso\"\n",
    "idx_max = Df_S1.groupby([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"Fecha_Proceso\"].idxmax()\n",
    "\n",
    "# Filtrar el DataFrame para quedarnos solo con los registros de fecha máxima en cada grupo\n",
    "Df_S1 = Df_S1.loc[idx_max].copy()\n",
    "\n",
    "# Mantener la columna \"Fecha_Proceso\" en formato \"dd/mm/yyyy\"\n",
    "Df_S1[\"Fecha_Proceso\"] = Df_S1[\"Fecha_Proceso\"].dt.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del filtro: 987337\n",
      "Despues del filtro: 67124\n"
     ]
    }
   ],
   "source": [
    "print(f\"Antes del filtro: {len(Df_Pila_SIE)}\")\n",
    "\n",
    "# Extraer únicamente el patrón \"YYYY-MM\" de la columna \"Perido Pago\"\n",
    "Df_Pila_SIE[\"Perido Pago\"] = Df_Pila_SIE[\"Perido Pago\"].str.extract(r\"(\\d{4}-\\d{2})\")[0]\n",
    "\n",
    "# Convertir la columna \"Perido Pago\" a datetime asumiendo el día 1 para todas las fechas\n",
    "Df_Pila_SIE[\"Perido Pago\"] = pd.to_datetime(Df_Pila_SIE[\"Perido Pago\"], format=\"%Y-%m\")\n",
    "\n",
    "# Para cada grupo definido por [\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "# obtener la fecha máxima y filtrar solo los registros que tengan esa fecha.\n",
    "max_per_group = Df_Pila_SIE.groupby([\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"])[\"Perido Pago\"].transform(\"max\")\n",
    "Df_Pila_SIE = Df_Pila_SIE[Df_Pila_SIE[\"Perido Pago\"] == max_per_group].copy()\n",
    "\n",
    "# Dar formato de \"dd/mm/yyyy\" a la columna \"Perido Pago\" (queda como string)\n",
    "Df_Pila_SIE[\"Perido Pago\"] = Df_Pila_SIE[\"Perido Pago\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "print(f\"Despues del filtro: {len(Df_Pila_SIE)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"Días Cotizados\" a numérica para poder sumar\n",
    "Df_Pila_SIE[\"Días Cotizados\"] = pd.to_numeric(Df_Pila_SIE[\"Días Cotizados\"], errors=\"coerce\")\n",
    "\n",
    "# Sumar los \"Días Cotizados\" por cada grupo de [\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"]\n",
    "Df_Pila_SIE[\"Días Cotizados\"] = Df_Pila_SIE.groupby(\n",
    "    [\"N° Identificación Aportante\", \"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"]\n",
    ")[\"Días Cotizados\"].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos en ING: [0 1]\n",
      "Valores únicos en RET: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# Convertir las columnas \"ING\" y \"RET\" a valores numéricos enteros\n",
    "# Esto unifica los valores \"0\", \"1\", 0 y 1, quedando solo 0 y 1.\n",
    "for col in [\"ING\", \"RET\"]:\n",
    "    Df_Pila_SIE[col] = pd.to_numeric(Df_Pila_SIE[col], errors=\"coerce\").astype(int)\n",
    "\n",
    "# Verificar los valores únicos en cada columna\n",
    "print(\"Valores únicos en ING:\", Df_Pila_SIE[\"ING\"].unique())\n",
    "print(\"Valores únicos en RET:\", Df_Pila_SIE[\"RET\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las columnas \"ING\" y \"RET\", si algún registro del grupo tiene \"1\",\n",
    "# se asigna \"1\" a todos los registros de ese grupo.\n",
    "for col in [\"ING\", \"RET\"]:\n",
    "    Df_Pila_SIE[col] = Df_Pila_SIE.groupby(\n",
    "        [\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\", \"N° Identificación Aportante\"]\n",
    "    )[col].transform(lambda x: pd.Series(1, index=x.index) if (x == 1).any() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros únicos: 60265\n"
     ]
    }
   ],
   "source": [
    "# Dejar un solo registro por cada grupo de las columnas indicadas\n",
    "Df_Pila_SIE = Df_Pila_SIE.drop_duplicates(\n",
    "    subset=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\", \"N° Identificación Aportante\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "\n",
    "print(f\"Número de registros únicos: {len(Df_Pila_SIE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer las columnas 4, 5 y 28 dari Df_MS_ADRES, y renombrarlas para hacer el join\n",
    "df_ms_subset = Df_MS_ADRES[[0, 4, 5, 17, 33, 34, 41, 42]].copy()\n",
    "df_ms_subset.columns = [\"Serial_BDUA\", \"Tipo Documento Cotizante\", \"N° Identificación Cotizante\", \"Tipo Afiliado\", \"Estado ADRES\", \"Ultima_Fecha_ADRES\", \"Población\", \"Esatado_espcial_BDUA\"]\n",
    "\n",
    "# Hacer un merge de Df_Pila_SIE con la subset de Df_MS_ADRES usando las columnas de identificación\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(df_ms_subset, on=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"], how=\"left\")\n",
    "\n",
    "# Filtrar el dataframe para eliminar los registros con \"Estado ADRES\" igual a \"AF\" o \"SM\"\n",
    "Df_Pila_SIE = Df_Pila_SIE[Df_Pila_SIE[\"Estado ADRES\"].notna() & ~Df_Pila_SIE[\"Estado ADRES\"].isin([\"AF\", \"SM\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Pila_SIE[\"Cant_Aportante\"] = Df_Pila_SIE.groupby(\n",
    "    [\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"]\n",
    ")[\"N° Identificación Cotizante\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osmarrincon\\AppData\\Local\\Temp\\ipykernel_13576\\2416456157.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ).apply(process_group)\n"
     ]
    }
   ],
   "source": [
    "def process_group(group):\n",
    "    # Si todos los registros del grupo tienen RET==1,\n",
    "    # sumar \"Días Cotizados\" y dejar un solo registro con el mensaje de validación.\n",
    "    if (group[\"RET\"] == 1).all():\n",
    "        new_row = group.iloc[0].copy()\n",
    "        new_row[\"Días Cotizados\"] = group[\"Días Cotizados\"].sum()\n",
    "        new_row[\"Validar_Retiro\"] = \"Todos Los Aportantes Marcan Retiro\"\n",
    "        return new_row.to_frame().T  # Regresa un DataFrame de una sola fila\n",
    "    else:\n",
    "        # Si no todos tienen RET==1, dejar el grupo sin cambios,\n",
    "        # pero agregar la columna de validación a cada registro.\n",
    "        group = group.copy()\n",
    "        group[\"Validar_Retiro\"] = \"Todos Los Aportantes No Marcan Retiro\"\n",
    "        return group\n",
    "\n",
    "# Aplicar la función de agrupación según las columnas indicadas.\n",
    "Df_Pila_SIE = Df_Pila_SIE.groupby(\n",
    "    [\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "    group_keys=False\n",
    ").apply(process_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la columna \"Fecha_Efectiva\" a \"Fecha_R1_Auto\" en Df_R1_Auto\n",
    "Df_R1_Auto = Df_R1_Auto.rename(columns={\"Fecha_Efectiva\": \"Fecha_R1_Auto\"})\n",
    "\n",
    "# Seleccionar únicamente las columnas de identificación y la nueva columna de fecha\n",
    "df_r1_subset = Df_R1_Auto[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Fecha_R1_Auto\"]]\n",
    "\n",
    "# Realizar el merge entre Df_Pila_SIE y el subset de Df_R1_Auto\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(\n",
    "    df_r1_subset,\n",
    "    left_on=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_Pila_SIE = Df_Pila_SIE.drop(columns=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la columna \"FECHA_NOVEDAD\" a \"Fecha_NC\" en Df_NC\n",
    "Df_NC = Df_NC.rename(columns={\"FECHA_NOVEDAD\": \"Fecha_NC\"})\n",
    "\n",
    "# Seleccionar únicamente las columnas de identificación y la nueva columna de fecha\n",
    "df_NC_subset = Df_NC[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Fecha_NC\"]]\n",
    "\n",
    "# Realizar el merge entre Df_Pila_SIE y el subset de Df_NC\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(\n",
    "    df_NC_subset,\n",
    "    left_on=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_Pila_SIE = Df_Pila_SIE.drop(columns=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la columna \"Fecha_Efectiva\" a \"Fecha_S1_Auto\" en Df_S1\n",
    "Df_S1 = Df_S1.rename(columns={\"Fecha_Efectiva\": \"Fecha_S1_Auto\"})\n",
    "\n",
    "# Seleccionar únicamente las columnas de identificación y la nueva columna de fecha\n",
    "df_S1_subset = Df_S1[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"Fecha_S1_Auto\"]]\n",
    "\n",
    "# Realizar el merge entre Df_Pila_SIE y el subset de Df_S1\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(\n",
    "    df_S1_subset,\n",
    "    left_on=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_Pila_SIE = Df_Pila_SIE.drop(columns=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la columna \"Fecha_Efectiva\" a \"Fecha_R1_Auto\" en Df_S1\n",
    "Df_Relacion_LAboral = Df_Relacion_LAboral.rename(columns={\"fecha_ingreso\": \"Fecha_ing_Relacion_Laboral\"})\n",
    "\n",
    "# Seleccionar únicamente las columnas de identificación y la nueva columna de fecha\n",
    "df_RL_subset = Df_Relacion_LAboral[[\"tipo_documento\", \"numero_identificacion\", \"Fecha_ing_Relacion_Laboral\"]]\n",
    "\n",
    "# Realizar el merge entre Df_Pila_SIE y el subset de Df_S1\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(\n",
    "    df_RL_subset,\n",
    "    left_on=[\"Tipo Documento Cotizante\", \"N° Identificación Cotizante\"],\n",
    "    right_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_Pila_SIE = Df_Pila_SIE.drop(columns=[\"tipo_documento\", \"numero_identificacion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la columna \"Periodo_Compensado\" a \"Fecha_ACX_ABX\" en Df_ABX_ACX\n",
    "Df_ABX_ACX = Df_ABX_ACX.rename(columns={\"Periodo_Compensado\": \"Fecha_ACX_ABX\"})\n",
    "\n",
    "# Seleccionar únicamente las columnas de identificación y las columnas a agregar\n",
    "df_ABX_ACX_subset = Df_ABX_ACX[[\"Serial_BDUA\", \"Fecha_ACX_ABX\", \"Dias_Cotizados\"]]\n",
    "\n",
    "# Realizar el merge entre Df_Pila_SIE y el subconjunto de Df_ABX_ACX usando \"Serial_BDUA\" como llave\n",
    "Df_Pila_SIE = Df_Pila_SIE.merge(\n",
    "    df_ABX_ACX_subset,\n",
    "    on=\"Serial_BDUA\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def ajustar_fecha(row):\n",
    "    # Convertir las fechas de cadena a objeto datetime\n",
    "    fecha_pp = datetime.strptime(row[\"Perido Pago\"], \"%d/%m/%Y\")\n",
    "    # Si Fecha_R1_Auto no es nula, se usa; de lo contrario, se toma fecha_pp\n",
    "    fecha_r1 = datetime.strptime(row[\"Fecha_R1_Auto\"], \"%d/%m/%Y\") if pd.notnull(row[\"Fecha_R1_Auto\"]) else fecha_pp\n",
    "    dias = int(row[\"Días Cotizados\"])\n",
    "    \n",
    "    # Si \"Perido Pago\" y \"Fecha_R1_Auto\" pertenecen al mismo mes y año, se usa fecha_r1 como base,\n",
    "    # de lo contrario se utiliza fecha_pp.\n",
    "    if fecha_pp.month == fecha_r1.month and fecha_pp.year == fecha_r1.year:\n",
    "        fecha_base = fecha_r1\n",
    "        validacion = \"Dias sumados R1\"\n",
    "    else:\n",
    "        fecha_base = fecha_pp\n",
    "        validacion = \"Dias sumados pila\"\n",
    "        \n",
    "    # Sumar los días a la fecha base\n",
    "    fecha_calculada = fecha_base + timedelta(days=dias)\n",
    "    \n",
    "    # Si la fecha calculada está en el mismo mes y año que la fecha base, se conserva;\n",
    "    # de lo contrario, se asigna el primer día del mes siguiente a la fecha base.\n",
    "    if fecha_calculada.month == fecha_base.month and fecha_calculada.year == fecha_base.year:\n",
    "        fecha_final = fecha_calculada\n",
    "    else:\n",
    "        if fecha_base.month == 12:\n",
    "            fecha_final = fecha_base.replace(year=fecha_base.year + 1, month=1, day=1)\n",
    "        else:\n",
    "            fecha_final = fecha_base.replace(month=fecha_base.month + 1, day=1)\n",
    "            \n",
    "    return pd.Series([fecha_final.strftime(\"%d/%m/%Y\"), validacion])\n",
    "\n",
    "# Aplicar la función a Df_Pila_SIE para crear las nuevas columnas\n",
    "Df_Pila_SIE[[\"fecha_Ajustada_Pila\", \"Validar_suma_dias\"]] = Df_Pila_SIE.apply(ajustar_fecha, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "def ajustar_fecha_acxab(row):\n",
    "    fecha_str = row[\"Fecha_ACX_ABX\"]\n",
    "    # Manejar valores nulos\n",
    "    if pd.isnull(fecha_str):\n",
    "        return \"\"\n",
    "    # Asegurarse de que es string\n",
    "    if not isinstance(fecha_str, str):\n",
    "        fecha_str = str(fecha_str)\n",
    "    try:\n",
    "        fecha = datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al convertir {fecha_str}: {e}\")\n",
    "        return \"\"\n",
    "    dias = int(row[\"Dias_Cotizados\"])\n",
    "    fecha_sumada = fecha + timedelta(days=dias)\n",
    "    ultimo_dia = calendar.monthrange(fecha.year, fecha.month)[1]\n",
    "    if (fecha_sumada.month != fecha.month) or (fecha_sumada.day == ultimo_dia):\n",
    "        next_month = fecha.month + 1\n",
    "        next_year = fecha.year\n",
    "        if next_month > 12:\n",
    "            next_month = 1\n",
    "            next_year += 1\n",
    "        fecha_ajustada = datetime(next_year, next_month, 1)\n",
    "    else:\n",
    "        fecha_ajustada = fecha_sumada\n",
    "    return fecha_ajustada.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "Df_Pila_SIE[\"Fecha_ACX_ABX_Ajustada\"] = Df_Pila_SIE.apply(ajustar_fecha_acxab, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val < mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Inicializar la nueva columna \"Movilidad\" con valores vacíos.\n",
    "Df_Pila_SIE[\"Movilidad\"] = \"\"\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    (Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date)) &\n",
    "    (Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date))\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Mora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir \"Perido Pago\" a datetime para la comparación\n",
    "Df_Pila_SIE[\"Perido Pago_dt\"] = pd.to_datetime(Df_Pila_SIE[\"Perido Pago\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Condición:\n",
    "# - \"Fecha_ACX_ABX\" es nula o vacía\n",
    "# - \"Perido Pago\" (en formato datetime) es menor que la fecha de mora (mora_dt)\n",
    "# - \"Fecha_ing_Relacion_Laboral\" es nula o vacía\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    (Df_Pila_SIE[\"Fecha_ACX_ABX\"].isna() | (Df_Pila_SIE[\"Fecha_ACX_ABX\"] == \"\")) &\n",
    "    (Df_Pila_SIE[\"Perido Pago_dt\"] < mora_dt) &\n",
    "    (Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].isna() | (Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"] == \"\"))\n",
    ")\n",
    "\n",
    "# Asignar el valor de \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Mora\"\n",
    "\n",
    "# Si se desea, se puede eliminar la columna temporal \"Perido Pago_dt\"\n",
    "Df_Pila_SIE.drop(columns=[\"Perido Pago_dt\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val > mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    #(Df_Pila_SIE[\"Movilidad\"].isna()) | \n",
    "    #(Df_Pila_SIE[\"Movilidad\"] == \"\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    #(Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    (Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date)) \n",
    "    #(Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date))\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Relación laboral Reciente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir las variables Mora y Dia a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "Dia_dt = datetime.strptime(Dia, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha:\n",
    "# Si está vacío, considerarlo True;\n",
    "# en caso contrario, validar que la fecha esté entre mora_dt y Dia_dt.\n",
    "def check_date_in_range(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return mora_dt <= dt_val <= Dia_dt\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha (Perido Pago, Fecha_R1_Auto,\n",
    "#   Fecha_ing_Relacion_Laboral y Fecha_ACX_ABX) debe estar vacía o con fecha\n",
    "#   entre mora_dt y Dia_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    (Df_Pila_SIE[\"Perido Pago\"].apply(check_date_in_range)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date_in_range)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date_in_range)) &\n",
    "    (Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date_in_range))\n",
    ")\n",
    "\n",
    "# Asignar \"Al dia\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Al dia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val >= mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    #(Df_Pila_SIE[\"Movilidad\"].isna()) | \n",
    "    #(Df_Pila_SIE[\"Movilidad\"] == \"\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    #(Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) \n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date)) \n",
    "    (Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date))\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Al dia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val >= mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    #(Df_Pila_SIE[\"Movilidad\"].isna()) | \n",
    "    #(Df_Pila_SIE[\"Movilidad\"] == \"\") &\n",
    "    #(Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    (Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) \n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date)) \n",
    "    #(Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date))\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Al dia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val < mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Inicializar la nueva columna \"Movilidad\" con valores vacíos.\n",
    "#Df_Pila_SIE[\"Movilidad\"] = \"\"\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    (Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    (Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].isna()) &\n",
    "    (Df_Pila_SIE[\"Fecha_ACX_ABX\"].isna())\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Mora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convertir la variable Mora a datetime\n",
    "mora_dt = datetime.strptime(mora, \"%d/%m/%Y\")\n",
    "\n",
    "# Función para evaluar cada valor de fecha: si es vacío, considerar True;\n",
    "# en caso contrario, validar que la fecha sea anterior a mora_dt.\n",
    "def check_date(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return True\n",
    "    try:\n",
    "        dt_val = datetime.strptime(val, \"%d/%m/%Y\")\n",
    "        return dt_val > mora_dt\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Condición:\n",
    "# - \"Validar_Retiro\" debe ser \"Todos Los Aportantes No Marcan Retiro\"\n",
    "# - \"Esatado_espcial_BDUA\" debe ser \"0\"\n",
    "# - Cada una de las columnas de fecha, ya sea vacía o con fecha anterior a mora_dt.\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\") &\n",
    "    #(Df_Pila_SIE[\"Movilidad\"].isna()) | \n",
    "    #(Df_Pila_SIE[\"Movilidad\"] == \"\") &\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"0\") &\n",
    "    #(Df_Pila_SIE[\"Perido Pago\"].apply(check_date)) &\n",
    "    #(Df_Pila_SIE[\"Fecha_R1_Auto\"].apply(check_date)) &\n",
    "    (Df_Pila_SIE[\"Fecha_ing_Relacion_Laboral\"].apply(check_date)) \n",
    "    #(Df_Pila_SIE[\"Fecha_ACX_ABX\"].apply(check_date))\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Relación laboral Reciente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"1\")\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Activo Regimen especial\"\n",
    "\n",
    "cond = (\n",
    "    (Df_Pila_SIE[\"Esatado_espcial_BDUA\"] == \"4\")\n",
    ")\n",
    "\n",
    "# Asignar \"Mora\" a la columna \"Movilidad\" para los registros que cumplan la condición.\n",
    "Df_Pila_SIE.loc[cond, \"Movilidad\"] = \"Pencionado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna clave en Df_Pila_SIE uniendo 'Tipo Documento Cotizante' y 'N° Identificación Cotizante'\n",
    "Df_Pila_SIE[\"key\"] = Df_Pila_SIE[\"Tipo Documento Cotizante\"].astype(str) + \"|\" + Df_Pila_SIE[\"N° Identificación Cotizante\"].astype(str)\n",
    "\n",
    "# Crear la clave correspondiente en Df_UGPP\n",
    "ugpp_keys = Df_UGPP[\"TIPO DE DOCUMENTO COTIZANTE\"].astype(str) + \"|\" + Df_UGPP[\"NÚMERO DE DOCUMENTO DEL COTIZANTE\"].astype(str)\n",
    "\n",
    "# Condición: el registro está en Df_UGPP y 'Validar_Retiro' es \"Todos Los Aportantes No Marcan Retiro\"\n",
    "cond_ugpp = (Df_Pila_SIE[\"key\"].isin(ugpp_keys)) & (Df_Pila_SIE[\"Validar_Retiro\"] == \"Todos Los Aportantes No Marcan Retiro\")\n",
    "\n",
    "# Actualizar la columna 'Movilidad' a 'UGPP' donde se cumplan las condiciones\n",
    "Df_Pila_SIE.loc[cond_ugpp, \"Movilidad\"] = \"UGPP\"\n",
    "\n",
    "# Eliminar la columna temporal\n",
    "Df_Pila_SIE.drop(columns=[\"key\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicar la columna \"Población\" y nombrarla \"Población_2\"\n",
    "Df_Pila_SIE.insert(14, \"Población_2\", Df_Pila_SIE[\"Población\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "jerarquia = ['9', '17', '28', '2', '1']\n",
    "\n",
    "def limpiar_poblacion(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "\n",
    "    original = valor  # Para posible trazabilidad\n",
    "\n",
    "    # 1. Reemplazar si es exactamente SIV(Dnn)\n",
    "    if re.fullmatch(r\"SIV\\(D\\d{2}\\)\", valor.strip()):\n",
    "        return \"Sisben D\"\n",
    "\n",
    "    # 2. Eliminar sufijos -SIV(...) y -SIII(...)\n",
    "    valor = re.sub(r\"-SIV\\([^\\)]*\\)\", \"\", valor)\n",
    "    valor = re.sub(r\"-SIII\\([^\\)]*\\)\", \"\", valor)\n",
    "\n",
    "    # 3. Si es solo SIII(...), eliminar todo\n",
    "    if re.fullmatch(r\"SIII\\([^\\)]*\\)\", valor.strip()):\n",
    "        return \"\"\n",
    "\n",
    "    # 4. Eliminar prefijo \"LC(0)-\"\n",
    "    if valor.startswith(\"LC(0)-\"):\n",
    "        valor = valor.replace(\"LC(0)-\", \"\")\n",
    "\n",
    "    # 5. Si queda solo \"LC(0)\", eliminar\n",
    "    if valor.strip() == \"LC(0)\":\n",
    "        return \"\"\n",
    "\n",
    "    # 6. Normalizar LC(...) con jerarquía\n",
    "    def jerarquia_lc(match):\n",
    "        contenido = match.group(1)\n",
    "        valores = [v.strip() for v in contenido.split('|')]\n",
    "\n",
    "        if len(valores) == 1:\n",
    "            return f\"LC({valores[0]})\"\n",
    "\n",
    "        for prioridad in jerarquia:\n",
    "            if prioridad in valores:\n",
    "                return f\"LC({prioridad})\"\n",
    "\n",
    "        return \"\"  # Eliminar si no hay valor válido\n",
    "\n",
    "    valor = re.sub(r\"LC\\(([^\\)]*)\\)\", jerarquia_lc, valor)\n",
    "\n",
    "    return valor.strip()\n",
    "\n",
    "# Aplicar la función\n",
    "Df_Pila_SIE[\"Población_2\"] = Df_Pila_SIE[\"Población_2\"].apply(limpiar_poblacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def ajustar_fecha_envio(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar la columna original\n",
    "    df[\"Fecha envio\"] = df[\"fecha_Ajustada_Pila\"]\n",
    "\n",
    "    # Convertir todas las columnas de fechas a datetime\n",
    "    columnas_fecha = [\n",
    "        \"Ultima_Fecha_ADRES\",\n",
    "        \"Fecha_R1_Auto\",\n",
    "        \"Fecha_NC\",\n",
    "        \"Fecha envio\"\n",
    "    ]\n",
    "    for col in columnas_fecha:\n",
    "        df[col] = pd.to_datetime(df[col], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "    # Ajustar la fecha de envío si es menor o igual que alguna de las otras\n",
    "    columnas_comparacion = [\n",
    "        \"Ultima_Fecha_ADRES\",\n",
    "        \"Fecha_R1_Auto\",\n",
    "        \"Fecha_NC\"\n",
    "    ]\n",
    "    for col in columnas_comparacion:\n",
    "        df[\"Fecha envio\"] = df[[\"Fecha envio\", col]].apply(\n",
    "            lambda row: row[col] + timedelta(days=1) if pd.notna(row[col]) and row[\"Fecha envio\"] <= row[col] else row[\"Fecha envio\"],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Convertir de nuevo al formato \"dd/mm/yyyy\"\n",
    "    for col in columnas_fecha:\n",
    "        df[col] = df[col].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "Df_Pila_SIE = ajustar_fecha_envio(Df_Pila_SIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Asegúrate de que ambas columnas sean datetime\n",
    "Df_Pila_SIE[[\"Fecha envio\",\"Fecha_ACX_ABX_Ajustada\"]] = (\n",
    "    Df_Pila_SIE[[\"Fecha envio\",\"Fecha_ACX_ABX_Ajustada\"]]\n",
    "    .apply(pd.to_datetime, format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    ")\n",
    "\n",
    "# 2) Donde 'Fecha envio' sea menor, reasignas el valor de 'Fecha_ACX_ABX_Ajustada'\n",
    "mask = Df_Pila_SIE[\"Fecha envio\"] < Df_Pila_SIE[\"Fecha_ACX_ABX_Ajustada\"]\n",
    "Df_Pila_SIE.loc[mask, \"Fecha envio\"] = Df_Pila_SIE.loc[mask, \"Fecha_ACX_ABX_Ajustada\"]\n",
    "\n",
    "# 3) (Opcional) Si quieres volver a strftime:\n",
    "Df_Pila_SIE[\"Fecha envio\"] = Df_Pila_SIE[\"Fecha envio\"].dt.strftime(\"%d/%m/%Y\")\n",
    "Df_Pila_SIE[\"Fecha_ACX_ABX_Ajustada\"] = Df_Pila_SIE[\"Fecha_ACX_ABX_Ajustada\"].dt.strftime(\"%d/%m/%Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Guardando datafarmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Pila_SIE.to_excel(S_Excel, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "print(Df_Pila_SIE[\"RET\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "print(Df_Pila_SIE[\"ING\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300\n"
     ]
    }
   ],
   "source": [
    "print(len(Df_Pila_SIE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
