{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3056243",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6350e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Añadir la carpeta raíz del proyecto a sys.path\n",
    "sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "#sys.path.append(os.path.abspath(\"D:\\Proyectos Python\\capresoca-data-automation\"))\n",
    "\n",
    "from datetime import datetime  \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from src.file_loader import cargar_maestros_ADRES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539faed",
   "metadata": {},
   "source": [
    "# Rutas y constantes\n",
    "*1. Oficce*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79141933",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Novedades = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "R_Ingresos = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "R_MS_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0029052025.TXT\"\n",
    "R_MS_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0029052025.TXT\"\n",
    "\n",
    "# Ruta de Salida\n",
    "\n",
    "Carpeta = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\informes\\2025\\CTO135.2025 Informe  #6\\ACTIVIDAD 11\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c24cfc",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e893a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"01/05/2025\"\n",
    "N_txt = \"GREPS0254062025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094d753",
   "metadata": {},
   "source": [
    "*2. Home*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03ca255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R_Novedades = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "#R_Ingresos = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "#R_S3 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "#R_MS_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0026052025.TXT\"\n",
    "#R_MS_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0026052025.TXT\"\n",
    "\n",
    "# Ruta de Salida\n",
    "\n",
    "#Carpeta = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\informes\\2025\\CTO135.2025 Informe  #6\\ACTIVIDAD 11\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d06f0",
   "metadata": {},
   "source": [
    "# Carga de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf0bc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar los maestros\n",
    "maestro_combinado = cargar_maestros_ADRES(R_MS_EPS025, R_MS_EPSC25)\n",
    "# Cargar los archivos en dataframes\n",
    "df_S3 = pd.read_csv(R_S3, sep=',', header=0, dtype=str, encoding='ansi')\n",
    "df_Novedades = pd.read_csv(R_Novedades, sep=',', header=0, dtype=str, encoding='ansi')\n",
    "df_Ingresos = pd.read_csv(R_Ingresos, sep=',', header=0, dtype=str, encoding='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1fe96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el nuevo dataframe con las columnas especificadas y valores vacíos\n",
    "columnas = [\"CONSECUTIVO\", \"TIPO_GLOSA\", \"Rev_TPS_IDN_ID\", \"Rev_HST_IDN_NUMERO_IDENTIFICACION\", \n",
    "            \"Rev_AFL_PRIMER_APELLIDO\", \"Rev_AFL_SEGUNDO_APELLIDO\", \"Rev_AFL_PRIMER_NOMBRE\", \n",
    "            \"Rev_AFL_SEGUNDO_NOMBRE\", \"Rev_AFL_FECHA_NACIMIENTO\", \"Rev_TPS_GNR_ID\", \n",
    "            \"Correc_TPS_IDN_ID\", \"Correc_HST_IDN_NUMERO_IDENTIFICACION\", \"Correc_AFL_PRIMER_APELLIDO\", \n",
    "            \"Correc_AFL_SEGUNDO_APELLIDO\", \"Correc_AFL_PRIMER_NOMBRE\", \"Correc_AFL_SEGUNDO_NOMBRE\", \n",
    "            \"Correc_AFL_FECHA_NACIMIENTO\", \"Correc_TPS_GNR_ID\", \"NOMBRE_ANEXO\"]\n",
    "\n",
    "Auditoria_059_169 = pd.DataFrame(columns=columnas, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69384a7",
   "metadata": {},
   "source": [
    "# Limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381f6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas a datetime\n",
    "df_S3['Fecha_Proceso'] = pd.to_datetime(df_S3['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "df_Ingresos['Fecha_Proceso'] = pd.to_datetime(df_Ingresos['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "df_Novedades['Fecha_Proceso'] = pd.to_datetime(df_Novedades['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "fecha_limite = pd.to_datetime(Fecha, format='%d/%m/%Y')\n",
    "\n",
    "# Columnas clave y orden final\n",
    "columnas_base = [\n",
    "    'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
    "    'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
    "    'AFL_FECHA_NACIMIENTO', 'TPS_GNR_ID', 'GLOSA', 'Nombre_Archivo', 'Fecha_Proceso'\n",
    "]\n",
    "\n",
    "# Renombres\n",
    "renombres = {\n",
    "    'TPS_IDN_ID': \"Correc_TPS_IDN_ID\",\n",
    "    'HST_IDN_NUMERO_IDENTIFICACION': \"Correc_HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    'AFL_PRIMER_APELLIDO': \"Correc_AFL_PRIMER_APELLIDO\",\n",
    "    'AFL_SEGUNDO_APELLIDO': \"Correc_AFL_SEGUNDO_APELLIDO\",\n",
    "    'AFL_PRIMER_NOMBRE': \"Correc_AFL_PRIMER_NOMBRE\",\n",
    "    'AFL_SEGUNDO_NOMBRE': \"Correc_AFL_SEGUNDO_NOMBRE\",\n",
    "    'AFL_FECHA_NACIMIENTO': \"Correc_AFL_FECHA_NACIMIENTO\",\n",
    "    'TPS_GNR_ID': \"Correc_TPS_GNR_ID\"\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_S3\n",
    "# -----------------------\n",
    "filtro_S3 = (df_S3['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_S3['GLOSA'].str.contains('GN0059') | df_S3['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "df_S3_filtrado = df_S3.loc[filtro_S3, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_Ingresos\n",
    "# -----------------------\n",
    "filtro_Ingresos = (df_Ingresos['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_Ingresos['GLOSA'].str.contains('GN0059') | df_Ingresos['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "df_Ingresos_filtrado = df_Ingresos.loc[filtro_Ingresos, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_Novedades (sin TPS_GNR_ID)\n",
    "# -----------------------\n",
    "filtro_Novedades = (df_Novedades['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_Novedades['GLOSA'].str.contains('GN0059') | df_Novedades['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "\n",
    "# Añadir columna vacía TPS_GNR_ID\n",
    "df_Novedades_tmp = df_Novedades.copy()\n",
    "df_Novedades_tmp['TPS_GNR_ID'] = np.nan\n",
    "\n",
    "df_Novedades_filtrado = df_Novedades_tmp.loc[filtro_Novedades, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Concatenar todo\n",
    "# -----------------------\n",
    "Auditoria_059_169 = pd.concat([\n",
    "    Auditoria_059_169,\n",
    "    df_S3_filtrado,\n",
    "    df_Ingresos_filtrado,\n",
    "    df_Novedades_filtrado\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d52be60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo las glosas GN0059 y GN0169 completas, con o sin contenido entre paréntesis\n",
    "Auditoria_059_169['GLOSA'] = Auditoria_059_169['GLOSA'].apply(\n",
    "    lambda x: ';'.join(re.findall(r'GN0059(?:\\([^)]*\\))?|GN0169(?:\\([^)]*\\))?', x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75382e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(Auditoria_059_169.shape[0])\n",
    "Auditoria_059_169 = Auditoria_059_169.drop_duplicates(subset=[\"Correc_TPS_IDN_ID\", \"Correc_HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "print(Auditoria_059_169.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0496cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la variable Fecha a un rango de 3 meses atrás\n",
    "fecha_inicio = pd.to_datetime(Fecha, format='%d/%m/%Y') - pd.DateOffset(months=3)\n",
    "fecha_fin = pd.to_datetime(Fecha, format='%d/%m/%Y')\n",
    "\n",
    "# Filtrar los dataframes por las glosas \"GN0059\" y \"GN0169\" y el rango de fechas\n",
    "glosas_validas = ['GN0059', 'GN0169']\n",
    "\n",
    "# Filtrar df_Ingresos\n",
    "df_Ingresos_filtrado = df_Ingresos[\n",
    "    (df_Ingresos['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_Ingresos['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_Ingresos['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Filtrar df_Novedades\n",
    "df_Novedades_filtrado = df_Novedades[\n",
    "    (df_Novedades['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_Novedades['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_Novedades['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Filtrar df_S3\n",
    "df_S3_filtrado = df_S3[\n",
    "    (df_S3['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_S3['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_S3['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Función para contar ocurrencias en un dataframe\n",
    "def contar_ocurrencias(df, col_id, col_hst, auditoria):\n",
    "    return auditoria.apply(\n",
    "        lambda row: df[\n",
    "            (df[col_id] == row['Correc_TPS_IDN_ID']) &\n",
    "            (df[col_hst] == row['Correc_HST_IDN_NUMERO_IDENTIFICACION'])\n",
    "        ].shape[0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Contar ocurrencias en cada dataframe\n",
    "Auditoria_059_169['df_Ingresos'] = contar_ocurrencias(\n",
    "    df_Ingresos_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "Auditoria_059_169['df_Novedades'] = contar_ocurrencias(\n",
    "    df_Novedades_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "Auditoria_059_169['df_S3'] = contar_ocurrencias(\n",
    "    df_S3_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "\n",
    "# Calcular el total de ocurrencias\n",
    "Auditoria_059_169['Total'] = (\n",
    "    Auditoria_059_169['df_Ingresos'] +\n",
    "    Auditoria_059_169['df_Novedades'] +\n",
    "    Auditoria_059_169['df_S3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92059d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros sin TPS_GNR_ID después del merge: 1\n"
     ]
    }
   ],
   "source": [
    "# Asegurar que todas las columnas clave sean de tipo string y estén limpias (sin espacios)\n",
    "for col in ['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    Auditoria_059_169[col] = Auditoria_059_169[col].astype(str).str.strip()\n",
    "\n",
    "for col in ['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    maestro_combinado[col] = maestro_combinado[col].astype(str).str.strip()\n",
    "\n",
    "# Realizar merge para traer TPS_GNR_ID desde maestro_combinado\n",
    "Auditoria_059_169_actualizado = Auditoria_059_169.merge(\n",
    "    maestro_combinado[['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'TPS_GNR_ID']],\n",
    "    how='left',\n",
    "    left_on=['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION'],\n",
    "    right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']\n",
    ")\n",
    "\n",
    "# Actualizar valores vacíos en Correc_TPS_GNR_ID con los traídos del merge\n",
    "Auditoria_059_169_actualizado['Correc_TPS_GNR_ID'] = Auditoria_059_169_actualizado['Correc_TPS_GNR_ID'].fillna(\n",
    "    Auditoria_059_169_actualizado['TPS_GNR_ID']\n",
    ")\n",
    "\n",
    "# Eliminar columnas auxiliares usadas solo para el merge si no se necesitan\n",
    "Auditoria_059_169_actualizado.drop(columns=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'TPS_GNR_ID'], inplace=True)\n",
    "\n",
    "# Asignar el DataFrame actualizado al original\n",
    "Auditoria_059_169 = Auditoria_059_169_actualizado\n",
    "\n",
    "# Diagnóstico opcional: mostrar cuántos registros aún tienen Correc_TPS_GNR_ID vacío\n",
    "faltantes = Auditoria_059_169[Auditoria_059_169['Correc_TPS_GNR_ID'].isna()]\n",
    "print(f\"Registros sin TPS_GNR_ID después del merge: {len(faltantes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdc9ad",
   "metadata": {},
   "source": [
    "## TIPO_GLOSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "928359f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tipo_glosa(glosa):\n",
    "    tiene_59 = 'GN0059' in glosa\n",
    "    tiene_169 = 'GN0169' in glosa\n",
    "    if tiene_59 and tiene_169:\n",
    "        return 3\n",
    "    elif tiene_59:\n",
    "        return 1\n",
    "    elif tiene_169:\n",
    "        return 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "Auditoria_059_169[\"TIPO_GLOSA\"] = Auditoria_059_169[\"GLOSA\"].apply(tipo_glosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a5e43",
   "metadata": {},
   "source": [
    "## Columnas de ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9091098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total\n",
      "0     28\n",
      "8      7\n",
      "4      3\n",
      "5      2\n",
      "7      2\n",
      "3      1\n",
      "10     1\n",
      "2      1\n",
      "1      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Auditoria_059_169[\"Total\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9263fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     GN0169(PRIMER APELLIDO|OLIVARES|OLIVARES MANCO...\n",
       "1            GN0169(PRIMER APELLIDO|PUERTA|PUERTAS||||)\n",
       "2     GN0169(|SEGUNDO APELLIDO|CASTELLANOS|CASTELLAN...\n",
       "3     GN0169(|SEGUNDO APELLIDO|LEGUIZAMON|LEGIZAMON|||)\n",
       "4                GN0169(||PRIMER NOMBRE|RONALD|RONAL||)\n",
       "5                                  GN0169(||||SEXO|F|M)\n",
       "6                                  GN0169(||||SEXO|F|M)\n",
       "7                                  GN0169(||||SEXO|F|M)\n",
       "8                                  GN0169(||||SEXO|M|F)\n",
       "9                GN0169(||PRIMER NOMBRE|ARIANA|ARANA||)\n",
       "10                                 GN0169(||||SEXO|F|M)\n",
       "11    GN0169(PRIMER APELLIDO|HINOJOSA|DELGADO|SEGUND...\n",
       "12                                 GN0169(||||SEXO|F|M)\n",
       "13                                 GN0169(||||SEXO|F|M)\n",
       "14          GN0169(|SEGUNDO APELLIDO|ANGULO|AGUDELO|||)\n",
       "15                GN0169(||PRIMER NOMBRE|KARIM|KARIN||)\n",
       "16                                       GN0169(||||||)\n",
       "17                                       GN0169(||||||)\n",
       "18                   GN0169(|||SEGUNDO NOMBRE||ISABEL|)\n",
       "19                                       GN0169(||||||)\n",
       "20        GN0169(||PRIMER NOMBRE|ALEXANDRA|ALEJANDRA||)\n",
       "21      GN0169(PRIMER APELLIDO|VELASQUEZ|VELAZQUEZ||||)\n",
       "22                                               GN0059\n",
       "23                                       GN0169(||||||)\n",
       "24    GN0169(||||FECHA NACIMIENTO|08/12/2005|01/02/2...\n",
       "25                                               GN0059\n",
       "26          GN0059;GN0169(|||SEGUNDO NOMBRE||ANGELINA|)\n",
       "27              GN0059;GN0169(|||SEGUNDO NOMBRE||JOSE|)\n",
       "28    GN0059;GN0169(|||SEGUNDO NOMBRE|KATERINE|KATHE...\n",
       "29    GN0059;GN0169(||PRIMER NOMBRE|BRAULIO|LUIS|SEG...\n",
       "30    GN0059;GN0169(||PRIMER NOMBRE|YEDIXSN|YEDIXSÒN||)\n",
       "31    GN0059;GN0169(PRIMER APELLIDO|GOMEZ|OROZCO|SEG...\n",
       "32     GN0059;GN0169(|SEGUNDO APELLIDO|CATAO|CATAÑO|||)\n",
       "33    GN0059;GN0169(|SEGUNDO APELLIDO|MONTAEZ|MONTAÑ...\n",
       "34    GN0059;GN0169(PRIMER APELLIDO|CASTELBLANCO|MEN...\n",
       "35    GN0059;GN0169(PRIMER APELLIDO|LOMBANA|PINTO|SE...\n",
       "36          GN0059;GN0169(PRIMER APELLIDO|NIO|NIÑO||||)\n",
       "37        GN0059;GN0169(PRIMER APELLIDO|NUEZ|NUÑEZ||||)\n",
       "38    GN0059;GN0169(PRIMER APELLIDO|PREGONERO|TORRES...\n",
       "39    GN0059;GN0169(PRIMER APELLIDO|CARREO|CARREÑO||||)\n",
       "40    GN0169(||||FECHA NACIMIENTO|25/03/2007|25/04/2...\n",
       "41    GN0169(||||FECHA NACIMIENTO|24/03/1968|24/03/1...\n",
       "42                                               GN0059\n",
       "43                                               GN0059\n",
       "44                                 GN0169(||||SEXO|F|M)\n",
       "45                                               GN0059\n",
       "Name: GLOSA, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auditoria_059_169.shape[0]\n",
    "Auditoria_059_169.columns\n",
    "Auditoria_059_169[\"GLOSA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fd95df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que las columnas clave sean de tipo string y estén limpias\n",
    "for col in ['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    Auditoria_059_169[col] = Auditoria_059_169[col].astype(str).str.strip()\n",
    "\n",
    "for col in ['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    maestro_combinado[col] = maestro_combinado[col].astype(str).str.strip()\n",
    "\n",
    "# Realizar el merge para traer las columnas adicionales desde maestro_combinado\n",
    "Auditoria_059_169_actualizado = Auditoria_059_169.merge(\n",
    "    maestro_combinado[[\n",
    "        'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO', \n",
    "        'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE', \n",
    "        'AFL_FECHA_NACIMIENTO', 'TPS_GNR_ID'\n",
    "    ]],\n",
    "    how='left',\n",
    "    left_on=['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION'],\n",
    "    right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']\n",
    ")\n",
    "\n",
    "# Eliminar columnas auxiliares usadas solo para el merge si no se necesitan\n",
    "Auditoria_059_169_actualizado.drop(columns=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], inplace=True)\n",
    "\n",
    "# Asignar el DataFrame actualizado al original\n",
    "Auditoria_059_169 = Auditoria_059_169_actualizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b4d8b",
   "metadata": {},
   "source": [
    "# Guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf8d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "ruta_carpeta = os.path.join(Carpeta, N_txt)\n",
    "os.makedirs(ruta_carpeta, exist_ok=True)\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "ruta_excel = os.path.join(Carpeta, f\"{N_txt}.xlsx\")\n",
    "Auditoria_059_169.to_excel(ruta_excel, index=False)\n",
    "\n",
    "# Guardar el DataFrame en un archivo .txt sin las últimas 5 columnas\n",
    "ruta_txt = os.path.join(Carpeta, f\"{N_txt}.txt\")\n",
    "Auditoria_059_169.iloc[:, :-5].to_csv(ruta_txt, sep=',', index=False, header=False, encoding='ansi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
