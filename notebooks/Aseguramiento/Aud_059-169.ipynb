{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3056243",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6350e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Añadir la carpeta raíz del proyecto a sys.path\n",
    "#sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "sys.path.append(os.path.abspath(\"D:\\Proyectos Python\\capresoca-data-automation\"))\n",
    "\n",
    "from datetime import datetime  \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from src.file_loader import cargar_maestros_ADRES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539faed",
   "metadata": {},
   "source": [
    "# Rutas y constantes\n",
    "*1. Oficce*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79141933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R_Novedades = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "#R_Ingresos = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "#R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "#R_MS_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0026052025.TXT\"\n",
    "#R_MS_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0026052025.TXT\"\n",
    "\n",
    "# Ruta de Salida\n",
    "\n",
    "#Carpeta = r\"\\\\Servernas\\AYC2\\ASEGURAMIENTO\\ASEGURAMIENTO\\PROCESO_ASEGURAMIENTO\\REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\_AUDITORIAS\\AUD GN0059-GN0169\\06_JUNIO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c24cfc",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e893a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"01/05/2025\"\n",
    "N_txt = \"GREPS0254062025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094d753",
   "metadata": {},
   "source": [
    "*2. Home*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Novedades = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\NS\\NS Negado\\all-NS-NEG.txt\"\n",
    "R_Ingresos = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\MS\\MS Negado\\All_MS_NEG.TXT\"\n",
    "R_S3 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\All-S3.txt\"\n",
    "R_MS_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0026052025.TXT\"\n",
    "R_MS_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0026052025.TXT\"\n",
    "\n",
    "# Ruta de Salida\n",
    "\n",
    "Carpeta = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\informes\\2025\\CTO135.2025 Informe  #6\\ACTIVIDAD 11\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d06f0",
   "metadata": {},
   "source": [
    "# Carga de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar los maestros\n",
    "maestro_combinado = cargar_maestros_ADRES(R_MS_EPS025, R_MS_EPSC25)\n",
    "# Cargar los archivos en dataframes\n",
    "df_S3 = pd.read_csv(R_S3, sep=',', header=0, dtype=str, encoding='ansi')\n",
    "df_Novedades = pd.read_csv(R_Novedades, sep=',', header=0, dtype=str, encoding='ansi')\n",
    "df_Ingresos = pd.read_csv(R_Ingresos, sep=',', header=0, dtype=str, encoding='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el nuevo dataframe con las columnas especificadas y valores vacíos\n",
    "columnas = [\"CONSECUTIVO\", \"TIPO_GLOSA\", \"Rev_TPS_IDN_ID\", \"Rev_HST_IDN_NUMERO_IDENTIFICACION\", \n",
    "            \"Rev_AFL_PRIMER_APELLIDO\", \"Rev_AFL_SEGUNDO_APELLIDO\", \"Rev_AFL_PRIMER_NOMBRE\", \n",
    "            \"Rev_AFL_SEGUNDO_NOMBRE\", \"Rev_AFL_FECHA_NACIMIENTO\", \"Rev_TPS_GNR_ID\", \n",
    "            \"Correc_TPS_IDN_ID\", \"Correc_HST_IDN_NUMERO_IDENTIFICACION\", \"Correc_AFL_PRIMER_APELLIDO\", \n",
    "            \"Correc_AFL_SEGUNDO_APELLIDO\", \"Correc_AFL_PRIMER_NOMBRE\", \"Correc_AFL_SEGUNDO_NOMBRE\", \n",
    "            \"Correc_AFL_FECHA_NACIMIENTO\", \"Correc_TPS_GNR_ID\", \"NOMBRE_ANEXO\"]\n",
    "\n",
    "Auditoria_059_169 = pd.DataFrame(columns=columnas, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69384a7",
   "metadata": {},
   "source": [
    "# Limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas a datetime\n",
    "df_S3['Fecha_Proceso'] = pd.to_datetime(df_S3['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "df_Ingresos['Fecha_Proceso'] = pd.to_datetime(df_Ingresos['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "df_Novedades['Fecha_Proceso'] = pd.to_datetime(df_Novedades['Fecha_Proceso'], format='%d/%m/%Y')\n",
    "fecha_limite = pd.to_datetime(Fecha, format='%d/%m/%Y')\n",
    "\n",
    "# Columnas clave y orden final\n",
    "columnas_base = [\n",
    "    'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
    "    'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
    "    'AFL_FECHA_NACIMIENTO', 'TPS_GNR_ID', 'GLOSA', 'Nombre_Archivo', 'Fecha_Proceso'\n",
    "]\n",
    "\n",
    "# Renombres\n",
    "renombres = {\n",
    "    'TPS_IDN_ID': \"Correc_TPS_IDN_ID\",\n",
    "    'HST_IDN_NUMERO_IDENTIFICACION': \"Correc_HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    'AFL_PRIMER_APELLIDO': \"Correc_AFL_PRIMER_APELLIDO\",\n",
    "    'AFL_SEGUNDO_APELLIDO': \"Correc_AFL_SEGUNDO_APELLIDO\",\n",
    "    'AFL_PRIMER_NOMBRE': \"Correc_AFL_PRIMER_NOMBRE\",\n",
    "    'AFL_SEGUNDO_NOMBRE': \"Correc_AFL_SEGUNDO_NOMBRE\",\n",
    "    'AFL_FECHA_NACIMIENTO': \"Correc_AFL_FECHA_NACIMIENTO\",\n",
    "    'TPS_GNR_ID': \"Correc_TPS_GNR_ID\"\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_S3\n",
    "# -----------------------\n",
    "filtro_S3 = (df_S3['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_S3['GLOSA'].str.contains('GN0059') | df_S3['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "df_S3_filtrado = df_S3.loc[filtro_S3, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_Ingresos\n",
    "# -----------------------\n",
    "filtro_Ingresos = (df_Ingresos['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_Ingresos['GLOSA'].str.contains('GN0059') | df_Ingresos['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "df_Ingresos_filtrado = df_Ingresos.loc[filtro_Ingresos, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Procesar df_Novedades (sin TPS_GNR_ID)\n",
    "# -----------------------\n",
    "filtro_Novedades = (df_Novedades['Fecha_Proceso'] >= fecha_limite) & (\n",
    "    df_Novedades['GLOSA'].str.contains('GN0059') | df_Novedades['GLOSA'].str.contains('GN0169')\n",
    ")\n",
    "\n",
    "# Añadir columna vacía TPS_GNR_ID\n",
    "df_Novedades_tmp = df_Novedades.copy()\n",
    "df_Novedades_tmp['TPS_GNR_ID'] = np.nan\n",
    "\n",
    "df_Novedades_filtrado = df_Novedades_tmp.loc[filtro_Novedades, columnas_base].rename(columns=renombres)\n",
    "\n",
    "# -----------------------\n",
    "# Concatenar todo\n",
    "# -----------------------\n",
    "Auditoria_059_169 = pd.concat([\n",
    "    Auditoria_059_169,\n",
    "    df_S3_filtrado,\n",
    "    df_Ingresos_filtrado,\n",
    "    df_Novedades_filtrado\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52be60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo las glosas GN0059 y GN0169 completas, con o sin contenido entre paréntesis\n",
    "Auditoria_059_169['GLOSA'] = Auditoria_059_169['GLOSA'].apply(\n",
    "    lambda x: ';'.join(re.findall(r'GN0059(?:\\([^)]*\\))?|GN0169(?:\\([^)]*\\))?', x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75382e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Auditoria_059_169.shape[0])\n",
    "Auditoria_059_169 = Auditoria_059_169.drop_duplicates(subset=[\"Correc_TPS_IDN_ID\", \"Correc_HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "print(Auditoria_059_169.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la variable Fecha a un rango de 3 meses atrás\n",
    "fecha_inicio = pd.to_datetime(Fecha, format='%d/%m/%Y') - pd.DateOffset(months=3)\n",
    "fecha_fin = pd.to_datetime(Fecha, format='%d/%m/%Y')\n",
    "\n",
    "# Filtrar los dataframes por las glosas \"GN0059\" y \"GN0169\" y el rango de fechas\n",
    "glosas_validas = ['GN0059', 'GN0169']\n",
    "\n",
    "# Filtrar df_Ingresos\n",
    "df_Ingresos_filtrado = df_Ingresos[\n",
    "    (df_Ingresos['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_Ingresos['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_Ingresos['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Filtrar df_Novedades\n",
    "df_Novedades_filtrado = df_Novedades[\n",
    "    (df_Novedades['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_Novedades['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_Novedades['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Filtrar df_S3\n",
    "df_S3_filtrado = df_S3[\n",
    "    (df_S3['Fecha_Proceso'] >= fecha_inicio) &\n",
    "    (df_S3['Fecha_Proceso'] <= fecha_fin) &\n",
    "    (df_S3['GLOSA'].str.contains('|'.join(glosas_validas)))\n",
    "]\n",
    "\n",
    "# Función para contar ocurrencias en un dataframe\n",
    "def contar_ocurrencias(df, col_id, col_hst, auditoria):\n",
    "    return auditoria.apply(\n",
    "        lambda row: df[\n",
    "            (df[col_id] == row['Correc_TPS_IDN_ID']) &\n",
    "            (df[col_hst] == row['Correc_HST_IDN_NUMERO_IDENTIFICACION'])\n",
    "        ].shape[0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Contar ocurrencias en cada dataframe\n",
    "Auditoria_059_169['df_Ingresos'] = contar_ocurrencias(\n",
    "    df_Ingresos_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "Auditoria_059_169['df_Novedades'] = contar_ocurrencias(\n",
    "    df_Novedades_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "Auditoria_059_169['df_S3'] = contar_ocurrencias(\n",
    "    df_S3_filtrado, 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', Auditoria_059_169\n",
    ")\n",
    "\n",
    "# Calcular el total de ocurrencias\n",
    "Auditoria_059_169['Total'] = (\n",
    "    Auditoria_059_169['df_Ingresos'] +\n",
    "    Auditoria_059_169['df_Novedades'] +\n",
    "    Auditoria_059_169['df_S3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92059d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que todas las columnas clave sean de tipo string y estén limpias (sin espacios)\n",
    "for col in ['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    Auditoria_059_169[col] = Auditoria_059_169[col].astype(str).str.strip()\n",
    "\n",
    "for col in ['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']:\n",
    "    maestro_combinado[col] = maestro_combinado[col].astype(str).str.strip()\n",
    "\n",
    "# Realizar merge para traer TPS_GNR_ID desde maestro_combinado\n",
    "Auditoria_059_169_actualizado = Auditoria_059_169.merge(\n",
    "    maestro_combinado[['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'TPS_GNR_ID']],\n",
    "    how='left',\n",
    "    left_on=['Correc_TPS_IDN_ID', 'Correc_HST_IDN_NUMERO_IDENTIFICACION'],\n",
    "    right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']\n",
    ")\n",
    "\n",
    "# Actualizar valores vacíos en Correc_TPS_GNR_ID con los traídos del merge\n",
    "Auditoria_059_169_actualizado['Correc_TPS_GNR_ID'] = Auditoria_059_169_actualizado['Correc_TPS_GNR_ID'].fillna(\n",
    "    Auditoria_059_169_actualizado['TPS_GNR_ID']\n",
    ")\n",
    "\n",
    "# Eliminar columnas auxiliares usadas solo para el merge si no se necesitan\n",
    "Auditoria_059_169_actualizado.drop(columns=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'TPS_GNR_ID'], inplace=True)\n",
    "\n",
    "# Asignar el DataFrame actualizado al original\n",
    "Auditoria_059_169 = Auditoria_059_169_actualizado\n",
    "\n",
    "# Diagnóstico opcional: mostrar cuántos registros aún tienen Correc_TPS_GNR_ID vacío\n",
    "faltantes = Auditoria_059_169[Auditoria_059_169['Correc_TPS_GNR_ID'].isna()]\n",
    "print(f\"Registros sin TPS_GNR_ID después del merge: {len(faltantes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdc9ad",
   "metadata": {},
   "source": [
    "## TIPO_GLOSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928359f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tipo_glosa(glosa):\n",
    "    tiene_59 = 'GN0059' in glosa\n",
    "    tiene_169 = 'GN0169' in glosa\n",
    "    if tiene_59 and tiene_169:\n",
    "        return 3\n",
    "    elif tiene_59:\n",
    "        return 1\n",
    "    elif tiene_169:\n",
    "        return 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "Auditoria_059_169[\"TIPO_GLOSA\"] = Auditoria_059_169[\"GLOSA\"].apply(tipo_glosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9091098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Auditoria_059_169[\"Total\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9263fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auditoria_059_169.shape[0]\n",
    "Auditoria_059_169.columns\n",
    "Auditoria_059_169[\"GLOSA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b4d8b",
   "metadata": {},
   "source": [
    "# Guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "ruta_carpeta = os.path.join(Carpeta, N_txt)\n",
    "os.makedirs(ruta_carpeta, exist_ok=True)\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "ruta_excel = os.path.join(Carpeta, f\"{N_txt}.xlsx\")\n",
    "Auditoria_059_169.to_excel(ruta_excel, index=False)\n",
    "\n",
    "# Guardar el DataFrame en un archivo .txt sin las últimas 5 columnas\n",
    "ruta_txt = os.path.join(Carpeta, f\"{N_txt}.txt\")\n",
    "Auditoria_059_169.iloc[:, :-5].to_csv(ruta_txt, sep=',', index=False, header=False, encoding='ansi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
