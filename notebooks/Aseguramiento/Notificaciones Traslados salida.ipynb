{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Automatizaci√≥n de Notificaciones de Traslados BDUA\n",
    "\n",
    "## Descripci√≥n General\n",
    "Este notebook automatiza el proceso completo de **notificaci√≥n masiva** relacionado con traslados de salida (aprobados y negados) en procesos BDUA. Abarca desde la generaci√≥n de documentos hasta el **env√≠o automatizado de correos electr√≥nicos masivos**, gestionando comunicaciones multicanal desde la EPS de origen hacia los usuarios finales.\n",
    "\n",
    "## Objetivos\n",
    "- **Enviar correos electr√≥nicos masivos** de forma automatizada con notificaciones personalizadas por usuario\n",
    "- Extraer y gestionar **n√∫meros de tel√©fono** para notificaciones v√≠a SMS\n",
    "- Crear **reportes en PDF** individualizados adjuntos a cada notificaci√≥n\n",
    "- Consolidar informaci√≥n en **archivo Excel** para seguimiento y trazabilidad\n",
    "\n",
    "## Insumos de Entrada\n",
    "| Insumo | Descripci√≥n |\n",
    "|--------|-------------|\n",
    "| Base de traslados BDUA | Registros de traslados aprobados y negados |\n",
    "| Informaci√≥n de usuarios | Correos electr√≥nicos, n√∫meros de tel√©fono |\n",
    "| Datos EPS de origen | Informaci√≥n complementaria de las EPS |\n",
    "\n",
    "## Productos de Salida\n",
    "1. **Correos masivos enviados**: Notificaciones autom√°ticas con PDF adjunto por cada usuario\n",
    "2. **PDF por usuario**: Documentos de notificaci√≥n personalizados con detalle del traslado\n",
    "3. **Excel consolidado**: Tabla de seguimiento con:\n",
    "   - Informaci√≥n del usuario\n",
    "   - N√∫meros de tel√©fono y correos electr√≥nicos\n",
    "   - Estado del traslado\n",
    "   - Registro de notificaciones enviadas\n",
    "\n",
    "## Estructura del Notebook\n",
    "1. Carga y validaci√≥n de datos fuente\n",
    "2. Procesamiento de informaci√≥n de usuarios\n",
    "3. Generaci√≥n de PDF de notificaciones\n",
    "4. Extracci√≥n de contactos (email y tel√©fono)\n",
    "5. **Env√≠o automatizado de correos masivos** con PDF adjunto\n",
    "6. Consolidaci√≥n de resultados en archivo Excel\n",
    "7. Reportes de ejecuci√≥n y trazabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Mudolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√≥dulos necesarios\n",
    "import os  # Trabajar con rutas del sistema\n",
    "import pandas as pd  # Trabajar con DataFrames\n",
    "import datetime  # Manejo de fechas\n",
    "from pathlib import Path  # Manejo de rutas\n",
    "import smtplib  # Env√≠o de correos\n",
    "from email.mime.text import MIMEText  # Crear correos con texto\n",
    "from email.mime.multipart import MIMEMultipart  # Correos con m√∫ltiples partes\n",
    "from PyPDF2 import PdfReader  # Leer archivos PDF\n",
    "import re  # Para validaci√≥n de correos electr√≥nicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Rutas y Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "Periodo_notificaci√≥n = \"01/01/2026\"\n",
    "Fecha_Correo = \"01/01/2026\"\n",
    "Informe = \"Informe #03\"\n",
    "\n",
    "# Base de datos Procesos BDUA traslados de entrada aprobados y negados\n",
    "R_s1_automatico = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\Automatico-S1\\All-AUTO-S1.txt\"\n",
    "R_s1_val = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S1.val\\All-S1-VAL.txt\"\n",
    "R_s5 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S5\\S5 TXT\\S5_consolidado.txt\"\n",
    "\n",
    "# Maestro SIE Correos y Telefonos\n",
    "R_maestro_sie = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validaci√≥n Archivos Maestro_2026_02_21.csv\"\n",
    "\n",
    "# Ruta Salida\n",
    "R_salida = fr\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\informes\\2026\\CTO 102.2026\\CTO102.2026 {Informe}\\12 Actividad\\Bases de datos notificaciones telefonicas\\TRASLADOS APROBADOS Y NEGADOS BDUA\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Cargue de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s1_automatico = pd.read_csv(R_s1_automatico, sep=\",\", encoding=\"latin-1\", dtype=str)\n",
    "df_s1_val = pd.read_csv(R_s1_val, sep=\",\", encoding=\"latin-1\", dtype=str)\n",
    "df_s5 = pd.read_csv(R_s5, sep=\",\", encoding=\"latin-1\", dtype=str)\n",
    "\n",
    "df_ms_sie = pd.read_csv(R_maestro_sie, sep=';', encoding='ANSI', header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Procesos BDUA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Fecha a reportar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer mes y a√±o del periodo de notificaci√≥n\n",
    "mes_filtro = int(Periodo_notificaci√≥n.split(\"/\")[1])  # Mes\n",
    "anio_filtro = int(Periodo_notificaci√≥n.split(\"/\")[2])  # A√±o\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"FILTRO APLICADO: Mes={mes_filtro:02d} | A√±o={anio_filtro}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Funci√≥n para filtrar por mes y a√±o y mostrar resumen\n",
    "def filtrar_por_periodo(df, nombre_df, col_fecha=\"FECHA_PROCESO\"):\n",
    "    \"\"\"Filtra el DataFrame por mes/a√±o y muestra resumen en consola.\"\"\"\n",
    "    registros_antes = len(df)\n",
    "    \n",
    "    # Convertir la columna de fecha a datetime para extraer mes y a√±o\n",
    "    fecha_parsed = pd.to_datetime(df[col_fecha], format=\"%d/%m/%Y\", dayfirst=True)\n",
    "    \n",
    "    # Filtrar por mes y a√±o\n",
    "    mask = (fecha_parsed.dt.month == mes_filtro) & (fecha_parsed.dt.year == anio_filtro)\n",
    "    df_filtrado = df[mask].copy()\n",
    "    \n",
    "    registros_despues = len(df_filtrado)\n",
    "    \n",
    "    # Reporte en consola\n",
    "    print(f\"üìã {nombre_df}\")\n",
    "    print(f\"   Registros antes: {registros_antes:,} ‚Üí Despu√©s: {registros_despues:,}\")\n",
    "    print(f\"   Distribuci√≥n por FECHA_PROCESO:\")\n",
    "    \n",
    "    conteo_fechas = df_filtrado[col_fecha].value_counts().sort_index()\n",
    "    for fecha, cantidad in conteo_fechas.items():\n",
    "        print(f\"     ‚Ä¢ {fecha}: {cantidad:,} registros\")\n",
    "    \n",
    "    print(f\"   Total fechas √∫nicas: {len(conteo_fechas)}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    return df_filtrado\n",
    "\n",
    "# Aplicar filtro a los 3 DataFrames\n",
    "df_s1_automatico = filtrar_por_periodo(df_s1_automatico, \"df_s1_automatico\")\n",
    "df_s1_val = filtrar_por_periodo(df_s1_val, \"df_s1_val\")\n",
    "df_s5 = filtrar_por_periodo(df_s5, \"df_s5\")\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESUMEN FINAL DEL FILTRO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  df_s1_automatico : {len(df_s1_automatico):>8,} registros\")\n",
    "print(f\"  df_s1_val        : {len(df_s1_val):>8,} registros\")\n",
    "print(f\"  df_s5            : {len(df_s5):>8,} registros\")\n",
    "print(f\"  {'‚îÄ'*40}\")\n",
    "print(f\"  TOTAL            : {len(df_s1_automatico) + len(df_s1_val) + len(df_s5):>8,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Filtrar Traslados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar traslados de EPS (eliminar movilidades)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"FILTRO: Traslados de EPS (eliminar movilidades)\")\n",
    "print(f\"  Excluir TIPO_TRASLADO: 3, 4, 5\")\n",
    "print(f\"  Excluir ENT_ID_ORIGEN: EPS025, EPSC25\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "tipos_excluir = [\"3\", \"4\", \"5\"]\n",
    "entidades_excluir = [\"EPS025\", \"EPSC25\"]\n",
    "\n",
    "def filtrar_traslados_eps(df, nombre_df):\n",
    "    \"\"\"Filtra movilidades y entidades propias, muestra resumen.\"\"\"\n",
    "    registros_antes = len(df)\n",
    "    \n",
    "    # Aplicar filtros: excluir tipos de traslado y entidades\n",
    "    mask = (\n",
    "        ~df[\"TIPO_TRASLADO\"].isin(tipos_excluir) &\n",
    "        ~df[\"ENT_ID_ORIGEN\"].isin(entidades_excluir)\n",
    "    )\n",
    "    df_filtrado = df[mask].copy()\n",
    "    \n",
    "    registros_despues = len(df_filtrado)\n",
    "    eliminados = registros_antes - registros_despues\n",
    "    \n",
    "    # Reporte en consola\n",
    "    print(f\"üìã {nombre_df}\")\n",
    "    print(f\"   Registros antes: {registros_antes:,} ‚Üí Despu√©s: {registros_despues:,} (eliminados: {eliminados:,})\")\n",
    "    \n",
    "    print(f\"\\n   Distribuci√≥n por TIPO_TRASLADO:\")\n",
    "    conteo_tipo = df_filtrado[\"TIPO_TRASLADO\"].value_counts().sort_index()\n",
    "    for tipo, cantidad in conteo_tipo.items():\n",
    "        print(f\"     ‚Ä¢ Tipo {tipo}: {cantidad:,} registros\")\n",
    "    \n",
    "    print(f\"\\n   Distribuci√≥n por ENT_ID_ORIGEN:\")\n",
    "    conteo_entidad = df_filtrado[\"ENT_ID_ORIGEN\"].value_counts().sort_index()\n",
    "    for entidad, cantidad in conteo_entidad.items():\n",
    "        print(f\"     ‚Ä¢ {entidad}: {cantidad:,} registros\")\n",
    "    \n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    return df_filtrado\n",
    "\n",
    "# Aplicar filtro a los 2 DataFrames\n",
    "df_s1_automatico = filtrar_traslados_eps(df_s1_automatico, \"df_s1_automatico\")\n",
    "df_s1_val = filtrar_traslados_eps(df_s1_val, \"df_s1_val\")\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESUMEN DESPU√âS DE FILTRAR TRASLADOS EPS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  df_s1_automatico : {len(df_s1_automatico):>8,} registros\")\n",
    "print(f\"  df_s1_val        : {len(df_s1_val):>8,} registros\")\n",
    "print(f\"  df_s5            : {len(df_s5):>8,} registros (sin cambios)\")\n",
    "print(f\"  {'‚îÄ'*40}\")\n",
    "print(f\"  TOTAL            : {len(df_s1_automatico) + len(df_s1_val) + len(df_s5):>8,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Unificar informaci√≥n en un solo datafarme S1.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONSOLIDACI√ìN DE TRASLADOS DE ENTRADA (Aprobados y Negados)\n",
    "# ============================================================\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"CONSOLIDACI√ìN DE TRASLADOS DE ENTRADA\")\n",
    "print(f\"  S1-Autom√°tico: Aprobados autom√°ticos por ADRES\")\n",
    "print(f\"  S5: Respuesta EPS origen (0=Negado, 1=Aprobado)\")\n",
    "print(f\"  S1-Val: Base maestra con informaci√≥n completa\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Columnas clave para el cruce\n",
    "cols_clave = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"FECHA_PROCESO\"]\n",
    "\n",
    "# ‚îÄ‚îÄ PASO 1: Marcar aprobados autom√°ticos desde S1-Autom√°tico ‚îÄ‚îÄ\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "print(f\"PASO 1: Identificar aprobados autom√°ticos (S1-Autom√°tico)\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "# Crear set de claves del S1-Autom√°tico para b√∫squeda eficiente\n",
    "claves_s1_auto = set(\n",
    "    df_s1_automatico[cols_clave].apply(lambda x: tuple(x), axis=1)\n",
    ")\n",
    "print(f\"  Registros √∫nicos en S1-Autom√°tico: {len(claves_s1_auto):,}\")\n",
    "\n",
    "# ‚îÄ‚îÄ PASO 2: Preparar S5 (respuesta EPS origen) ‚îÄ‚îÄ\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"PASO 2: Preparar respuestas del S5\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "# Crear DataFrame de respuestas S5 con las columnas clave + RESPUESTA\n",
    "df_s5_respuesta = df_s5[cols_clave + [\"RESPUESTA\"]].copy()\n",
    "df_s5_respuesta[\"RESPUESTA\"] = df_s5_respuesta[\"RESPUESTA\"].str.strip()\n",
    "\n",
    "conteo_s5 = df_s5_respuesta[\"RESPUESTA\"].value_counts()\n",
    "print(f\"  Total registros S5: {len(df_s5_respuesta):,}\")\n",
    "for resp, cant in conteo_s5.items():\n",
    "    etiqueta = \"Aprobado\" if resp == \"1\" else \"Negado\" if resp == \"0\" else f\"Desconocido ({resp})\"\n",
    "    print(f\"    ‚Ä¢ {etiqueta} (RESPUESTA={resp}): {cant:,}\")\n",
    "\n",
    "# Crear set de claves S5 para b√∫squeda\n",
    "claves_s5 = set(\n",
    "    df_s5_respuesta[cols_clave].apply(lambda x: tuple(x), axis=1)\n",
    ")\n",
    "print(f\"  Registros √∫nicos S5 por clave: {len(claves_s5):,}\")\n",
    "\n",
    "# ‚îÄ‚îÄ PASO 3: Cruzar S1-Val con S1-Autom√°tico y S5 ‚îÄ‚îÄ\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"PASO 3: Cruzar S1-Val con S1-Autom√°tico y S5\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "df_consolidado = df_s1_val.copy()\n",
    "registros_s1_val = len(df_consolidado)\n",
    "print(f\"  Registros en S1-Val: {registros_s1_val:,}\")\n",
    "\n",
    "# Crear tupla de claves en S1-Val\n",
    "df_consolidado[\"_clave\"] = list(\n",
    "    df_consolidado[cols_clave].apply(lambda x: tuple(x), axis=1)\n",
    ")\n",
    "\n",
    "# Marcar origen: S1-Autom√°tico o S5\n",
    "df_consolidado[\"ORIGEN_RESPUESTA\"] = df_consolidado[\"_clave\"].apply(\n",
    "    lambda x: \"S1-AUTOMATICO\" if x in claves_s1_auto \n",
    "              else (\"S5\" if x in claves_s5 else \"SIN_RESPUESTA\")\n",
    ")\n",
    "\n",
    "# Cruzar con S5 para obtener la respuesta (merge por claves)\n",
    "df_consolidado = df_consolidado.merge(\n",
    "    df_s5_respuesta[cols_clave + [\"RESPUESTA\"]],\n",
    "    on=cols_clave,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Asignar estado del traslado\n",
    "# S1-Autom√°tico ‚Üí siempre Aprobado\n",
    "# S5 ‚Üí seg√∫n columna RESPUESTA (1=Aprobado, 0=Negado)\n",
    "# Sin respuesta ‚Üí marcar como pendiente\n",
    "df_consolidado[\"ESTADO_TRASLADO\"] = df_consolidado.apply(\n",
    "    lambda row: \"APROBADO\" if row[\"ORIGEN_RESPUESTA\"] == \"S1-AUTOMATICO\"\n",
    "                else (\"APROBADO\" if row[\"RESPUESTA\"] == \"1\" \n",
    "                      else (\"NEGADO\" if row[\"RESPUESTA\"] == \"0\" \n",
    "                            else \"SIN_RESPUESTA\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Eliminar columna auxiliar\n",
    "df_consolidado.drop(columns=[\"_clave\"], inplace=True)\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(f\"\\n  Clasificaci√≥n por ORIGEN_RESPUESTA:\")\n",
    "conteo_origen = df_consolidado[\"ORIGEN_RESPUESTA\"].value_counts()\n",
    "for origen, cant in conteo_origen.items():\n",
    "    print(f\"    ‚Ä¢ {origen}: {cant:,}\")\n",
    "\n",
    "print(f\"\\n  Clasificaci√≥n por ESTADO_TRASLADO:\")\n",
    "conteo_estado = df_consolidado[\"ESTADO_TRASLADO\"].value_counts()\n",
    "for estado, cant in conteo_estado.items():\n",
    "    print(f\"    ‚Ä¢ {estado}: {cant:,}\")\n",
    "\n",
    "# ‚îÄ‚îÄ PASO 4: Deduplicaci√≥n priorizando aprobados ‚îÄ‚îÄ\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"PASO 4: Deduplicaci√≥n (priorizar aprobados sobre negados)\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "registros_antes_dedup = len(df_consolidado)\n",
    "\n",
    "# Columnas de identificaci√≥n del usuario (sin FECHA_PROCESO)\n",
    "cols_usuario = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# Crear prioridad: APROBADO > NEGADO > SIN_RESPUESTA\n",
    "prioridad_estado = {\"APROBADO\": 0, \"NEGADO\": 1, \"SIN_RESPUESTA\": 2}\n",
    "df_consolidado[\"_prioridad\"] = df_consolidado[\"ESTADO_TRASLADO\"].map(prioridad_estado)\n",
    "\n",
    "# Ordenar: por usuario, prioridad (aprobado primero), fecha proceso (m√°s reciente primero)\n",
    "df_consolidado[\"_fecha_orden\"] = pd.to_datetime(\n",
    "    df_consolidado[\"FECHA_PROCESO\"], format=\"%d/%m/%Y\", dayfirst=True\n",
    ")\n",
    "df_consolidado.sort_values(\n",
    "    by=cols_usuario + [\"_prioridad\", \"_fecha_orden\"],\n",
    "    ascending=[True, True, True, False],  # Prioridad ascendente (0=aprobado primero), fecha descendente\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Mantener el primer registro por usuario (el de mayor prioridad y fecha m√°s reciente)\n",
    "df_consolidado.drop_duplicates(subset=cols_usuario, keep=\"first\", inplace=True)\n",
    "\n",
    "# Eliminar columnas auxiliares\n",
    "df_consolidado.drop(columns=[\"_prioridad\", \"_fecha_orden\"], inplace=True)\n",
    "\n",
    "registros_despues_dedup = len(df_consolidado)\n",
    "duplicados_eliminados = registros_antes_dedup - registros_despues_dedup\n",
    "\n",
    "print(f\"  Registros antes: {registros_antes_dedup:,}\")\n",
    "print(f\"  Registros despu√©s: {registros_despues_dedup:,}\")\n",
    "print(f\"  Duplicados eliminados: {duplicados_eliminados:,}\")\n",
    "\n",
    "# ‚îÄ‚îÄ PASO 5: Validaci√≥n de coherencia temporal ‚îÄ‚îÄ\n",
    "print(f\"\\n{'‚îÄ'*70}\")\n",
    "print(f\"PASO 5: Validaci√≥n de coherencia temporal\")\n",
    "print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "print(f\"\\n  Distribuci√≥n por FECHA_PROCESO:\")\n",
    "conteo_fechas = df_consolidado[\"FECHA_PROCESO\"].value_counts().sort_index()\n",
    "for fecha, cant in conteo_fechas.items():\n",
    "    print(f\"    ‚Ä¢ {fecha}: {cant:,} registros\")\n",
    "\n",
    "print(f\"\\n  Cruce ESTADO_TRASLADO √ó ORIGEN_RESPUESTA:\")\n",
    "tabla_cruzada = pd.crosstab(\n",
    "    df_consolidado[\"ESTADO_TRASLADO\"], \n",
    "    df_consolidado[\"ORIGEN_RESPUESTA\"],\n",
    "    margins=True,\n",
    "    margins_name=\"TOTAL\"\n",
    ")\n",
    "print(tabla_cruzada.to_string(col_space=15))\n",
    "\n",
    "print(f\"\\n  Distribuci√≥n ESTADO √ó FECHA_PROCESO:\")\n",
    "tabla_fecha_estado = pd.crosstab(\n",
    "    df_consolidado[\"FECHA_PROCESO\"], \n",
    "    df_consolidado[\"ESTADO_TRASLADO\"],\n",
    "    margins=True,\n",
    "    margins_name=\"TOTAL\"\n",
    ")\n",
    "print(tabla_fecha_estado.to_string(col_space=12))\n",
    "\n",
    "# ‚îÄ‚îÄ RESUMEN FINAL ‚îÄ‚îÄ\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RESUMEN FINAL - CONSOLIDADO DE TRASLADOS DE ENTRADA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Total registros consolidados : {len(df_consolidado):>8,}\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ APROBADOS                : {len(df_consolidado[df_consolidado['ESTADO_TRASLADO'] == 'APROBADO']):>8,}\")\n",
    "print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ S1-Autom√°tico        : {len(df_consolidado[(df_consolidado['ESTADO_TRASLADO'] == 'APROBADO') & (df_consolidado['ORIGEN_RESPUESTA'] == 'S1-AUTOMATICO')]):>8,}\")\n",
    "print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ S5                   : {len(df_consolidado[(df_consolidado['ESTADO_TRASLADO'] == 'APROBADO') & (df_consolidado['ORIGEN_RESPUESTA'] == 'S5')]):>8,}\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ NEGADOS                  : {len(df_consolidado[df_consolidado['ESTADO_TRASLADO'] == 'NEGADO']):>8,}\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ SIN RESPUESTA            : {len(df_consolidado[df_consolidado['ESTADO_TRASLADO'] == 'SIN_RESPUESTA']):>8,}\")\n",
    "print(f\"  {'‚îÄ'*50}\")\n",
    "print(f\"  Usuarios √∫nicos              : {df_consolidado[cols_usuario].drop_duplicates().shape[0]:>8,}\")\n",
    "print(f\"  Fechas de proceso            : {df_consolidado['FECHA_PROCESO'].nunique():>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Eliminar dataframes, liberar espacio en RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_s1_automatico, df_s1_val, df_s5  # Liberar memoria de DataFrames originales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Depurar df_consolidado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Eliminar Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias del consolidado\n",
    "columnas_a_eliminar = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\",\n",
    "    \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"TPS_MDL_SBS_ID\"\n",
    "]\n",
    "\n",
    "df_consolidado.drop(columns=columnas_a_eliminar, inplace=True, errors=\"ignore\")\n",
    "print(\"Columnas innecesarias eliminadas del df_consolidado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Correos y telefono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def cruzar_maestro_sie(df_consolidado, df_ms_sie):\n",
    "    \"\"\"\n",
    "    Realiza el cruce entre el consolidado y el maestro SIE.\n",
    "    Aplica buenas pr√°cticas de limpieza de llaves y auditor√≠a de datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Estandarizaci√≥n de llaves (Prevenci√≥n de fallos por espacios o tipos de datos)\n",
    "    keys_consolidado = ['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']\n",
    "    keys_sie = ['tipo_documento', 'numero_identificacion']\n",
    "    \n",
    "    for df, keys in [(df_consolidado, keys_consolidado), (df_ms_sie, keys_sie)]:\n",
    "        for key in keys:\n",
    "            df[key] = df[key].astype(str).str.strip()\n",
    "\n",
    "    # 2. Selecci√≥n de columnas necesarias del maestro para optimizar memoria\n",
    "    cols_interes = keys_sie + ['celular', 'telefono_1', 'telefono_2', 'correo_electronico']\n",
    "    df_ms_sie_subset = df_ms_sie[cols_interes].drop_duplicates(subset=keys_sie)\n",
    "\n",
    "    # 3. Proceso de Cruce (Left Join)\n",
    "    # Usamos left join para no perder registros del df_consolidado original\n",
    "    df_resultado = pd.merge(\n",
    "        df_consolidado,\n",
    "        df_ms_sie_subset,\n",
    "        left_on=keys_consolidado,\n",
    "        right_on=keys_sie,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 4. Auditor√≠a de Calidad\n",
    "    print(\"=== REPORTE DE CALIDAD DEL CRUCE ===\")\n",
    "    \n",
    "    # Cantidad de registros que NO cruzaron (donde las columnas nuevas quedaron NaN)\n",
    "    # Tomamos 'celular' como referencia, pero lo ideal es validar contra la llave del SIE\n",
    "    no_cruzaron = df_resultado['tipo_documento'].isna().sum()\n",
    "    total_registros = len(df_resultado)\n",
    "    \n",
    "    print(f\"Total registros en consolidado: {total_registros}\")\n",
    "    print(f\"Registros que NO se encontraron en SIE: {no_cruzaron} ({(no_cruzaron/total_registros)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n--- Conteo de datos recuperados (No nulos) ---\")\n",
    "    columnas_nuevas = ['celular', 'telefono_1', 'telefono_2', 'correo_electronico']\n",
    "    for col in columnas_nuevas:\n",
    "        conteo = df_resultado[col].notna().sum()\n",
    "        print(f\"Columna '{col}': {conteo} registros con informaci√≥n.\")\n",
    "\n",
    "    # 5. Limpieza post-cruce (opcional: eliminar llaves duplicadas del maestro)\n",
    "    df_resultado = df_resultado.drop(columns=keys_sie)\n",
    "    \n",
    "    return df_resultado, no_cruzaron\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df_consolidado, fallos = cruzar_maestro_sie(df_consolidado, df_ms_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Eliminar df_ms_sie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ms_sie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Depurar telefonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def limpiar_y_validar_telefonos(df, columnas):\n",
    "    \"\"\"\n",
    "    Limpia y valida celulares colombianos en el DataFrame consolidado.\n",
    "    Distingue entre datos que ya ven√≠an vac√≠os y datos que fueron eliminados por mala calidad.\n",
    "    \"\"\"\n",
    "    reporte = {}\n",
    "\n",
    "    def es_valido(numero):\n",
    "        # Si qued√≥ vac√≠o tras la limpieza o era nulo, no es v√°lido\n",
    "        if not numero or numero == 'nan':\n",
    "            return False\n",
    "        # 1. Longitud 10, inicia con 3 y tiene al menos 4 d√≠gitos distintos (Entrop√≠a)\n",
    "        return (len(numero) == 10 and \n",
    "                numero.startswith('3') and \n",
    "                len(set(numero)) >= 4)\n",
    "\n",
    "    for col in columnas:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # 1. Contamos cu√°ntos datos REALES (no nulos) hay antes de empezar\n",
    "        # Esto evita contar los NaN del merge fallido como \"datos iniciales\"\n",
    "        datos_reales_iniciales = df[col].dropna().count()\n",
    "        \n",
    "        # 2. Limpieza de caracteres\n",
    "        # Usamos fillna('') antes de convertir a str para evitar el texto \"nan\"\n",
    "        df[col] = df[col].fillna('').astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        \n",
    "        # 3. Aplicamos validaci√≥n de estructura y calidad\n",
    "        mask_validos = df[col].apply(es_valido)\n",
    "        \n",
    "        # 4. C√°lculo de m√©tricas para Lumethik\n",
    "        validados = mask_validos.sum()\n",
    "        # Eliminados son los que ten√≠an algo pero no pasaron la regla de calidad\n",
    "        eliminados = datos_reales_iniciales - validados\n",
    "        \n",
    "        # 5. Limpieza final en el DataFrame: lo que no es v√°lido se vuelve NaN real\n",
    "        df.loc[~mask_validos, col] = None\n",
    "        \n",
    "        reporte[col] = {\n",
    "            'iniciales': datos_reales_iniciales,\n",
    "            'validados': validados,\n",
    "            'eliminados': eliminados,\n",
    "            'vacios_finales': df[col].isna().sum()\n",
    "        }\n",
    "\n",
    "    # --- Salida por Consola ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{'AUDITOR√çA DE CALIDAD TELEF√ìNICA':^80}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'COLUMNA':<20} | {'EXISTENTES':<12} | {'VALIDADOS':<12} | {'ELIMINADOS':<12} | {'NULOS FINAL'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for col, stats in reporte.items():\n",
    "        print(f\"{col:<20} | {stats['iniciales']:<12} | {stats['validados']:<12} | {stats['eliminados']:<12} | {stats['vacios_finales']}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicaci√≥n directa sobre tu consolidado\n",
    "columnas_telefonos = ['celular', 'telefono_1', 'telefono_2']\n",
    "df_consolidado = limpiar_y_validar_telefonos(df_consolidado, columnas_telefonos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Depurar Correos df_consolidado[correo_electronico]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalizar_y_validar_correos(df, columna='correo_electronico'):\n",
    "    \"\"\"\n",
    "    Normaliza correos: min√∫sculas, corrige errores de puntuaci√≥n y \n",
    "    dominios comunes mal escritos. Valida estructura final.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario de correcciones comunes (Heur√≠stica de Lumethik)\n",
    "    correcciones_dominios = {\n",
    "        r'@gamil\\.': '@gmail.',\n",
    "        r'@gamail\\.': '@gmail.',\n",
    "        r'@gimal\\.': '@gmail.',\n",
    "        r'@gimail\\.': '@gmail.',\n",
    "        r'@hotmial\\.': '@hotmail.',\n",
    "        r'@hotmal\\.': '@hotmail.',\n",
    "        r'@outlok\\.': '@outlook.',\n",
    "        r'@outluk\\.': '@outlook.',\n",
    "        r'@msn\\.con$': '@msn.com',\n",
    "        r'\\.con$': '.com',  # Error com√∫n de dedo\n",
    "        r',com$': '.com',   # Error de coma por punto\n",
    "    }\n",
    "\n",
    "    def validar_estructura(email):\n",
    "        if not email or email == 'nan':\n",
    "            return None\n",
    "        # Regex est√°ndar para email (RFC 5322 simplificada)\n",
    "        patron = r'^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$'\n",
    "        if re.match(patron, email):\n",
    "            return email\n",
    "        return None\n",
    "\n",
    "    # 1. Conteo inicial (datos que no son nulos)\n",
    "    total_inicial = df[columna].dropna().count()\n",
    "\n",
    "    # 2. Pre-procesamiento b√°sico: Min√∫sculas y quitar espacios\n",
    "    df[columna] = df[columna].fillna('').astype(str).str.lower().str.strip()\n",
    "\n",
    "    # 3. Limpieza de errores de puntuaci√≥n y dominios (Fuzzy Fix)\n",
    "    # Reemplazamos comas por puntos antes de las correcciones de dominio\n",
    "    df[columna] = df[columna].str.replace(',', '.', regex=False)\n",
    "    \n",
    "    for error, correccion in correcciones_dominios.items():\n",
    "        df[columna] = df[columna].str.replace(error, correccion, regex=True)\n",
    "\n",
    "    # 4. Validaci√≥n de estructura final\n",
    "    mask_validos = df[columna].apply(validar_estructura).notna()\n",
    "    \n",
    "    # Identificamos cu√°ntos se \"arreglaron\" vs cu√°ntos se eliminaron\n",
    "    validados = mask_validos.sum()\n",
    "    eliminados = total_inicial - validados\n",
    "\n",
    "    # 5. Aplicar limpieza final (lo no v√°lido a None)\n",
    "    df.loc[~mask_validos, columna] = None\n",
    "\n",
    "    # --- Reporte de Auditor√≠a ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{'REPORTE DE CALIDAD: CORREOS ELECTR√ìNICOS':^60}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Registros con correo inicialmente:   {total_inicial}\")\n",
    "    print(f\"Correos v√°lidos (y corregidos):      {validados}\")\n",
    "    print(f\"Correos eliminados (irreparables):   {eliminados}\")\n",
    "    print(f\"Efectividad de recuperaci√≥n:         {(validados/total_inicial)*100:.2f}%\" if total_inicial > 0 else \"N/A\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejecuci√≥n\n",
    "df_consolidado = normalizar_y_validar_correos(df_consolidado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
