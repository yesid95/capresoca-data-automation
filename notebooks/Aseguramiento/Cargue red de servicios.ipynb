{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys  # Acceso a variables y funciones del sistema\n",
    "import re  # Expresiones regulares para procesamiento de texto\n",
    "import os  # Operaciones del sistema de archivos\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "# Importar función y clase personalizada del proyecto\n",
    "from src.file_loader import cargar_maestros_ADRES  # Función para cargar archivos maestros ADRES\n",
    "from src.data_cleaning import BduaReportProcessor      # Clase para limpiar y normalizar población Maestro ADRES\n",
    "from src.data_cleaning import DataCleaner # Clase para limpiar y normalizar DataFrames de Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Raiz = r\"\\\\Servernas\\AYC2\\(Z)RSERVER(Z)\\LORENA CARDOZO\\RED DE SERVICIOS\"\n",
    "\n",
    "Hoja_Com = \"COMPENSADOS\"\n",
    "Hoja_LMA = \"BD\"\n",
    "R_Compensados= Raiz + r\"\\COMPENSADOS\\COMPENSADOS.xlsx\"\n",
    "r_LMA  = Raiz + r\"\\LMA\\LMA DICIEMBRE v1.xlsx\"\n",
    "r_MsEPS025 = Raiz + r\"\\MS\\EPS025MS0029012026.TXT\"\n",
    "r_MsEPSC25 = Raiz + r\"\\MS\\EPSC25MC0029012026.TXT\"\n",
    "\n",
    "Hoja_pnEPSC25 = \"CONTRIBUTIVO\"\n",
    "r_P_Nacional_EPSC25 = Raiz + r\"\\PORTABILIDAD\\PORTABILIDAD_NACIONAL_REGIMEN_CONTRIBUTIVO_DICIEMBRE25.xlsx\"\n",
    "Hoja_pnEPSS025 = \"SUBSIDIADO\"\n",
    "r_P_Nacional_EPS025 = Raiz + r\"\\PORTABILIDAD\\PORTABILIDAD_NACIONAL_REGIMEN_SUBSIDIADO_DICIEMBRE25.xlsx\"\n",
    "\n",
    "r_P_Regional_EPSC25 = Raiz + r\"\\PORTABILIDAD\\PORTABILIDAD_REGIONAL_REGIMEN_CONTRIBUTIVO_DICIEMBRE25.xlsx\"\n",
    "r_P_Regional_EPS025 = Raiz + r\"\\PORTABILIDAD\\PORTABILIDAD_REGIONAL_REGIMEN_SUBSIDIADO_DICIEMBRE25.xlsx\"\n",
    "\n",
    "r_Municipios_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Reporte_MUNICIPIOS_2025_05_14.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_municipios_sie = pd.read_csv(r_Municipios_SIE, encoding='ansi', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compensados = pd.read_excel(R_Compensados, sheet_name=Hoja_Com, header=0, dtype=str)\n",
    "df_lma = pd.read_excel(r_LMA, sheet_name=Hoja_LMA, header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar portabilidad nacional contributivo\n",
    "df_PN_contributivo = pd.read_excel(r_P_Nacional_EPSC25, sheet_name=Hoja_pnEPSC25, header=0, dtype=str)\n",
    "df_PN_contributivo['regimen_pn'] = 'CONTRIBUTIVO'\n",
    "\n",
    "# Cargar portabilidad nacional subsidiado\n",
    "df_PN_subsidiado = pd.read_excel(r_P_Nacional_EPS025, sheet_name=Hoja_pnEPSS025, header=0, dtype=str)\n",
    "df_PN_subsidiado['regimen_pn'] = 'SUBSIDIADO'\n",
    "\n",
    "# Unificar en un solo dataframe\n",
    "df_PN = pd.concat([df_PN_contributivo, df_PN_subsidiado], ignore_index=True)\n",
    "\n",
    "print(f\"Registros portabilidad contributivo: {len(df_PN_contributivo)}\")\n",
    "print(f\"Registros portabilidad subsidiado: {len(df_PN_subsidiado)}\")\n",
    "print(f\"Total portabilidad nacional: {len(df_PN)}\")\n",
    "print(f\"\\nPrimeros registros:\")\n",
    "print(df_PN.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar portabilidad nacional contributivo\n",
    "df_PR_contributivo = pd.read_excel(r_P_Regional_EPSC25, sheet_name=Hoja_pnEPSC25, header=0, dtype=str)\n",
    "df_PR_contributivo['regimen_pn'] = 'CONTRIBUTIVO'\n",
    "\n",
    "# Cargar portabilidad nacional subsidiado\n",
    "df_PR_subsidiado = pd.read_excel(r_P_Regional_EPS025, sheet_name=Hoja_pnEPSS025, header=0, dtype=str)\n",
    "df_PR_subsidiado['regimen_pn'] = 'SUBSIDIADO'\n",
    "\n",
    "# Unificar en un solo dataframe\n",
    "df_PR = pd.concat([df_PR_contributivo, df_PR_subsidiado], ignore_index=True)\n",
    "print(f\"Registros portabilidad contributivo: {len(df_PR_contributivo)}\")\n",
    "print(f\"Registros portabilidad subsidiado: {len(df_PR_subsidiado)}\")\n",
    "print(f\"Total portabilidad nacional: {len(df_PR)}\")\n",
    "print(f\"\\nPrimeros registros:\")\n",
    "print(df_PR.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maestro_ADRES = cargar_maestros_ADRES(r_MsEPS025, r_MsEPSC25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Depurar dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compensados = df_compensados.drop(columns=['TOTAL', 'Nuevo_Nombre2', 'Nuevo_Nombre1', 'Tipo_Afiliado', 'Estado_Registro', 'Serial_BDUA', 'Recursos_Fondo_PyP', 'Upc_Reconocer', 'Total_Dias_Cotizados', 'Total_Dias_Compensados', 'Periodo_compensado', 'Fecha_Proceso_Giro_Compensación', 'Código_EPS'])\n",
    "df_lma = df_lma[['Unnamed: 1', 'Unnamed: 2', 'Unnamed: 12', 'Unnamed: 13']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lma = df_lma.rename(columns={\n",
    "    'Unnamed: 1': 'Tp_Dc',\n",
    "    'Unnamed: 2': 'Num_coti',\n",
    "    'Unnamed: 12': 'Departamento',\n",
    "    'Unnamed: 13': 'Municipio'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compensados['compensados'] = 'COMPENSADOS'\n",
    "df_lma['compensados'] = 'LMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lma = df_lma.dropna(subset=['Tp_Dc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado = pd.concat([df_compensados[['Tp_Dc', 'Num_coti', 'Departamento', 'Municipio', 'compensados']], df_lma], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unificado = df_unificado.merge(\n",
    "    maestro_ADRES[['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'ENT_ID']],\n",
    "    left_on=['Tp_Dc', 'Num_coti'],\n",
    "    right_on=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'],\n",
    "    how='left'\n",
    ").drop(columns=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar duplicados en df_unificado\n",
    "duplicados = df_unificado[df_unificado.duplicated(subset=['Tp_Dc', 'Num_coti'], keep=False)]\n",
    "cantidad_duplicados = duplicados.duplicated(subset=['Tp_Dc', 'Num_coti']).sum()\n",
    "\n",
    "print(f\"Cantidad de registros duplicados: {cantidad_duplicados}\")\n",
    "print(f\"Total de filas con duplicados: {len(duplicados)}\")\n",
    "\n",
    "# Obtener los primeros 3 pares duplicados únicos\n",
    "pares_duplicados = duplicados[['Tp_Dc', 'Num_coti']].drop_duplicates().head(3)\n",
    "\n",
    "print(f\"\\nPrimeros 3 ejemplos de registros duplicados:\\n\")\n",
    "for idx, (tp_dc, num_coti) in enumerate(pares_duplicados.values, 1):\n",
    "    print(f\"--- Ejemplo {idx}: {tp_dc} - {num_coti} ---\")\n",
    "    registros = duplicados[(duplicados['Tp_Dc'] == tp_dc) & (duplicados['Num_coti'] == num_coti)]\n",
    "    print(registros)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Validar duplicados en df_PN por tipo_identificacion y numero_identificacion\n",
    "duplicados_PN = df_PN[df_PN.duplicated(subset=['tipo_identificacion', 'numero_identificacion'], keep=False)]\n",
    "\n",
    "cantidad_duplicados_PN = duplicados_PN.duplicated(subset=['tipo_identificacion', 'numero_identificacion']).sum()\n",
    "\n",
    "print(f\"Cantidad de registros duplicados: {cantidad_duplicados_PN}\")\n",
    "print(f\"Total de filas con duplicados: {len(duplicados_PN)}\")\n",
    "\n",
    "# Obtener los primeros 3 pares duplicados únicos\n",
    "pares_duplicados_PN = duplicados_PN[['tipo_identificacion', 'numero_identificacion']].drop_duplicates().head(3)\n",
    "\n",
    "print(f\"\\nPrimeros 3 ejemplos de registros duplicados:\\n\")\n",
    "for idx, (tipo_id, num_id) in enumerate(pares_duplicados_PN.values, 1):\n",
    "    print(f\"--- Ejemplo {idx}: {tipo_id} - {num_id} ---\")\n",
    "    registros = duplicados_PN[(duplicados_PN['tipo_identificacion'] == tipo_id) & (duplicados_PN['numero_identificacion'] == num_id)]\n",
    "    print(registros[['tipo_identificacion', 'numero_identificacion', 'primer_nombre', 'primer_apellido', 'regimen_pn', 'estado_movilidad']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la prioridad: 1 es mayor prioridad, 2 es menor\n",
    "# Contributivo (EPSC25) + COMPENSADOS: máxima prioridad\n",
    "# Subsidiado (EPS025) + LMA: segunda prioridad\n",
    "df_unificado['prioridad'] = 4  # default baja prioridad\n",
    "\n",
    "# Prioridad 1: Contributivo (EPSC25) con COMPENSADOS\n",
    "df_unificado.loc[\n",
    "    (df_unificado['ENT_ID'] == 'EPSC25') & \n",
    "    (df_unificado['compensados'] == 'COMPENSADOS'), \n",
    "    'prioridad'\n",
    "] = 1\n",
    "\n",
    "# Prioridad 2: Subsidiado (EPS025) con LMA\n",
    "df_unificado.loc[\n",
    "    (df_unificado['ENT_ID'] == 'EPS025') & \n",
    "    (df_unificado['compensados'] == 'LMA'), \n",
    "    'prioridad'\n",
    "] = 2\n",
    "\n",
    "# Prioridad 3: Otros casos\n",
    "df_unificado.loc[\n",
    "    (df_unificado['ENT_ID'].isin(['EPSC25', 'EPS025'])) & \n",
    "    (df_unificado['prioridad'] == 4), \n",
    "    'prioridad'\n",
    "] = 3\n",
    "\n",
    "# Ordenar por ID y prioridad, luego eliminar duplicados manteniendo el de mayor prioridad\n",
    "df_unificado = df_unificado.sort_values(\n",
    "    by=['Tp_Dc', 'Num_coti', 'prioridad']\n",
    ").drop_duplicates(\n",
    "    subset=['Tp_Dc', 'Num_coti'], \n",
    "    keep='first'\n",
    ").drop(columns=['prioridad'])\n",
    "\n",
    "# Validar resultados\n",
    "print(f\"Registros después de eliminar duplicados: {len(df_unificado)}\")\n",
    "\n",
    "# Verificar que no hay duplicados\n",
    "duplicados_validacion = df_unificado[df_unificado.duplicated(subset=['Tp_Dc', 'Num_coti'], keep=False)]\n",
    "print(f\"Duplicados restantes: {len(duplicados_validacion)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos de lo que quedó\n",
    "print(\"\\nEjemplos de registros mantenidos:\")\n",
    "print(df_unificado[['Tp_Dc', 'Num_coti', 'compensados', 'ENT_ID']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantizar formato de dos dígitos para Departamento y tres dígitos para Municipio\n",
    "df_unificado['Departamento'] = df_unificado['Departamento'].fillna('00').astype(str).str.zfill(2)\n",
    "df_unificado['Municipio'] = df_unificado['Municipio'].fillna('000').astype(str).str.zfill(3)\n",
    "\n",
    "print(\"Formatos aplicados:\")\n",
    "print(f\"Departamento - Ejemplos: {df_unificado['Departamento'].unique()[:5]}\")\n",
    "print(f\"Municipio - Ejemplos: {df_unificado['Municipio'].unique()[:5]}\")\n",
    "print(f\"\\nDepartamentos con formato incorrecto: {(df_unificado['Departamento'].str.len() != 2).sum()}\")\n",
    "print(f\"Municipios con formato incorrecto: {(df_unificado['Municipio'].str.len() != 3).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_unificado con df_PN usando tipo_identificacion y numero_identificacion\n",
    "df_unificado = df_unificado.merge(\n",
    "    df_PN[['tipo_identificacion', 'numero_identificacion', 'municipio_receptor', 'departamento_receptor']],\n",
    "    left_on=['Tp_Dc', 'Num_coti'],\n",
    "    right_on=['tipo_identificacion', 'numero_identificacion'],\n",
    "    how='left'\n",
    ").drop(columns=['tipo_identificacion', 'numero_identificacion'])\n",
    "\n",
    "# Renombrar columnas de PN\n",
    "df_unificado = df_unificado.rename(columns={\n",
    "    'municipio_receptor': 'municipio_receptor_PN',\n",
    "    'departamento_receptor': 'departamento_receptor_PN'\n",
    "})\n",
    "\n",
    "print(f\"Registros con Portabilidad Nacional (PN): {df_unificado['municipio_receptor_PN'].notna().sum()}\")\n",
    "\n",
    "# Merge df_unificado con df_PR usando tipo_identificacion y numero_identificacion\n",
    "df_PR_merge = df_PR[['tipo_identificacion', 'numero_identificacion', 'municipio_receptor', 'departamento_receptor']].copy()\n",
    "df_PR_merge.columns = ['tipo_identificacion', 'numero_identificacion', 'municipio_receptor_PR', 'departamento_receptor_PR']\n",
    "\n",
    "df_unificado = df_unificado.merge(\n",
    "    df_PR_merge,\n",
    "    left_on=['Tp_Dc', 'Num_coti'],\n",
    "    right_on=['tipo_identificacion', 'numero_identificacion'],\n",
    "    how='left'\n",
    ").drop(columns=['tipo_identificacion', 'numero_identificacion'])\n",
    "\n",
    "print(f\"Registros con Portabilidad Regional (PR): {df_unificado['municipio_receptor_PR'].notna().sum()}\")\n",
    "\n",
    "# Contar registros que tienen ambas (PN y PR)\n",
    "ambas = (df_unificado['municipio_receptor_PN'].notna()) & (df_unificado['municipio_receptor_PR'].notna())\n",
    "cantidad_ambas = ambas.sum()\n",
    "\n",
    "print(f\"Registros que tienen AMBAS (PN y PR): {cantidad_ambas}\")\n",
    "print(f\"  → Estos serán sobrescritos por PR\")\n",
    "\n",
    "# Crear columnas finales: PR sobrescriba a PN si existe\n",
    "# Primero usar PR, si no existe usar PN\n",
    "df_unificado['municipio_receptor'] = df_unificado['municipio_receptor_PR'].fillna(df_unificado['municipio_receptor_PN'])\n",
    "df_unificado['departamento_receptor'] = df_unificado['departamento_receptor_PR'].fillna(df_unificado['departamento_receptor_PN'])\n",
    "\n",
    "# Crear columna Tipo_Portabilidad basada en lo que quedó final\n",
    "df_unificado['Tipo_Portabilidad'] = 'Sin portabilidad'\n",
    "\n",
    "# Regional: si tiene PR (independientemente de si tiene PN)\n",
    "df_unificado.loc[df_unificado['municipio_receptor_PR'].notna(), 'Tipo_Portabilidad'] = 'Regional'\n",
    "\n",
    "# Nacional: solo si tiene PN y NO tiene PR\n",
    "df_unificado.loc[\n",
    "    (df_unificado['municipio_receptor_PN'].notna()) & (df_unificado['municipio_receptor_PR'].isna()), \n",
    "    'Tipo_Portabilidad'\n",
    "] = 'Nacional'\n",
    "\n",
    "# Mostrar detalles de validación\n",
    "print(f\"\\n=== VALIDACIÓN DE ASIGNACIÓN ===\")\n",
    "print(f\"\\nConteos de Tipo_Portabilidad:\")\n",
    "print(df_unificado['Tipo_Portabilidad'].value_counts())\n",
    "\n",
    "solo_PN = (df_unificado['municipio_receptor_PN'].notna()) & (df_unificado['municipio_receptor_PR'].isna())\n",
    "solo_PR = (df_unificado['municipio_receptor_PR'].notna()) & (df_unificado['municipio_receptor_PN'].isna())\n",
    "\n",
    "print(f\"\\nDesglose detallado:\")\n",
    "print(f\"  Solo PN (Nacional): {solo_PN.sum()}\")\n",
    "print(f\"  Solo PR (Regional): {solo_PR.sum()}\")\n",
    "print(f\"  PN y PR (Regional sobrescribe): {cantidad_ambas}\")\n",
    "print(f\"  Sin portabilidad: {(df_unificado['Tipo_Portabilidad'] == 'Sin portabilidad').sum()}\")\n",
    "\n",
    "# Eliminar columnas intermedias\n",
    "df_unificado = df_unificado.drop(columns=['municipio_receptor_PN', 'departamento_receptor_PN', 'municipio_receptor_PR', 'departamento_receptor_PR'])\n",
    "\n",
    "print(f\"\\n=== RESULTADO FINAL ===\")\n",
    "print(f\"Ejemplos de registros:\")\n",
    "print(df_unificado[['Tp_Dc', 'Num_coti', 'municipio_receptor', 'departamento_receptor', 'Tipo_Portabilidad']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_municipios_sie\n",
    "df_unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear diccionarios de búsqueda desde df_municipios_sie\n",
    "# Mapeo: (nombre_municipio, nombre_departamento) -> código_municipio\n",
    "municipios_lookup = {}\n",
    "for idx, row in df_municipios_sie.iterrows():\n",
    "    # Verificar que no sean valores nulos\n",
    "    if pd.notna(row['descripcion']) and pd.notna(row['descripcion_departamento']):\n",
    "        key = (str(row['descripcion']).strip(), str(row['descripcion_departamento']).strip())\n",
    "        municipios_lookup[key] = row['municipio']\n",
    "\n",
    "# Crear diccionario para nombres de municipio -> código\n",
    "municipio_nombre_codigo = {}\n",
    "for idx, row in df_municipios_sie.iterrows():\n",
    "    if pd.notna(row['descripcion']):\n",
    "        municipio_nombre_codigo[str(row['descripcion']).strip()] = row['municipio']\n",
    "\n",
    "print(f\"Municipios en lookup: {len(municipios_lookup)}\")\n",
    "print(f\"Municipios en nombre_codigo: {len(municipio_nombre_codigo)}\")\n",
    "\n",
    "# Función para asignar código de red\n",
    "def asignar_codigo_red(row):\n",
    "    # Si tiene municipio_receptor (tiene portabilidad)\n",
    "    if pd.notna(row['municipio_receptor']) and pd.notna(row['departamento_receptor']):\n",
    "        # Buscar el código en df_municipios_sie usando municipio_receptor y departamento_receptor\n",
    "        key = (str(row['municipio_receptor']).strip(), str(row['departamento_receptor']).strip())\n",
    "        codigo = municipios_lookup.get(key)\n",
    "        if codigo:\n",
    "            return codigo\n",
    "        # Si no encuentra, intentar solo con el municipio_receptor\n",
    "        return municipio_nombre_codigo.get(str(row['municipio_receptor']).strip())\n",
    "    # Si no tiene portabilidad, usar Departamento + Municipio\n",
    "    return row['Departamento'] + row['Municipio']\n",
    "\n",
    "# Aplicar la función para obtener código de red\n",
    "df_unificado['municipio_codigo_red'] = df_unificado.apply(asignar_codigo_red, axis=1)\n",
    "\n",
    "# Función para asignar nombre del municipio donde se asigna la red\n",
    "def asignar_municipio_nombre(row):\n",
    "    # Si tiene municipio_receptor, usar ese\n",
    "    if pd.notna(row['municipio_receptor']):\n",
    "        return str(row['municipio_receptor']).strip()\n",
    "    # Si no, buscar en el lookup usando el código combinado\n",
    "    codigo = row['Departamento'] + row['Municipio']\n",
    "    # Buscar en df_municipios_sie por código\n",
    "    resultado = df_municipios_sie[df_municipios_sie['municipio'] == codigo]\n",
    "    if not resultado.empty:\n",
    "        desc = resultado.iloc[0]['descripcion']\n",
    "        if pd.notna(desc):\n",
    "            return str(desc).strip()\n",
    "    return None\n",
    "\n",
    "# Aplicar función para obtener nombre del municipio\n",
    "df_unificado['municipio_nombre_red'] = df_unificado.apply(asignar_municipio_nombre, axis=1)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"\\nRegistros con municipio_código_red asignado: {df_unificado['municipio_codigo_red'].notna().sum()}\")\n",
    "print(f\"Registros con municipio_nombre_red asignado: {df_unificado['municipio_nombre_red'].notna().sum()}\")\n",
    "print(f\"Registros con ambos vacíos: {(df_unificado['municipio_codigo_red'].isna() & df_unificado['municipio_nombre_red'].isna()).sum()}\")\n",
    "\n",
    "print(f\"\\nEjemplos CON Portabilidad:\")\n",
    "con_portabilidad = df_unificado[df_unificado['Tipo_Portabilidad'] != 'Sin portabilidad'].head(5)\n",
    "print(con_portabilidad[['Departamento', 'Municipio', 'municipio_receptor', 'departamento_receptor', 'municipio_codigo_red', 'municipio_nombre_red', 'Tipo_Portabilidad']])\n",
    "\n",
    "print(f\"\\nEjemplos SIN Portabilidad:\")\n",
    "sin_portabilidad = df_unificado[df_unificado['Tipo_Portabilidad'] == 'Sin portabilidad'].head(5)\n",
    "print(sin_portabilidad[['Departamento', 'Municipio', 'municipio_receptor', 'departamento_receptor', 'municipio_codigo_red', 'municipio_nombre_red', 'Tipo_Portabilidad']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Guardar dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataframe de logs con información de los archivos procesados\n",
    "logs_data = {\n",
    "    'Archivo': [\n",
    "        'COMPENSADOS',\n",
    "        'LMA',\n",
    "        'Maestro ADRES EPS025',\n",
    "        'Maestro ADRES EPSC25',\n",
    "        'Portabilidad Nacional EPSC25',\n",
    "        'Portabilidad Nacional EPS025',\n",
    "        'Portabilidad Regional EPSC25',\n",
    "        'Portabilidad Regional EPS025'\n",
    "    ],\n",
    "    'Ruta': [\n",
    "        R_Compensados,\n",
    "        r_LMA,\n",
    "        r_MsEPS025,\n",
    "        r_MsEPSC25,\n",
    "        r_P_Nacional_EPSC25,\n",
    "        r_P_Nacional_EPS025,\n",
    "        r_P_Regional_EPSC25,\n",
    "        r_P_Regional_EPS025\n",
    "    ],\n",
    "    'Fecha_Proceso': [datetime.now()] * 8\n",
    "}\n",
    "\n",
    "df_logs = pd.DataFrame(logs_data)\n",
    "\n",
    "# Guardar df_unificado en Excel con múltiples hojas\n",
    "output_path = Raiz + r\"\\df_unificado_final.xlsx\"\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    df_unificado.to_excel(writer, index=False, sheet_name='Unificado')\n",
    "    df_logs.to_excel(writer, index=False, sheet_name='Logs')\n",
    "\n",
    "print(f\"Archivo guardado en: {output_path}\")\n",
    "print(f\"Total de registros guardados: {len(df_unificado)}\")\n",
    "print(f\"\\nArchivos procesados:\")\n",
    "print(df_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
