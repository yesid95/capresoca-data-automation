{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # para manipulación de dataframes y tablas\n",
    "from datetime import datetime, date, timedelta  # para trabajar con fechas y tiempos\n",
    "import os  # para manejar rutas del sistema y operaciones relacionadas\n",
    "from zipfile import ZipFile # para trabajar con archivos zip\n",
    "import shutil  # para copiar y mover archivos y directorios\n",
    "from pathlib import Path  # para trabajar de manera más cómoda con rutas en el sistema\n",
    "import numpy as np  # para trabajar con arreglos y matrices numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Ms_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validación Archivos Maestro_2025_12_18 (2).csv\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-02\\EPS025MS0016122025.TXT\"\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0016122025.TXT\"\n",
    "R_Expedientes_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\Expedientes\\Años\"\n",
    "R_Historico_Identificacion_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Historico\\2025\\20251203-HISTORICOS\\HISTORICO_IDENTIFICACION_S_E.TXT.ZIP\"\n",
    "R_Historico_Grupo_Familia_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Historico\\2025\\20251203-HISTORICOS\\HISTORIA_GRUPO_FAMILIAR_E.TXT.ZIP\"\n",
    "R_Historico_Identificacion_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Historico\\2025\\20251203-HISTORICOS\\HISTORICO_IDENTIFICACION_S_E.TXT.ZIP\"\n",
    "R_Historico_Grupo_Familia_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Historico\\2025\\20251203-HISTORICOS\\HISTORIA_GRUPO_FAMILIAR_E.TXT.ZIP\"\n",
    "R_Municipios_DANE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Constantes\\Departamentos.txt\"\n",
    "R_Municipios_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Reporte_MUNICIPIOS_2025_05_14.csv\"\n",
    "R_Parentesco_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Parentescos Codificados.txt\"\n",
    "R_Estado_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\ESTADO DE AFILIACION.txt\"\n",
    "R_Regimen_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\regimen codificado.txt\"\n",
    "R_Tipo_Afiliados_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Tipo de afiliado.txt\"\n",
    "\n",
    "R_Salida = r\"\\\\Servernas\\AYC2\\(01. ASEGURAMIENTO)\\01. ASEGURAMIENTO\\01. REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\ACTUALIZACION SIE\\01_Procesos BDUA validación SIE\\12_Diciembre\\YESID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Parentesco_SIE = pd.read_csv(R_Parentesco_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Estado_SIE = pd.read_csv(R_Estado_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Regimen_SIE = pd.read_csv(R_Regimen_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Tipo_Afiliados_SIE = pd.read_csv(R_Tipo_Afiliados_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "df_Municipios_SIE = pd.read_csv(R_Municipios_SIE, sep=';', dtype=str, encoding='ANSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que df_Municipios_SIE[\"municipio\"] tenga 5 dígitos\n",
    "df_Municipios_SIE[\"municipio\"] = df_Municipios_SIE[\"municipio\"].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todos los archivos .TXT en la carpeta R_Expedientes_SIE\n",
    "txt_files = list(Path(R_Expedientes_SIE).glob(\"*.TXT\"))\n",
    "\n",
    "# Cargar cada archivo en un dataframe y luego concatenarlos\n",
    "df_list = [pd.read_csv(file, sep=\"|\", encoding=\"ANSI\", dtype=str) for file in txt_files]\n",
    "DF_Expedientes_SIE = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    \"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"HST_IDN_FECHA_INICIO\", \"HST_IDN_FECHA_FIN\", \"ENT_ID\"\n",
    "]\n",
    "\n",
    "def load_txt_from_zip(zip_path: str,\n",
    "                      txt_name: str,\n",
    "                      columns: list[str],\n",
    "                      sep: str = \",\",\n",
    "                      encoding: str = \"ANSI\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Abre zip_path, extrae txt_name y lo carga en un DataFrame con las columnas dadas.\n",
    "    \"\"\"\n",
    "    with ZipFile(zip_path) as z:\n",
    "        with z.open(txt_name) as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=sep,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                encoding=encoding\n",
    "            )\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "# ahora, para cada archivo ZIP basta con:\n",
    "Df_H_I_EPS025 = load_txt_from_zip(\n",
    "    R_Historico_Identificacion_EPS025,\n",
    "    \"HISTORICO_IDENTIFICACION_S_E.TXT\",\n",
    "    new_columns\n",
    ")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 1. Definimos los posibles nombres que esperamos encontrar\n",
    "posibles_archivos = [\n",
    "    \"HISTORIA_IDENTIFICACION.TXT\", \n",
    "    \"HISTORICO_IDENTIFICACION_S_E.TXT\"\n",
    "]\n",
    "\n",
    "nombre_archivo_encontrado = None\n",
    "\n",
    "# 2. Inspeccionamos el ZIP sin descomprimirlo todo\n",
    "try:\n",
    "    with zipfile.ZipFile(R_Historico_Identificacion_EPSC25, 'r') as z:\n",
    "        archivos_en_zip = z.namelist()\n",
    "        \n",
    "        # Buscamos cuál de los candidatos existe en el ZIP\n",
    "        for candidato in posibles_archivos:\n",
    "            if candidato in archivos_en_zip:\n",
    "                nombre_archivo_encontrado = candidato\n",
    "                break # Rompemos el ciclo apenas encontramos uno válido\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: El archivo {R_Historico_Identificacion_EPSC25} no es un ZIP válido.\")\n",
    "\n",
    "# 3. Ejecutamos la carga solo si encontramos el archivo\n",
    "if nombre_archivo_encontrado:\n",
    "    Df_H_I_EPSC25 = load_txt_from_zip(\n",
    "        R_Historico_Identificacion_EPSC25,\n",
    "        nombre_archivo_encontrado,\n",
    "        new_columns\n",
    "    )\n",
    "    print(f\"Archivo cargado exitosamente: {nombre_archivo_encontrado}\")\n",
    "else:\n",
    "    # Manejo de error si no está ninguno de los dos\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontró ninguno de los archivos esperados en el ZIP: {posibles_archivos}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    \"GRP_FML_COTIZANTE_ID\", \"GRP_FML_AFILIADO_ID\", \"TPS_PRN_ID\",\n",
    "    \"GRP_FML_FECHA_INICIO\", \"GRP_FML_FECHA_FIN\", \"ENT_ID\"\n",
    "]\n",
    "\n",
    "def load_txt_from_zip(zip_path: str,\n",
    "                      txt_name: str,\n",
    "                      columns: list[str],\n",
    "                      sep: str = \",\",\n",
    "                      encoding: str = \"UTF-8\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Abre zip_path, extrae txt_name y lo carga en un DataFrame con las columnas dadas.\n",
    "    \"\"\"\n",
    "    with ZipFile(zip_path) as z:\n",
    "        with z.open(txt_name) as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=sep,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                encoding=encoding\n",
    "            )\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "# ahora, para cada archivo ZIP basta con:\n",
    "Df_H_GF_EPSC25 = load_txt_from_zip(\n",
    "    R_Historico_Grupo_Familia_EPSC25,\n",
    "    \"HISTORIA_GRUPO_FAMILIAR_E.TXT\",\n",
    "    new_columns\n",
    ")\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# Definimos los candidatos para este caso específico\n",
    "candidatos_gf = [\n",
    "    \"HISTORIA_GRUPO_FAMILIAR.TXT\", \n",
    "    \"HISTORIA_GRUPO_FAMILIAR_E.TXT\"\n",
    "]\n",
    "\n",
    "archivo_seleccionado = None\n",
    "\n",
    "# Verificamos cuál existe\n",
    "try:\n",
    "    with zipfile.ZipFile(R_Historico_Grupo_Familia_EPS025, 'r') as z:\n",
    "        nombres_en_zip = set(z.namelist()) # Usamos set para búsqueda O(1)\n",
    "        for cand in candidatos_gf:\n",
    "            if cand in nombres_en_zip:\n",
    "                archivo_seleccionado = cand\n",
    "                break\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error crítico: El archivo {R_Historico_Grupo_Familia_EPS025} está corrupto.\")\n",
    "\n",
    "if archivo_seleccionado:\n",
    "    Df_H_GF_EPS025 = load_txt_from_zip(\n",
    "        R_Historico_Grupo_Familia_EPS025,\n",
    "        archivo_seleccionado,\n",
    "        new_columns\n",
    "    )\n",
    "    print(f\"Cargado: {archivo_seleccionado}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No se encontró ninguno de los archivos: {candidatos_gf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = pd.read_csv(R_Ms_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Cod_DANE = pd.read_csv(R_Municipios_DANE, sep=';', dtype=str, encoding='utf-8')\n",
    "\n",
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO_2\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 1.1 Limpiar Bases de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 1.1.1 Expedientes SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_Expedientes_SIE.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])\n",
    "print( DF_Expedientes_SIE['Proceso'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros antes de limpiar expedientes:\", DF_Expedientes_SIE.shape[0])\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE[DF_Expedientes_SIE['Estado Expediente'] == 'Cerrado']\n",
    "print(\"Número de registros con 'Cerrado':\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])\n",
    "# 1) Convierte a datetime\n",
    "DF_Expedientes_SIE['Fecha Grabado'] = pd.to_datetime(\n",
    "    DF_Expedientes_SIE['Fecha Grabado'],\n",
    "    format='%Y/%m/%d %H:%M',  # ajusta si tu formato es distinto\n",
    "    errors='coerce'\n",
    ")\n",
    "DF_Expedientes_SIE['Fecha Cierre'] = pd.to_datetime(\n",
    "    DF_Expedientes_SIE['Fecha Cierre'],\n",
    "    format='%Y/%m/%d %H:%M',  # ajusta si tu formato es distinto\n",
    "    errors='coerce'\n",
    ")\n",
    "# 2) Ordena de más antiguo a más reciente\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE.sort_values('Fecha Grabado')\n",
    "\n",
    "# 3) Elimina duplicados, quedándote con el último (el más reciente)\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE.drop_duplicates(\n",
    "    subset=['Proceso', 'Tipo Documento', 'Número Identificación'],\n",
    "    keep='last'\n",
    ")\n",
    "# 4) Re-formatea la fecha a DD/MM/YYYY\n",
    "DF_Expedientes_SIE['Fecha Grabado'] = DF_Expedientes_SIE['Fecha Grabado'].dt.strftime('%d/%m/%Y')\n",
    "DF_Expedientes_SIE['Fecha Cierre'] = DF_Expedientes_SIE['Fecha Cierre'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 1.1.2 Maestro ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### 1.1.3 Historicos Identificación ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "Df_H_I = pd.concat(\n",
    "    [Df_H_I_EPS025, Df_H_I_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_H_I_EPS025, Df_H_I_EPSC25\n",
    "\n",
    "# 3. Ya tienes un único DataFrame con todos los registros:\n",
    "print(Df_H_I.columns)\n",
    "\n",
    "print(Df_H_I.shape)   # filas totales, 6 columnas\n",
    "Df_H_I = Df_H_I.drop_duplicates(subset=[\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "print(Df_H_I.shape)   # filas totales, 6 columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registros totales en Df_H_I:\", len(Df_H_I))\n",
    "\n",
    "# 1) Tus columnas clave\n",
    "key_cols = [\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# 2) Anti-join: todos los de DF_ADRES que NO están en Df_H_I\n",
    "anti = DF_ADRES.merge(\n",
    "    Df_H_I[key_cols],\n",
    "    on=key_cols,\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "missing = anti.loc[anti[\"_merge\"] == \"left_only\", DF_ADRES.columns]\n",
    "\n",
    "# 3) Elimina columnas duplicadas (por si DF_ADRES traía nombres repetidos)\n",
    "missing = missing.loc[:, ~missing.columns.duplicated()]\n",
    "\n",
    "# 4) Asegura que todas las columnas de Df_H_I estén en `missing` (rellena con NA las que falten)\n",
    "for col in Df_H_I.columns:\n",
    "    if col not in missing.columns:\n",
    "        missing[col] = pd.NA\n",
    "\n",
    "# 5) Reordena `missing` para que las columnas queden en el mismo orden que Df_H_I\n",
    "missing = missing[Df_H_I.columns]\n",
    "\n",
    "# 6) Concatenamos los registros faltantes\n",
    "Df_H_I = pd.concat([Df_H_I, missing], ignore_index=True)\n",
    "\n",
    "# 7) Eliminamos cualquier duplicado en base a las 3 claves, dejando el primero\n",
    "Df_H_I = Df_H_I.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "\n",
    "print(\"Registros totales en Df_H_I tras la ampliación:\", len(Df_H_I))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 1.1.4 Historicos Grupo Familiar ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar ambos DataFrames uno debajo del otro\n",
    "Df_H_GF = pd.concat(\n",
    "    [Df_H_GF_EPS025, Df_H_GF_EPSC25],\n",
    "    ignore_index=True,  # reindexa de 0…n-1\n",
    "    sort=False          # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_H_GF_EPSC25, Df_H_GF_EPS025\n",
    "\n",
    "print(\"Registros totales en Df_H_GF:\", len(Df_H_GF))\n",
    "# 3. Creamos una columna temporal con la fecha como datetime para comparaciones\n",
    "#    Usamos dayfirst=True porque el formato es dd/mm/yyyy\n",
    "Df_H_GF['_fecha_dt'] = pd.to_datetime(\n",
    "    Df_H_GF['GRP_FML_FECHA_INICIO'],\n",
    "    format='%d/%m/%Y',\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# 4. Ordenamos por esa fecha de manera descendente (el más reciente primero)\n",
    "#    y eliminamos duplicados dejando únicamente el primer registro de cada grupo\n",
    "Df_H_GF = (\n",
    "    Df_H_GF\n",
    "    .sort_values('_fecha_dt', ascending=False)\n",
    "    .drop_duplicates(subset='GRP_FML_AFILIADO_ID', keep='first')\n",
    "    .drop(columns='_fecha_dt')  # limpiamos la columna auxiliar\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 5. Mostrar resultado\n",
    "print(\"Columnas resultantes:\", Df_H_GF.columns.tolist())\n",
    "print(\"Registros totales en Df_H_GF:\", len(Df_H_GF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como tu ID es numérico o string, ajusta el tipo según corresponda:\n",
    "buscado = '112855395'   # o sin comillas si tu columna es numérica: buscado = 112855395\n",
    "# Filtrar\n",
    "resultado = Df_H_GF.loc[Df_H_GF['GRP_FML_AFILIADO_ID'] == buscado]\n",
    "# Imprimir\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 1.1.5 Maestro SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = Df_SIE.merge(\n",
    "    Df_H_I[[\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]],\n",
    "    left_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_SIE = Df_SIE.drop([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = Df_SIE.drop(\n",
    "    columns=[\"primer_nombre_bdua\", \"segundo_nombre_bdua\", \"primer_apellido_bdua\", \"segundo_apellido_bdua\"],\n",
    "    errors=\"ignore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Parentesco_SIE\n",
    "mapping_parentesco = dict(zip(Df_Parentesco_SIE[\"Parentesco\"], Df_Parentesco_SIE[\"Cod_Resolu_762_2023\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"parentesco_codificado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"parentesco_codificado\"] = Df_SIE[\"parentesco_codificado\"].map(mapping_parentesco)\n",
    "\n",
    "del Df_Parentesco_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Estado_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Estado_SIE[\"Estado_SIE\"], Df_Estado_SIE[\"Cod_Resolu_762_2023\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"estado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"estado\"] = Df_SIE[\"estado\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Estado_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Regimen_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Regimen_SIE[\"regimen_SIE\"], Df_Regimen_SIE[\"Regimen_ADRES\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"regimen\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"regimen\"] = Df_SIE[\"regimen\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Regimen_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos la columna \"municipio\" de df_Municipios_SIE a \"ID_COD_municipio\"\n",
    "df_munis = df_Municipios_SIE.rename(columns={\"municipio\": \"ID_COD_municipio\"})[[\"descripcion\", \"ID_COD_municipio\"]]\n",
    "\n",
    "# Hacemos merge entre Df_SIE y df_munis usando Df_SIE[\"municipio\"] y df_munis[\"descripcion\"]\n",
    "Df_SIE = Df_SIE.merge(df_munis, left_on=\"municipio\", right_on=\"descripcion\", how=\"left\")\n",
    "\n",
    "# Eliminamos la columna \"descripcion\" que ya no es necesaria\n",
    "Df_SIE.drop(\"descripcion\", axis=1, inplace=True)\n",
    "\n",
    "# Reordenamos las columnas para que \"ID_COD_municipio\" quede justo a la derecha de \"municipio\"\n",
    "cols = list(Df_SIE.columns)\n",
    "idx = cols.index(\"municipio\")\n",
    "cols.remove(\"ID_COD_municipio\")\n",
    "cols.insert(idx + 1, \"ID_COD_municipio\")\n",
    "Df_SIE = Df_SIE[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"fecha_nacimiento\" a datetime usando el formato original, \n",
    "# y formatearla a dd/mm/yyyy; asigna \"Fecha no validad\" en caso de error.\n",
    "Df_SIE[\"fecha_nacimiento\"] = pd.to_datetime(\n",
    "    Df_SIE[\"fecha_nacimiento\"],\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\"\n",
    ").apply(lambda x: x.strftime(\"%d/%m/%Y\") if pd.notnull(x) else \"Fecha no validad\")\n",
    "\n",
    "# Contar cuántos registros tienen \"Fecha no validad\" y mostrar el resultado\n",
    "invalid_count = (Df_SIE[\"fecha_nacimiento\"] == \"Fecha no validad\").sum()\n",
    "print(\"Número de registros con fecha no válida:\", invalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Tipo_Afiliados_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Tipo_Afiliados_SIE[\"Tipo_afiliado_SIE\"], Df_Tipo_Afiliados_SIE[\"Tipo_afiliado_ADRES\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"tipo_afiliado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"tipo_afiliado\"] = Df_SIE[\"tipo_afiliado\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Tipo_Afiliados_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Lista de columnas que quieres “traer” desde DF_ADRES\n",
    "columnas = [\n",
    "    \"ENT_ID\",\n",
    "    \"TPS_IDN_ID_CF\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_CF\",\n",
    "    \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\",\n",
    "    \"AFL_MUNICIPIO_NACIMIENTO\",\n",
    "    \"AFL_DISCAPACIDAD\",\n",
    "    \"TPS_AFL_ID\",\n",
    "    \"TPS_PRN_ID\",\n",
    "    \"TPS_GRP_PBL_ID\",\n",
    "    \"TPS_NVL_SSB_ID\",\n",
    "    \"TPS_CND_BNF_ID\",\n",
    "    \"DPR_ID\",\n",
    "    \"MNC_ID\",\n",
    "    \"TPS_EST_AFL_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\n",
    "    \"PORTABILIDAD\",\n",
    "    \"COD_IPS_P\",\n",
    "    \"MTDLG_G_P\",\n",
    "    \"SUB_SISBEN_IV\",\n",
    "    \"MARCASISBENIV+MARCASISBENIII\",\n",
    "    \"CRUCE_BDEX_RNEC\",\n",
    "]\n",
    "\n",
    "# 2) Hacemos un copy para no tocar el original\n",
    "df_merged = Df_SIE.copy()\n",
    "\n",
    "# 3) Merge “left” por AFL_ID; todas las columnas nuevas vendrán con sufijo \"_adres\"\n",
    "df_merged = df_merged.merge(\n",
    "    DF_ADRES[['AFL_ID'] + columnas],\n",
    "    on='AFL_ID',\n",
    "    how='left',\n",
    "    suffixes=('', '_adres')\n",
    ")\n",
    "\n",
    "# 4) Para cada campo en nuestra lista:\n",
    "#    - Si existe la columna \"{col}_adres\", rellenamos NaN de la original con el valor de _adres\n",
    "#    - Luego eliminamos la columna auxiliar \"_adres\"\n",
    "for col in columnas:\n",
    "    col_adres = f\"{col}_adres\"\n",
    "    if col_adres in df_merged.columns:\n",
    "        # fillna en la columna original con los valores _adres\n",
    "        df_merged[col] = df_merged[col_adres].fillna(df_merged.get(col))\n",
    "        # eliminamos la auxiliar\n",
    "        df_merged.drop(columns=col_adres, inplace=True)\n",
    "\n",
    "# 5) Ya tienes el resultado “fusionado” en df_merged; si quieres, reasignas:\n",
    "Df_SIE = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Selecciona sólo las columnas que quieres traer\n",
    "cols_to_add = [\n",
    "    \"GRP_FML_AFILIADO_ID\",\n",
    "    \"GRP_FML_COTIZANTE_ID\",\n",
    "    \"TPS_PRN_ID\",\n",
    "    \"GRP_FML_FECHA_INICIO\",\n",
    "    \"GRP_FML_FECHA_FIN\"\n",
    "]\n",
    "\n",
    "# 2) Haz el merge de Df_SIE con Df_H_GF\n",
    "Df_SIE = Df_SIE.merge(\n",
    "    Df_H_GF[cols_to_add],\n",
    "    left_on=\"AFL_ID\",\n",
    "    right_on=\"GRP_FML_AFILIADO_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) Como ya no necesitas la columna auxiliar, la quitas\n",
    "Df_SIE.drop(columns=\"GRP_FML_AFILIADO_ID\", inplace=True)\n",
    "\n",
    "# 4) Eliminar dataframer para limpiar memoria\n",
    "del Df_H_GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_SIE[\"tipo_afiliado\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# 2 Validaciones y Novedades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 2.1 Duplicados SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna \"Validar_Duplicidad\" según las condiciones solicitadas\n",
    "cond_duplicado = Df_SIE[\"AFL_ID\"].notna() & (Df_SIE[\"AFL_ID\"] != \"\") & (Df_SIE.duplicated(\"AFL_ID\", keep=False))\n",
    "cond_unico = Df_SIE[\"AFL_ID\"].notna() & (Df_SIE[\"AFL_ID\"] != \"\") & (~Df_SIE.duplicated(\"AFL_ID\", keep=False))\n",
    "cond_sin_id = Df_SIE[\"AFL_ID\"].isna() | (Df_SIE[\"AFL_ID\"] == \"\")\n",
    "\n",
    "Df_SIE[\"Validar_Duplicidad\"] = np.select(\n",
    "    [cond_duplicado, cond_unico, cond_sin_id],\n",
    "    [\"Registro Duplicado SIE\", \"Registro unico SIE\", \"Registro Sin ID\"],\n",
    "    default=\"—sin clasificar—\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Asegurar que ENT_ID no tenga valores nulos\n",
    "Df_SIE[\"ENT_ID\"] = Df_SIE[\"ENT_ID\"].fillna(\"Sin dato\")\n",
    "\n",
    "# Agrupar por Validar_Duplicidad y ENT_ID\n",
    "conteo = Df_SIE.groupby([\"Validar_Duplicidad\", \"ENT_ID\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Filtrar solo las categorías de interés en filas\n",
    "categorias_interes = [\"Registro Duplicado SIE\", \"Registro Sin ID\"]\n",
    "conteo_filtrado = conteo.loc[categorias_interes]\n",
    "\n",
    "# Ordenar las columnas de forma personalizada si deseas\n",
    "orden_columnas = [\"EPS025\", \"EPSC25\", \"Sin dato\"]\n",
    "conteo_filtrado = conteo_filtrado[[col for col in orden_columnas if col in conteo_filtrado.columns]]\n",
    "\n",
    "# Colores dinámicos\n",
    "colores = cm.get_cmap('Set2', len(conteo_filtrado.columns)).colors\n",
    "\n",
    "# Crear gráfico\n",
    "ax = conteo_filtrado.plot(\n",
    "    kind='bar',\n",
    "    figsize=(12, 6),\n",
    "    color=colores,\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "# Títulos y etiquetas\n",
    "plt.title(\"Distribución de Registros Problemáticos por Tipo de ENT_ID\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Tipo de registro\", fontsize=13)\n",
    "plt.ylabel(\"Cantidad de registros\", fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"ENT_ID\", fontsize=10, title_fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Agregar etiquetas numéricas sobre las barras con precisión\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(\n",
    "            rect.get_x() + rect.get_width() / 2,  # centro de la barra\n",
    "            height + max(conteo_filtrado.values.flatten()) * 0.01,  # ligeramente encima\n",
    "            f'{int(height):,}',  # formateado con coma si quieres: f\"{height:,}\"\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontweight='bold',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 2.2 Validar Estado SIE VS ADRES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 2.2.1 N14 Activo SIE, NO EXISTE  ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores para los filtros\n",
    "ent_id_valores = [\"EPS025\", \"Sin dato\"]\n",
    "tps_est_afl_id_valores = [\"RE\", \"SM\", \"SD\", \"\"] # Ya incluye cadenas vacías\n",
    "\n",
    "# Aplicar el filtro corregido\n",
    "DF_Retirar = Df_SIE[\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\") &\n",
    "    (Df_SIE[\"ENT_ID\"].isin(ent_id_valores)) &\n",
    "    (\n",
    "        # Condición para TPS_EST_AFL_ID: que esté en la lista O que sea nulo\n",
    "        Df_SIE[\"TPS_EST_AFL_ID\"].isin(tps_est_afl_id_valores) | \n",
    "        Df_SIE[\"TPS_EST_AFL_ID\"].isnull()\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Número de registros:\", DF_Retirar.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Proceso para validar y corregir la columna de fecha ---\n",
    "\n",
    "# 1. Intenta convertir la columna a formato de fecha.\n",
    "#    Las fechas que no sigan el formato 'DD/MM/YYYY' se convertirán en NaT (Not a Time).\n",
    "fechas_convertidas = pd.to_datetime(DF_Retirar[\"CND_AFL_FECHA_INICIO\"], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# 2. Obtiene la fecha de hoy para rellenar los valores inválidos.\n",
    "hoy = pd.Timestamp.now()\n",
    "\n",
    "# 3. Rellena los NaT (fechas inválidas) con la fecha de hoy.\n",
    "fechas_corregidas = fechas_convertidas.fillna(hoy)\n",
    "\n",
    "# 4. Asigna la columna corregida y formateada de vuelta al DataFrame.\n",
    "#    Toda la columna quedará como texto en formato 'DD/MM/YYYY'.\n",
    "DF_Retirar[\"CND_AFL_FECHA_INICIO\"] = fechas_corregidas.dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# --- Fin del proceso de validación ---\n",
    "\n",
    "# Construir el DataFrame df_N14 con los campos indicados.\n",
    "df_N14 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": list(range(1, len(DF_Retirar) + 1)),\n",
    "    \"ENT_ID\": DF_Retirar[\"regimen\"].tolist(),\n",
    "    \"TPS_IDN_ID\": DF_Retirar[\"tipo_documento\"].tolist(),\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": DF_Retirar[\"numero_identificacion\"].tolist(),\n",
    "    \"AFL_PRIMER_APELLIDO\": DF_Retirar[\"primer_apellido\"].tolist(),\n",
    "    \"AFL_SEGUNDO_APELLIDO\": DF_Retirar[\"segundo_apellido\"].tolist(),\n",
    "    \"AFL_PRIMER_NOMBRE\": DF_Retirar[\"primer_nombre\"].tolist(),\n",
    "    \"AFL_SEGUNDO_NOMBRE\": DF_Retirar[\"segundo_nombre\"].tolist(),\n",
    "    \"AFL_FECHA_NACIMIENTO\": DF_Retirar[\"fecha_nacimiento\"].tolist(),\n",
    "    \"DPR_ID\": DF_Retirar[\"ID_COD_municipio\"].str[:2].tolist(),\n",
    "    \"MNC_ID\": DF_Retirar[\"ID_COD_municipio\"].str[-3:].tolist(),\n",
    "    \"NOVEDAD\": [\"N14\"] * len(DF_Retirar),\n",
    "    \"FECHA_NOVEDAD\": DF_Retirar[\"CND_AFL_FECHA_INICIO\"].tolist(),\n",
    "    \"COD_1_NOVEDAD\": [\"RE\"] * len(DF_Retirar),\n",
    "    \"COD_2_NOVEDAD\": [\"4\"] * len(DF_Retirar),\n",
    "    \"COD_3_NOVEDAD\": [\"\" for _ in range(len(DF_Retirar))],\n",
    "    \"COD_4_NOVEDAD\": [\"\" for _ in range(len(DF_Retirar))],\n",
    "    \"COD_5_NOVEDAD\": [\"\" for _ in range(len(DF_Retirar))],\n",
    "    \"COD_6_NOVEDAD\": [\"\" for _ in range(len(DF_Retirar))],\n",
    "    \"COD_7_NOVEDAD\": [\"\" for _ in range(len(DF_Retirar))],\n",
    "    \"Motivo\": [\"Activos SIE: RE, DS, SM O NO ADRES \"] * len(DF_Retirar),\n",
    "})\n",
    "\n",
    "print(\"Número de registros en DF_Retirar:\", DF_Retirar.shape[0])\n",
    "print(\"Número de registros en df_N14:\", df_N14.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 2.2.2 N09 Activo Retirado SIE, Fallecido ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Fallecidos = Df_SIE[\n",
    "    (Df_SIE[\"estado\"].isin([\"AC\", \"RE\"])) &\n",
    "    #(Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\") &\n",
    "    #(Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"TPS_EST_AFL_ID\"].isin([\"AF\"]))\n",
    "]\n",
    "\n",
    "print(\"Número de registros:\", DF_Fallecidos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas de DF_ADRES que se requieren para el nuevo dataframe\n",
    "cols_adres = [\n",
    "    \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\",\n",
    "    \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"CND_AFL_FECHA_INICIO\"\n",
    "]\n",
    "\n",
    "# Realizamos merge de DF_Fallecidos con DF_ADRES usando el id: \n",
    "# DF_Fallecidos[\"tipo_documento\"] corresponde a DF_ADRES[\"TPS_IDN_ID\"]\n",
    "# DF_Fallecidos[\"numero_identificacion\"] corresponde a DF_ADRES[\"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "df_merged = DF_Fallecidos.merge(\n",
    "    DF_ADRES[cols_adres],\n",
    "    left_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_adres\")\n",
    ")\n",
    "\n",
    "\n",
    "# Construir el DataFrame df_N09 con los campos indicados.\n",
    "df_N09 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": list(range(1, len(df_merged) + 1)),\n",
    "    \"ENT_ID\": df_merged[\"ENT_ID\"].tolist(),\n",
    "    \"TPS_IDN_ID\": df_merged[\"TPS_IDN_ID\"].tolist(),\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_merged[\"HST_IDN_NUMERO_IDENTIFICACION\"].tolist(),\n",
    "    \"AFL_PRIMER_APELLIDO\": df_merged[\"AFL_PRIMER_APELLIDO\"].tolist(),\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_merged[\"AFL_SEGUNDO_APELLIDO\"].tolist(),\n",
    "    \"AFL_PRIMER_NOMBRE\": df_merged[\"AFL_PRIMER_NOMBRE\"].tolist(),\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_merged[\"AFL_SEGUNDO_NOMBRE\"].tolist(),\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_merged[\"AFL_FECHA_NACIMIENTO\"].tolist(),\n",
    "    \"DPR_ID\": df_merged[\"DPR_ID\"].tolist(),\n",
    "    \"MNC_ID\": df_merged[\"MNC_ID\"].tolist(),\n",
    "    \"NOVEDAD\": [\"N09\"] * len(df_merged),\n",
    "    \"FECHA_NOVEDAD\": df_merged[\"CND_AFL_FECHA_INICIO\"].tolist(),\n",
    "    \"COD_1_NOVEDAD\": [\"\"] * len(df_merged),\n",
    "    \"COD_2_NOVEDAD\": [\"\"] * len(df_merged),\n",
    "    \"COD_3_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_4_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_5_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_6_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_7_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"Motivo\": [\"Activos SIE: AF ADRES \"] * len(df_merged),\n",
    "})\n",
    "\n",
    "print(\"Número de registros en DF_Fallecidos:\", DF_Fallecidos.shape[0])\n",
    "print(\"Número de registros en df_N09:\", df_N09.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 2.2.2 activo SIE, No existe en ADRES con nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    ((Df_SIE[\"AFL_ID\"].isna()) | (Df_SIE[\"AFL_ID\"] == \"\")) &\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (~Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\")\n",
    ")\n",
    "\n",
    "# 2. Define la lista de columnas que quieres conservar en el nuevo df\n",
    "cols = [\n",
    "    \"regimen\",\n",
    "    \"tipo_documento\",\n",
    "    \"numero_identificacion\",\n",
    "    \"primer_apellido\",\n",
    "    \"segundo_apellido\",\n",
    "    \"primer_nombre\",\n",
    "    \"segundo_nombre\",\n",
    "    \"fecha_nacimiento\",\n",
    "    \"municipio\",\n",
    "    \"AFL_ID\"\n",
    "]\n",
    "# 3. Aplica el filtro y selecciona solo las columnas deseadas\n",
    "DF_Retirar = Df_SIE.loc[mask, cols].copy()\n",
    "print(\"Número de registros en DF_Retirar:\", DF_Retirar.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Identifico que usuarios del SIE no cruzan con historico de identificación y por ende no tienen ID_BDUA y estan Activos en SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_sin_AFL_ID = Df_SIE[\n",
    "    ((Df_SIE[\"AFL_ID\"].isna()) | (Df_SIE[\"AFL_ID\"] == \"\")) &\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (~Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\")\n",
    "    ]\n",
    "print(\"Número de registros en DF_Retirar:\", Df_sin_AFL_ID.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 2.3 Validar I02[ Serial] SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define la lista de columnas que quieres conservar en el nuevo df\n",
    "cols = [\n",
    "    \"regimen\",\n",
    "    \"tipo_documento\",\n",
    "    \"numero_identificacion\",\n",
    "    \"primer_apellido\",\n",
    "    \"segundo_apellido\",\n",
    "    \"primer_nombre\",\n",
    "    \"segundo_nombre\",\n",
    "    \"fecha_nacimiento\",\n",
    "    \"municipio\",\n",
    "    \"AFL_ID\"\n",
    "]\n",
    "\n",
    "# 2. Construye la máscara de filtrado\n",
    "mask = (\n",
    "    # AFL_ID no es nulo\n",
    "    Df_SIE[\"AFL_ID\"].notna()\n",
    "    # AFL_ID no está vacío (por si hay cadenas vacías o solo espacios)\n",
    "    & (Df_SIE[\"AFL_ID\"].astype(str).str.strip() != \"\")\n",
    "    # serial_fosyga y AFL_ID son distintos\n",
    "    & (Df_SIE[\"serial_fosyga\"] != Df_SIE[\"AFL_ID\"])\n",
    ")\n",
    "\n",
    "# 3. Aplica el filtro y selecciona solo las columnas deseadas\n",
    "df_I02 = Df_SIE.loc[mask, cols].copy()\n",
    "\n",
    "# 4. (Opcional) Reinicia el índice del nuevo DataFrame\n",
    "df_I02.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Ya lo tienes:\n",
    "print(df_I02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Inserta 'NUM_SOLICITUD_NOVEDAD' como primera columna, con valores 1…n\n",
    "df_I02.insert(\n",
    "    loc=0,\n",
    "    column=\"NUM_SOLICITUD_NOVEDAD\",\n",
    "    value=range(1, len(df_I02) + 1)\n",
    ")\n",
    "\n",
    "# 2. Sustituye en 'regimen'\n",
    "df_I02[\"regimen\"] = df_I02[\"regimen\"].replace({\n",
    "    \"Subsidiado\": \"EPS025\",\n",
    "    \"Contributivo\": \"EPSC25\"\n",
    "})\n",
    "\n",
    "# 3. Renombra 'AFL_ID' a 'COD_1_NOVEDAD'\n",
    "df_I02 = df_I02.rename(columns={\"AFL_ID\": \"COD_1_NOVEDAD\"})\n",
    "\n",
    "# 4. Crea las nuevas columnas (todas al final por ahora)\n",
    "df_I02[\"DPR_ID\"]         = \"\"                             # vacía\n",
    "df_I02[\"MNC_ID\"]         = \"\"                             # vacía\n",
    "df_I02[\"NOVEDAD\"]        = \"I02\"                          # valor fijo\n",
    "df_I02[\"FECHA_NOVEDAD\"]  = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "# Columnas COD_2_NOVEDAD … COD_7_NOVEDAD vacías\n",
    "for i in range(2, 8):\n",
    "    df_I02[f\"COD_{i}_NOVEDAD\"] = \"\"\n",
    "\n",
    "# 5. Reordena para obtener exactamente las 20 columnas en el orden pedido\n",
    "df_I02 = df_I02[\n",
    "    [\n",
    "        \"NUM_SOLICITUD_NOVEDAD\",\n",
    "        \"regimen\",\n",
    "        \"tipo_documento\",\n",
    "        \"numero_identificacion\",\n",
    "        \"primer_apellido\",\n",
    "        \"segundo_apellido\",\n",
    "        \"primer_nombre\",\n",
    "        \"segundo_nombre\",\n",
    "        \"fecha_nacimiento\",\n",
    "        \"DPR_ID\",\n",
    "        \"MNC_ID\",\n",
    "        \"NOVEDAD\",\n",
    "        \"FECHA_NOVEDAD\",\n",
    "        \"COD_1_NOVEDAD\",\n",
    "        \"COD_2_NOVEDAD\",\n",
    "        \"COD_3_NOVEDAD\",\n",
    "        \"COD_4_NOVEDAD\",\n",
    "        \"COD_5_NOVEDAD\",\n",
    "        \"COD_6_NOVEDAD\",\n",
    "        \"COD_7_NOVEDAD\",\n",
    "        \"municipio\"\n",
    "    ]\n",
    "]\n",
    "# 6. Convierte a datetime (ajusta el formato si ya viene como cadena):\n",
    "df_I02['fecha_nacimiento'] = pd.to_datetime(\n",
    "    df_I02['fecha_nacimiento'],\n",
    "    format='%Y-%m-%d',      # o el formato que realmente tengas; quita este argumento si varía\n",
    "    errors='coerce'         # convierte lo que no encaje en NaT\n",
    ")\n",
    "# 7. Ahora formatea al string DD/MM/YYYY:\n",
    "df_I02['fecha_nacimiento'] = df_I02['fecha_nacimiento'].dt.strftime('%d/%m/%Y')\n",
    "# Listo: ahora `df_I02` tiene 20 columnas en el orden y con los valores solicitados.\n",
    "print(df_I02.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar merge entre df_I02 y df_Municipios_SIE\n",
    "# Suponemos que en df_I02 la columna que contiene el id es \"municipio\"\n",
    "df_I02 = df_I02.merge(\n",
    "    df_Municipios_SIE[[\"descripcion\", \"municipio\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"municipio\",\n",
    "    right_on=\"descripcion\"\n",
    ")\n",
    "\n",
    "# Por el merge, la columna de df_Municipios_SIE se renombrará a \"municipio_y\" y la original en df_I02 es \"municipio_x\".\n",
    "# Usamos la columna \"municipio_y\" para extraer el código.\n",
    "df_I02[\"DPR_ID\"] = df_I02[\"municipio_y\"].str[:2]    #Primeros dos dígitos: código del departamento\n",
    "df_I02[\"MNC_ID\"] = df_I02[\"municipio_y\"].str[2:]    #Últimos tres dígitos: código del municipio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 2.4 Validar Grupos Familiares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asumimos que Df_SIE ya tiene estas cuatro columnas:\n",
    "#   SIE: tipo_documento_padre, numero_documento_padre\n",
    "#   ADRES: TPS_IDN_ID_CF, HST_IDN_NUMERO_IDENTIFICACION_CF\n",
    "\n",
    "# 1) Normaliza los “vacíos” para unificar: conviértelos a np.nan\n",
    "for c in [\"tipo_documento_padre\", \"numero_documento_padre\",\n",
    "          \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"]:\n",
    "    Df_SIE[c] = Df_SIE[c].replace(\"\", np.nan)\n",
    "\n",
    "# 2) Prepara las condiciones en orden\n",
    "condiciones = [\n",
    "    # todos vacíos → es el mismo afiliado, o sea, \"él\" es cabeza\n",
    "    Df_SIE[[\"tipo_documento_padre\", \"numero_documento_padre\",\n",
    "            \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"]]\n",
    "      .isna().all(axis=1),\n",
    "    # ambos lados tienen valor y coinciden\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna() &\n",
    "    (Df_SIE[\"tipo_documento_padre\"] == Df_SIE[\"TPS_IDN_ID_CF\"]) &\n",
    "    (Df_SIE[\"numero_documento_padre\"] == Df_SIE[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"]),\n",
    "    # ambos lados tienen valor pero NO coinciden\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna() &\n",
    "    ((Df_SIE[\"tipo_documento_padre\"] != Df_SIE[\"TPS_IDN_ID_CF\"]) |\n",
    "     (Df_SIE[\"numero_documento_padre\"] != Df_SIE[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"])),\n",
    "    # SIE tiene CF, ADRES no\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].isna(),\n",
    "    # ADRES tiene CF, SIE no\n",
    "    Df_SIE[\"tipo_documento_padre\"].isna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna()\n",
    "]\n",
    "\n",
    "resultados = [\n",
    "    \"Es Cabeza de familia\",\n",
    "    \"Cabeza de familia igual en SIE y en ADRES\",\n",
    "    \"Cabeza de familia diferente en SIE y en ADRES\",\n",
    "    \"SIE con cabeza familiar, ADRES sin cabeza de familia\",\n",
    "    \"ADRES con cabeza de familia, SIE sin cabeza de familia\"\n",
    "]\n",
    "\n",
    "# 3) Crea la columna Validar_CF con np.select\n",
    "Df_SIE[\"Validar_CF\"] = np.select(condiciones, resultados, default=\"—sin clasificar—\")\n",
    "\n",
    "# 4) Imprimir cuántos registros hay por cada categoría en la columna Validar_CF\n",
    "conteos = Df_SIE[\"Validar_CF\"].value_counts()\n",
    "\n",
    "print(\"Distribución de la validación de cabezas de familia:\")\n",
    "for categoria, cantidad in conteos.items():\n",
    "    print(f\"  {categoria}: {cantidad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear tabla pivote para contar las inconsistencias por régimen\n",
    "counts = pd.pivot_table(\n",
    "    Df_SIE,\n",
    "    index=\"regimen\",\n",
    "    columns=\"Validar_CF\",\n",
    "    values=\"AFL_ID\",  # cualquier columna sirve para contar\n",
    "    aggfunc=\"count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Simplificar seleccionando solo las categorías de interés\n",
    "categorias_interes = [\n",
    "    \"Cabeza de familia diferente en SIE y en ADRES\",\n",
    "    \"SIE con cabeza familiar, ADRES sin cabeza de familia\",\n",
    "    \"ADRES con cabeza de familia, SIE sin cabeza de familia\"\n",
    "]\n",
    "counts = counts[categorias_interes]\n",
    "\n",
    "# 1) Calculamos proporciones\n",
    "pct = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# 2) Dibujamos barras apiladas al 100%\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "pct.plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    ax=ax,\n",
    "    color=[\"C0\",\"C2\",\"C1\"],\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "# 3) Etiquetas de porcentaje en cada segmento\n",
    "for i, regimen in enumerate(pct.index):\n",
    "    left = 0\n",
    "    for cat in pct.columns:\n",
    "        w = pct.loc[regimen, cat]\n",
    "        if w > 5:  # sólo anotar donde haya suficiente espacio\n",
    "            ax.text(\n",
    "                left + w/2, i,\n",
    "                f\"{w:.0f}%\",\n",
    "                va=\"center\", ha=\"center\",\n",
    "                color=\"white\", fontsize=9\n",
    "            )\n",
    "        left += w\n",
    "\n",
    "# 4) Ajustes\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_xlabel(\"Porcentaje de inconsistencias\")\n",
    "ax.set_title(\"Proporción de inconsistencias de Cabeza de Familia\\npor Régimen\")\n",
    "ax.legend(title=\"Tipo de inconsistencia\", bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## 2.5. N01 Tipo de documento diferente\n",
    "Evoluciones efectivas en ADRES y no en SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar los registros donde tipo_documento y TPS_IDN_ID son diferentes y TPS_IDN_ID no está vacío\n",
    "mask = (\n",
    "    (Df_SIE[\"tipo_documento\"] != Df_SIE[\"TPS_IDN_ID\"]) &\n",
    "    (Df_SIE[\"TPS_IDN_ID\"].notna()) &\n",
    "    (Df_SIE[\"Validar_Duplicidad\"].astype(str).str.strip() == \"Registro unico SIE\") &\n",
    "    (Df_SIE[\"TPS_IDN_ID\"].astype(str).str.strip() != \"\")\n",
    ")\n",
    "\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_tmp = Df_SIE.loc[mask, [\n",
    "    \"ENT_ID\", \"tipo_documento\", \"numero_identificacion\", \"primer_apellido\",\n",
    "    \"segundo_apellido\", \"primer_nombre\", \"segundo_nombre\", \"fecha_nacimiento\",\n",
    "    \"ID_COD_municipio\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_FECHA_NACIMIENTO\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_tmp[\"DPR_ID\"] = df_tmp[\"ID_COD_municipio\"].str[:2]\n",
    "df_tmp[\"MNC_ID\"] = df_tmp[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# 4. Insertar columnas fijas\n",
    "df_tmp[\"NOVEDAD\"] = \"N01\"\n",
    "df_tmp[\"CND_AFL_FECHA_INICIO\"] = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# 5. Reordenar columnas según lo solicitado\n",
    "df_N01 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": range(1, len(df_tmp) + 1),\n",
    "    \"ENT_ID\": df_tmp[\"ENT_ID\"],\n",
    "    \"TPS_IDN_ID\": df_tmp[\"tipo_documento\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_tmp[\"numero_identificacion\"],\n",
    "    \"AFL_PRIMER_APELLIDO\": df_tmp[\"primer_apellido\"],\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_tmp[\"segundo_apellido\"],\n",
    "    \"AFL_PRIMER_NOMBRE\": df_tmp[\"primer_nombre\"],\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_tmp[\"segundo_nombre\"],\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_tmp[\"fecha_nacimiento\"],\n",
    "    \"DPR_ID\": df_tmp[\"DPR_ID\"],\n",
    "    \"MNC_ID\": df_tmp[\"MNC_ID\"],\n",
    "    \"NOVEDAD\": df_tmp[\"NOVEDAD\"],\n",
    "    \"CND_AFL_FECHA_INICIO\": df_tmp[\"CND_AFL_FECHA_INICIO\"],\n",
    "    \"COD_1_NOVEDAD\": df_tmp[\"TPS_IDN_ID\"],\n",
    "    \"COD_2_NOVEDAD\": df_tmp[\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    \"COD_3_NOVEDAD\": df_tmp[\"AFL_FECHA_NACIMIENTO\"],\n",
    "    \"COD_4_NOVEDAD\": \"0\",\n",
    "    \"COD_5_NOVEDAD\": \"\",\n",
    "    \"COD_6_NOVEDAD\": \"\",\n",
    "    \"COD_7_NOVEDAD\": \"\",\n",
    "    \"Motivo\": \"Evolucion de documento SIE\"\n",
    "})\n",
    "\n",
    "# 6. (Opcional) Revisar el resultado\n",
    "print(df_N01.head())\n",
    "print(df_N01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## 2.6. N02 Nombres\n",
    "Actualización o corrección de nombres del afiliado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar los registros donde primer_nombre y AFL_PRIMER_NOMBRE son diferentes y AFL_PRIMER_NOMBRE no está vacío\n",
    "import numpy as np\n",
    "\n",
    "def nombres_iguales(row):\n",
    "    # Si no hay datos ADRES\n",
    "    if pd.isnull(row[\"TPS_EST_AFL_ID\"]) or str(row[\"TPS_EST_AFL_ID\"]).strip() == \"\":\n",
    "        return \"sin Datos ADRES\"\n",
    "    # Si ambos nombres son vacíos o nulos\n",
    "    if (pd.isnull(row[\"primer_nombre\"]) or str(row[\"primer_nombre\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"AFL_PRIMER_NOMBRE\"]) or str(row[\"AFL_PRIMER_NOMBRE\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"segundo_nombre\"]) or str(row[\"segundo_nombre\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"AFL_SEGUNDO_NOMBRE\"]) or str(row[\"AFL_SEGUNDO_NOMBRE\"]).strip() == \"\"):\n",
    "        return \"Nombres iguales\"\n",
    "    # Comparación normal\n",
    "    if (str(row[\"primer_nombre\"]).lower().strip() != str(row[\"AFL_PRIMER_NOMBRE\"]).lower().strip()) or \\\n",
    "       (str(row[\"segundo_nombre\"]).lower().strip() != str(row[\"AFL_SEGUNDO_NOMBRE\"]).lower().strip()):\n",
    "        return \"Nombres diferentes\"\n",
    "    return \"Nombres iguales\"\n",
    "\n",
    "Df_SIE[\"Validar_nombres\"] = Df_SIE.apply(nombres_iguales, axis=1)\n",
    "\n",
    "\n",
    "mask = (\n",
    "    (Df_SIE[\"Validar_nombres\"].astype(str).str.strip() == \"Nombres diferentes\") &\n",
    "    (Df_SIE[\"Validar_Duplicidad\"].astype(str).str.strip() == \"Registro unico SIE\") &\n",
    "    (Df_SIE[\"TPS_IDN_ID\"].astype(str).str.strip() != \"\") &\n",
    "    # El registro es válido solo si las 2 columnas son diferentes a \"CN\"\n",
    "    (\n",
    "\t\t(Df_SIE[\"tipo_documento\"].astype(str).str.strip().str.upper() != \"CN\") &\n",
    "\t\t(Df_SIE[\"TPS_IDN_ID\"].astype(str).str.strip().str.upper() != \"CN\")\n",
    "\t)\n",
    ")\n",
    "\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_tmp = Df_SIE.loc[mask, [\n",
    "    \"ENT_ID\", \"tipo_documento\", \"numero_identificacion\", \"primer_apellido\",\n",
    "    \"segundo_apellido\", \"primer_nombre\", \"segundo_nombre\", \"fecha_nacimiento\",\n",
    "    \"ID_COD_municipio\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_tmp[\"DPR_ID\"] = df_tmp[\"ID_COD_municipio\"].str[:2]\n",
    "df_tmp[\"MNC_ID\"] = df_tmp[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# 4. Insertar columnas fijas\n",
    "df_tmp[\"NOVEDAD\"] = \"N02\"\n",
    "df_tmp[\"CND_AFL_FECHA_INICIO\"] = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# 5. Reordenar columnas según lo solicitado\n",
    "df_N02 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": range(1, len(df_tmp) + 1),\n",
    "    \"ENT_ID\": df_tmp[\"ENT_ID\"],\n",
    "    \"TPS_IDN_ID\": df_tmp[\"tipo_documento\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_tmp[\"numero_identificacion\"],\n",
    "    \"AFL_PRIMER_APELLIDO\": df_tmp[\"primer_apellido\"],\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_tmp[\"segundo_apellido\"],\n",
    "    \"AFL_PRIMER_NOMBRE\": df_tmp[\"primer_nombre\"],\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_tmp[\"segundo_nombre\"],\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_tmp[\"fecha_nacimiento\"],\n",
    "    \"DPR_ID\": df_tmp[\"DPR_ID\"],\n",
    "    \"MNC_ID\": df_tmp[\"MNC_ID\"],\n",
    "    \"NOVEDAD\": df_tmp[\"NOVEDAD\"],\n",
    "    \"CND_AFL_FECHA_INICIO\": df_tmp[\"CND_AFL_FECHA_INICIO\"],\n",
    "    \"COD_1_NOVEDAD\": df_tmp[\"AFL_PRIMER_NOMBRE\"],\n",
    "    \"COD_2_NOVEDAD\": df_tmp[\"AFL_SEGUNDO_NOMBRE\"],\n",
    "    \"COD_3_NOVEDAD\": \"\",\n",
    "    \"COD_4_NOVEDAD\": \"\",\n",
    "    \"COD_5_NOVEDAD\": \"\",\n",
    "    \"COD_6_NOVEDAD\": \"\",\n",
    "    \"COD_7_NOVEDAD\": \"\",\n",
    "    \"Motivo\": \"Corrección nombres SIE\"\n",
    "})\n",
    "\n",
    "# 6. (Opcional) Revisar el resultado\n",
    "print(df_N02.head())\n",
    "print(df_N02.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## 2.7. N03 Apellidos\n",
    "Actualización o corrección de apellidos del afiliado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar los registros donde primer_apellido y AFL_PRIMER_APELLIDO son diferentes y AFL_PRIMER_APELLIDO no está vacío\n",
    "import numpy as np\n",
    "\n",
    "def apellidos_iguales(row):\n",
    "    # Si no hay datos ADRES\n",
    "    if pd.isnull(row[\"TPS_EST_AFL_ID\"]) or str(row[\"TPS_EST_AFL_ID\"]).strip() == \"\":\n",
    "        return \"sin Datos ADRES\"\n",
    "    # Si ambos apellidos son vacíos o nulos\n",
    "    if (pd.isnull(row[\"primer_apellido\"]) or str(row[\"primer_apellido\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"AFL_PRIMER_APELLIDO\"]) or str(row[\"AFL_PRIMER_APELLIDO\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"segundo_apellido\"]) or str(row[\"segundo_apellido\"]).strip() == \"\") and \\\n",
    "       (pd.isnull(row[\"AFL_SEGUNDO_APELLIDO\"]) or str(row[\"AFL_SEGUNDO_APELLIDO\"]).strip() == \"\"):\n",
    "        return \"Apellidos iguales\"\n",
    "    # Comparación normal\n",
    "    if (str(row[\"primer_apellido\"]).lower().strip() != str(row[\"AFL_PRIMER_APELLIDO\"]).lower().strip()) or \\\n",
    "       (str(row[\"segundo_apellido\"]).lower().strip() != str(row[\"AFL_SEGUNDO_APELLIDO\"]).lower().strip()):\n",
    "        return \"Apellidos diferentes\"\n",
    "    return \"Apellidos iguales\"\n",
    "\n",
    "Df_SIE[\"Validar_apellidos\"] = Df_SIE.apply(apellidos_iguales, axis=1)\n",
    "mask = (\n",
    "    (Df_SIE[\"Validar_apellidos\"].astype(str).str.strip() == \"Apellidos diferentes\") &\n",
    "    (Df_SIE[\"Validar_Duplicidad\"].astype(str).str.strip() == \"Registro unico SIE\") &\n",
    "    # El registro es válido solo si las 2 columnas son diferentes a \"CN\"\n",
    "    (\n",
    "\t\t(Df_SIE[\"tipo_documento\"].astype(str).str.strip().str.upper() != \"CN\") &\n",
    "\t\t(Df_SIE[\"TPS_IDN_ID\"].astype(str).str.strip().str.upper() != \"CN\")\n",
    "\t)\n",
    ")\n",
    "\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_tmp = Df_SIE.loc[mask, [\n",
    "    \"ENT_ID\", \"tipo_documento\", \"numero_identificacion\", \"primer_apellido\",\n",
    "    \"segundo_apellido\", \"primer_nombre\", \"segundo_nombre\", \"fecha_nacimiento\",\n",
    "    \"ID_COD_municipio\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_tmp[\"DPR_ID\"] = df_tmp[\"ID_COD_municipio\"].str[:2]\n",
    "df_tmp[\"MNC_ID\"] = df_tmp[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# 4. Insertar columnas fijas\n",
    "df_tmp[\"NOVEDAD\"] = \"N03\"\n",
    "df_tmp[\"CND_AFL_FECHA_INICIO\"] = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# 5. Reordenar columnas según lo solicitado\n",
    "df_N03 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": range(1, len(df_tmp) + 1),\n",
    "    \"ENT_ID\": df_tmp[\"ENT_ID\"],\n",
    "    \"TPS_IDN_ID\": df_tmp[\"tipo_documento\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_tmp[\"numero_identificacion\"],\n",
    "    \"AFL_PRIMER_APELLIDO\": df_tmp[\"primer_apellido\"],\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_tmp[\"segundo_apellido\"],\n",
    "    \"AFL_PRIMER_NOMBRE\": df_tmp[\"primer_nombre\"],\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_tmp[\"segundo_nombre\"],\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_tmp[\"fecha_nacimiento\"],\n",
    "    \"DPR_ID\": df_tmp[\"DPR_ID\"],\n",
    "    \"MNC_ID\": df_tmp[\"MNC_ID\"],\n",
    "    \"NOVEDAD\": df_tmp[\"NOVEDAD\"],\n",
    "    \"CND_AFL_FECHA_INICIO\": df_tmp[\"CND_AFL_FECHA_INICIO\"],\n",
    "    \"COD_1_NOVEDAD\": df_tmp[\"AFL_PRIMER_APELLIDO\"],\n",
    "    \"COD_2_NOVEDAD\": df_tmp[\"AFL_SEGUNDO_APELLIDO\"],\n",
    "    \"COD_3_NOVEDAD\": \"\",\n",
    "    \"COD_4_NOVEDAD\": \"\",\n",
    "    \"COD_5_NOVEDAD\": \"\",\n",
    "    \"COD_6_NOVEDAD\": \"\",\n",
    "    \"COD_7_NOVEDAD\": \"\",\n",
    "    \"Motivo\": \"Corrección apellidos SIE\"\n",
    "})\n",
    "\n",
    "# 6. (Opcional) Revisar el resultado\n",
    "print(df_N03.head())\n",
    "print(df_N03.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## 2.8. N21 sisben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar los registros donde primer_apellido y AFL_PRIMER_APELLIDO son diferentes y AFL_PRIMER_APELLIDO no está vacío\n",
    "import pandas as pd\n",
    "\n",
    "def validar_sisben(row):\n",
    "    tps_grp = str(row.get(\"TPS_GRP_PBL_ID\", \"\")).strip()\n",
    "    sub_sisben = str(row.get(\"SUB_SISBEN_IV\", \"\")).strip()\n",
    "    # Si no hay datos ADRES\n",
    "    if pd.isnull(row.get(\"TPS_EST_AFL_ID\")) or str(row.get(\"TPS_EST_AFL_ID\")).strip() == \"\":\n",
    "        return \"sin Datos ADRES\"\n",
    "    # Si TPS_GRP_PBL_ID es \"5\"\n",
    "    if tps_grp == \"5\":\n",
    "        # Si SUB_SISBEN_IV es vacío, espacio o nulo\n",
    "        if pd.isnull(row.get(\"SUB_SISBEN_IV\")) or sub_sisben == \"\":\n",
    "            return \"SSIBEN III\"\n",
    "        # Comparar subgrupo y SUB_SISBEN_IV\n",
    "        if str(row.get(\"subgrupo\", \"\")).strip() == sub_sisben:\n",
    "            return \"sisben igual\"\n",
    "        else:\n",
    "            return \"Sisben Diferente\"\n",
    "    # Si TPS_GRP_PBL_ID es \"31\" o \"34\"\n",
    "    if tps_grp in [\"31\", \"34\"]:\n",
    "        return \"afiliación oficion o contribución solidaria\"\n",
    "    # Comparar grupo_poblacional y TPS_GRP_PBL_ID\n",
    "    if str(row.get(\"grupo_poblacional\", \"\")).strip() == tps_grp:\n",
    "        return \"sisben igual\"\n",
    "    else:\n",
    "        return \"Sisben Diferente\"\n",
    "\n",
    "Df_SIE[\"Validar_Sisben\"] = Df_SIE.apply(validar_sisben, axis=1)\n",
    "\n",
    "\n",
    "mask = (\n",
    "    (Df_SIE[\"Validar_Sisben\"].astype(str).str.strip() == \"Sisben Diferente\") &\n",
    "    (Df_SIE[\"Validar_Duplicidad\"].astype(str).str.strip() == \"Registro unico SIE\") &\n",
    "    (Df_SIE[\"TPS_GRP_PBL_ID\"].notna()) &  # Verifica que no sea nulo (NaN)\n",
    "    (Df_SIE[\"TPS_GRP_PBL_ID\"].astype(str).str.strip().ne(\"\"))  # Verifica que no sea vacío o solo espacios\n",
    ")\n",
    "\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_tmp = Df_SIE.loc[mask, [\n",
    "    \"ENT_ID\", \"tipo_documento\", \"numero_identificacion\", \"primer_apellido\",\n",
    "    \"segundo_apellido\", \"primer_nombre\", \"segundo_nombre\", \"fecha_nacimiento\",\n",
    "    \"ID_COD_municipio\", \"TPS_GRP_PBL_ID\", \"SUB_SISBEN_IV\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_tmp[\"DPR_ID\"] = df_tmp[\"ID_COD_municipio\"].str[:2]\n",
    "df_tmp[\"MNC_ID\"] = df_tmp[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# 4. Insertar columnas fijas\n",
    "df_tmp[\"NOVEDAD\"] = \"N21\"\n",
    "df_tmp[\"CND_AFL_FECHA_INICIO\"] = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# 5. Reordenar columnas según lo solicitado\n",
    "df_N21 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": range(1, len(df_tmp) + 1),\n",
    "    \"ENT_ID\": df_tmp[\"ENT_ID\"],\n",
    "    \"TPS_IDN_ID\": df_tmp[\"tipo_documento\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_tmp[\"numero_identificacion\"],\n",
    "    \"AFL_PRIMER_APELLIDO\": df_tmp[\"primer_apellido\"],\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_tmp[\"segundo_apellido\"],\n",
    "    \"AFL_PRIMER_NOMBRE\": df_tmp[\"primer_nombre\"],\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_tmp[\"segundo_nombre\"],\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_tmp[\"fecha_nacimiento\"],\n",
    "    \"DPR_ID\": df_tmp[\"DPR_ID\"],\n",
    "    \"MNC_ID\": df_tmp[\"MNC_ID\"],\n",
    "    \"NOVEDAD\": df_tmp[\"NOVEDAD\"],\n",
    "    \"CND_AFL_FECHA_INICIO\": df_tmp[\"CND_AFL_FECHA_INICIO\"],\n",
    "    \"COD_1_NOVEDAD\": \"2\",\n",
    "    \"COD_2_NOVEDAD\": df_tmp[\"SUB_SISBEN_IV\"],\n",
    "    \"COD_3_NOVEDAD\": df_tmp[\"TPS_GRP_PBL_ID\"],\n",
    "    \"COD_4_NOVEDAD\": \"\",\n",
    "    \"COD_5_NOVEDAD\": \"\",\n",
    "    \"COD_6_NOVEDAD\": \"\",\n",
    "    \"COD_7_NOVEDAD\": \"\",\n",
    "    \"Motivo\": \"Corrección sisben SIE\"\n",
    "})\n",
    "\n",
    "def validar_novedades_N21(row):\n",
    "    cod_3 = str(row[\"COD_3_NOVEDAD\"]).strip()\n",
    "    cod_2 = str(row[\"COD_2_NOVEDAD\"]).strip()\n",
    "    # Inicializar los valores\n",
    "    cod_1, cod_4, cod_5, cod_6 = row[\"COD_1_NOVEDAD\"], row[\"COD_4_NOVEDAD\"], row[\"COD_5_NOVEDAD\"], row[\"COD_6_NOVEDAD\"]\n",
    "\n",
    "    # Si COD_3_NOVEDAD es \"5\"\n",
    "    if cod_3 == \"5\":\n",
    "        # Validar rango de COD_2_NOVEDAD\n",
    "        if cod_2[:1] in [\"A\", \"B\"]:\n",
    "            cod_4 = \"1\"\n",
    "        elif cod_2[:1] in [\"C\", \"D\"]:\n",
    "            cod_4 = \"2\"\n",
    "        cod_5 = \"06\"\n",
    "    # Si COD_3_NOVEDAD es \"17\"\n",
    "    elif cod_3 == \"17\":\n",
    "        cod_1 = \"3\"\n",
    "        cod_4 = \"N\"\n",
    "        cod_5 = \"01\"\n",
    "        cod_6 = \"999\"\n",
    "    # Si COD_3_NOVEDAD es diferente de \"5\" y \"17\"\n",
    "    else:\n",
    "        cod_1 = \"3\"\n",
    "        cod_4 = \"N\"\n",
    "        cod_5 = \"06\"\n",
    "    # Retornar los valores actualizados\n",
    "    return pd.Series([cod_1, cod_4, cod_5, cod_6], index=[\"COD_1_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\"])\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "df_N21[[\"COD_1_NOVEDAD\", \"COD_4_NOVEDAD\", \"COD_5_NOVEDAD\", \"COD_6_NOVEDAD\"]] = df_N21.apply(validar_novedades_N21, axis=1)\n",
    "\n",
    "# 6. (Opcional) Revisar el resultado\n",
    "print(df_N21.head())\n",
    "print(df_N21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## 2.4. I03 activar SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de estados de traslado que quieres filtrar\n",
    "estados_a_filtrar = [\"Pendiente Ingreso Traslado RS\", \"Pendiente Ingreso MS\"]\n",
    "\n",
    "# Aplicar las máscaras/filtros\n",
    "mask = (\n",
    "    (Df_SIE[\"estado_traslado\"].isin(estados_a_filtrar)) &\n",
    "    (Df_SIE[\"ENT_ID\"] == \"EPS025\") &\n",
    "    (Df_SIE[\"TPS_EST_AFL_ID\"] == \"AC\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas de DF_ADRES que se requieren para el nuevo dataframe\n",
    "cols_adres = [\n",
    "    \"tipo_documento\", \"numero_identificacion\", \"ENT_ID\",\n",
    "    \"primer_apellido\", \"segundo_apellido\", \"primer_nombre\",\n",
    "    \"segundo_nombre\", \"fecha_nacimiento\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"CND_AFL_FECHA_INICIO\"\n",
    "]\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_merged = Df_SIE.loc[mask, [\n",
    "    \"tipo_documento\", \"numero_identificacion\", \"ENT_ID\",\n",
    "    \"primer_apellido\", \"segundo_apellido\", \"primer_nombre\",\n",
    "    \"segundo_nombre\", \"fecha_nacimiento\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"CND_AFL_FECHA_INICIO\", \"ID_COD_municipio\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_merged[\"DPR_ID\"] = df_merged[\"ID_COD_municipio\"].str[:2]\n",
    "df_merged[\"MNC_ID\"] = df_merged[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# Asegurarse de que \"CND_AFL_FECHA_INICIO\" sea una Serie unidimensional:\n",
    "if isinstance(df_merged[\"CND_AFL_FECHA_INICIO\"], pd.DataFrame):\n",
    "    cnd_fecha = df_merged[\"CND_AFL_FECHA_INICIO\"].iloc[:, 0]\n",
    "else:\n",
    "    cnd_fecha = df_merged[\"CND_AFL_FECHA_INICIO\"]\n",
    "\n",
    "# Construir el DataFrame df_I03 con los campos indicados.\n",
    "df_I03 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": list(range(1, len(df_merged) + 1)),\n",
    "    \"ENT_ID\": df_merged[\"ENT_ID\"].tolist(),\n",
    "    \"TPS_IDN_ID\": df_merged[\"tipo_documento\"].tolist(),\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_merged[\"numero_identificacion\"].tolist(),\n",
    "    \"AFL_PRIMER_APELLIDO\": df_merged[\"primer_apellido\"].tolist(),\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_merged[\"segundo_apellido\"].tolist(),\n",
    "    \"AFL_PRIMER_NOMBRE\": df_merged[\"primer_nombre\"].tolist(),\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_merged[\"segundo_nombre\"].tolist(),\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_merged[\"fecha_nacimiento\"].tolist(),\n",
    "    \"DPR_ID\": df_merged[\"DPR_ID\"].tolist(),\n",
    "    \"MNC_ID\": df_merged[\"MNC_ID\"].tolist(),\n",
    "    \"NOVEDAD\": [\"I03\"] * len(df_merged),\n",
    "    \"FECHA_NOVEDAD\": cnd_fecha.tolist(),\n",
    "    \"COD_1_NOVEDAD\": [\"AC\"] * len(df_merged),\n",
    "    \"COD_2_NOVEDAD\": [\"\"] * len(df_merged),\n",
    "    \"COD_3_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_4_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_5_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_6_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_7_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"Motivo\": [\"Proceso traslado SIE EPS025: AC, EPS025 ADRES \"] * len(df_merged),\n",
    "})\n",
    "\n",
    "print(\"Número de registros en df_I03:\", df_I03.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## 2.3.1 Reportar evolucion a ADRES.\n",
    "Validando que en SIE el tipo de documento este bien pero no en ADRES. para identificar los afiliados y reportarlos ante ADRES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los pares de evolución válidos\n",
    "evoluciones_validas = {\n",
    "    (\"CN\", \"RC\"),\n",
    "    (\"RC\", \"TI\"),\n",
    "    (\"TI\", \"CC\"),\n",
    "    (\"PE\", \"PT\"),\n",
    "}\n",
    "\n",
    "# Verificar cuántos registros son inconsistentes\n",
    "print(f\"Total de registros: {len(df_N01)}\")\n",
    "\n",
    "# 1. Detectar registros con evolución actual VÁLIDA\n",
    "mask_evolucion_valida = df_N01.apply(\n",
    "    lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 2. Detectar registros con evolución actual INVÁLIDA\n",
    "mask_evolucion_invalida = ~mask_evolucion_valida\n",
    "\n",
    "# 3. De los inválidos, detectar cuáles se pueden corregir intercambiando\n",
    "mask_corregible_intercambiando = df_N01.apply(\n",
    "    lambda row: (row[\"COD_1_NOVEDAD\"], row[\"TPS_IDN_ID\"]) in evoluciones_validas,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 4. Máscara final: registros inválidos que se pueden corregir intercambiando\n",
    "mask_a_intercambiar = mask_evolucion_invalida & mask_corregible_intercambiando\n",
    "\n",
    "print(f\"Registros con evolución válida: {mask_evolucion_valida.sum()}\")\n",
    "print(f\"Registros con evolución inválida: {mask_evolucion_invalida.sum()}\")\n",
    "print(f\"Registros inválidos que se pueden corregir intercambiando: {mask_a_intercambiar.sum()}\")\n",
    "print(f\"Registros inválidos que NO se pueden corregir: {(mask_evolucion_invalida & ~mask_corregible_intercambiando).sum()}\")\n",
    "\n",
    "# 5. Intercambiar SOLO los registros que necesitan corrección\n",
    "if mask_a_intercambiar.any():\n",
    "    # Intercambiar valores\n",
    "    df_N01.loc[mask_a_intercambiar, [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]] = \\\n",
    "        df_N01.loc[mask_a_intercambiar, [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\"]].values\n",
    "    \n",
    "    # Cambiar el motivo SOLO para estos registros\n",
    "    df_N01.loc[mask_a_intercambiar, \"Motivo\"] = \"Reportar novedad ante ADRES\"\n",
    "    \n",
    "    print(f\"Se intercambiaron {mask_a_intercambiar.sum()} registros para corregir evoluciones\")\n",
    "else:\n",
    "    print(\"No se encontraron registros que requieran intercambio\")\n",
    "\n",
    "# Verificar los motivos después del cambio\n",
    "print(\"\\nDistribución de motivos:\")\n",
    "print(df_N01[\"Motivo\"].value_counts())\n",
    "\n",
    "# Verificación final: mostrar algunos ejemplos\n",
    "print(\"\\nEjemplos de evoluciones después del procesamiento:\")\n",
    "print(\"Evoluciones válidas (primeros 5):\")\n",
    "validas_final = df_N01[df_N01.apply(lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas, axis=1)]\n",
    "if len(validas_final) > 0:\n",
    "    print(validas_final[[\"TPS_IDN_ID\", \"COD_1_NOVEDAD\", \"Motivo\"]].head())\n",
    "\n",
    "print(\"\\nEvoluciones que siguen siendo inválidas:\")\n",
    "invalidas_final = df_N01[~df_N01.apply(lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas, axis=1)]\n",
    "if len(invalidas_final) > 0:\n",
    "    print(invalidas_final[[\"TPS_IDN_ID\", \"COD_1_NOVEDAD\", \"Motivo\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## 2.5. ESPACIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Filtrar registros donde 'numero_identificacion' contiene espacios o caracteres no numéricos\n",
    "\n",
    "# Expresión regular: busca espacios o cualquier carácter que no sea letra o número\n",
    "mask_espacios = Df_SIE[\"numero_identificacion\"].astype(str).str.contains(r\"[^\\w]\", regex=True)\n",
    "\n",
    "DF_ESPACIOS = Df_SIE[mask_espacios].copy()\n",
    "\n",
    "print(\"Registros con espacios o caracteres especiales en 'numero_identificacion':\", DF_ESPACIOS.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "# 4. Guardar archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## 4.1. Carpeta N01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N01\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N01\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N01.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N01_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## 4.2. Carpeta N02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N02\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N02\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N02.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N02_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## 4.3. Carpeta N03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N03\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N03\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N03.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N03_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## 4.2. Carpeta N09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N09\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N09\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N09.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N09_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## 4.3. Carpeta N14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N14\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N14\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N14.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N14_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## 4.4. Carpeta N21 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N21\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N21\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N21.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N21_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## 4.4. Carpeta I02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"I02\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"I02\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"municipio_x\", \"descripcion\", \"municipio_y\"]\n",
    "for dpr, grupo in df_I02.groupby(\"DPR_ID\"):\n",
    "    filename    = f\"I02_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Opción A: drop por parámetro\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## 4.4. Carpeta I03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"I03\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"I03\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_I03.groupby(\"DPR_ID\"):\n",
    "    filename = f\"I03_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "# ruta salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Suponiendo que tus DataFrames ya están cargados:\n",
    "# Df_SIE, DF_ADRES, df_N01, df_N02, df_N03, df_N09, df_N14, df_N21, df_I02, df_I03, DF_ESPACIOS, Df_sin_AFL_ID\n",
    "\n",
    "# 1. Crear una lista con los DataFrames y sus nombres\n",
    "# Esto facilita el manejo de todos los DataFrames en un bucle\n",
    "dfs_a_guardar = {\n",
    "    'N01': df_N01,\n",
    "    'N02': df_N02,\n",
    "    'N03': df_N03,\n",
    "    'N09': df_N09,\n",
    "    'N14': df_N14,\n",
    "    'N21': df_N21,\n",
    "    'I02': df_I02,\n",
    "    'I03': df_I03,\n",
    "}\n",
    "\n",
    "# 2. Crear el DataFrame de resumen\n",
    "# Inicializamos listas para los nombres y los conteos\n",
    "nombres_df = []\n",
    "conteo_registros = []\n",
    "\n",
    "# Iteramos sobre el diccionario para obtener el nombre y el tamaño de cada DataFrame\n",
    "for nombre, df in dfs_a_guardar.items():\n",
    "    nombres_df.append(nombre)\n",
    "    conteo_registros.append(len(df))\n",
    "\n",
    "# Creamos el DataFrame de resumen con las listas\n",
    "df_resumen = pd.DataFrame({\n",
    "    'Nombre DataFrame': nombres_df,\n",
    "    'Numero de Registros': conteo_registros\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(R_Salida, \"Reporte SIE.xlsx\")\n",
    "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "    df_resumen.to_excel(writer, sheet_name=\"Resumen\", index=False)\n",
    "    Df_SIE.to_excel(writer, sheet_name=\"MS_SIE\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "    df_N01.to_excel(writer, sheet_name=\"N01\", index=False)\n",
    "    df_N02.to_excel(writer, sheet_name=\"N02\", index=False)\n",
    "    df_N03.to_excel(writer, sheet_name=\"N03\", index=False)\n",
    "    df_N09.to_excel(writer, sheet_name=\"N09\", index=False)\n",
    "    df_N14.to_excel(writer, sheet_name=\"N14\", index=False)\n",
    "    df_N21.to_excel(writer, sheet_name=\"N21\", index=False)\n",
    "    df_I02.to_excel(writer, sheet_name=\"I02\", index=False)\n",
    "    df_I03.to_excel(writer, sheet_name=\"I03\", index=False)\n",
    "    DF_ESPACIOS.to_excel(writer, sheet_name=\"DF_ESPACIOS\", index=False)\n",
    "    Df_sin_AFL_ID.to_excel(writer, sheet_name=\"Df_sin_AFL_ID\", index=False)\n",
    "print(f\"Archivo guardado: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
