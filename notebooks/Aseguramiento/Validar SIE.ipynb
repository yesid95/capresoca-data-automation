{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3ab6f9",
   "metadata": {},
   "source": [
    "# 1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf94057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # para manipulación de dataframes y tablas\n",
    "from datetime import datetime, date, timedelta  # para trabajar con fechas y tiempos\n",
    "import os  # para manejar rutas del sistema y operaciones relacionadas\n",
    "from zipfile import ZipFile # para trabajar con archivos zip\n",
    "import shutil  # para copiar y mover archivos y directorios\n",
    "from pathlib import Path  # para trabajar de manera más cómoda con rutas en el sistema\n",
    "import numpy as np  # para trabajar con arreglos y matrices numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Ms_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\ms_sie\\Reporte_Validación Archivos Maestro_2025_07_28 (2).csv\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0025072025.TXT\"\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0022072025.TXT\"\n",
    "R_Expedientes_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\Aseguramiento\\Expedientes\\Años\"\n",
    "R_Historico_Identificacion_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Historico\\2025\\20250629-HISTORICOS\\HISTORICO_IDENTIFICACION_S_E.TXT.ZIP\"\n",
    "R_Historico_Grupo_Familia_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Historico\\2025\\20250629-HISTORICOS\\HISTORIA_GRUPO_FAMILIAR_E.TXT.ZIP\"\n",
    "R_Historico_Identificacion_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Historico\\2025\\20250629-HISTORICOS\\HISTORIA_IDENTIFICACION.TXT.ZIP\"\n",
    "R_Historico_Grupo_Familia_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Historico\\2025\\20250629-HISTORICOS\\HISTORIA_GRUPO_FAMILIAR.TXT.ZIP\"\n",
    "R_Municipios_DANE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Constantes\\Departamentos.txt\"\n",
    "R_Municipios_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Reporte_MUNICIPIOS_2025_05_14.csv\"\n",
    "R_Parentesco_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Parentescos Codificados.txt\"\n",
    "R_Estado_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\ESTADO DE AFILIACION.txt\"\n",
    "R_Regimen_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\regimen codificado.txt\"\n",
    "R_Tipo_Afiliados_SIE = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SIE\\codificación de variables categóricas\\Tipo de afiliado.txt\"\n",
    "\n",
    "R_Salida = r\"\\\\Servernas\\AYC2\\ASEGURAMIENTO\\ASEGURAMIENTO\\PROCESO_ASEGURAMIENTO\\REGIMEN SUBSIDIADO\\MUNICIPIOS 2025\\ACTUALIZACION SIE\\01_Procesos BDUA validación SIE\\07_Julio\\Yesid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Parentesco_SIE = pd.read_csv(R_Parentesco_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Estado_SIE = pd.read_csv(R_Estado_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Regimen_SIE = pd.read_csv(R_Regimen_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Tipo_Afiliados_SIE = pd.read_csv(R_Tipo_Afiliados_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "df_Municipios_SIE = pd.read_csv(R_Municipios_SIE, sep=';', dtype=str, encoding='ANSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que df_Municipios_SIE[\"municipio\"] tenga 5 dígitos\n",
    "df_Municipios_SIE[\"municipio\"] = df_Municipios_SIE[\"municipio\"].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e763cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todos los archivos .TXT en la carpeta R_Expedientes_SIE\n",
    "txt_files = list(Path(R_Expedientes_SIE).glob(\"*.TXT\"))\n",
    "\n",
    "# Cargar cada archivo en un dataframe y luego concatenarlos\n",
    "df_list = [pd.read_csv(file, sep=\"|\", encoding=\"ANSI\", dtype=str) for file in txt_files]\n",
    "DF_Expedientes_SIE = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5119ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    \"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"HST_IDN_FECHA_INICIO\", \"HST_IDN_FECHA_FIN\", \"ENT_ID\"\n",
    "]\n",
    "\n",
    "def load_txt_from_zip(zip_path: str,\n",
    "                      txt_name: str,\n",
    "                      columns: list[str],\n",
    "                      sep: str = \",\",\n",
    "                      encoding: str = \"ANSI\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Abre zip_path, extrae txt_name y lo carga en un DataFrame con las columnas dadas.\n",
    "    \"\"\"\n",
    "    with ZipFile(zip_path) as z:\n",
    "        with z.open(txt_name) as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=sep,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                encoding=encoding\n",
    "            )\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "# ahora, para cada archivo ZIP basta con:\n",
    "Df_H_I_EPS025 = load_txt_from_zip(\n",
    "    R_Historico_Identificacion_EPS025,\n",
    "    \"HISTORICO_IDENTIFICACION_S_E.TXT\",\n",
    "    new_columns\n",
    ")\n",
    "\n",
    "Df_H_I_EPSC25 = load_txt_from_zip(\n",
    "    R_Historico_Identificacion_EPSC25,\n",
    "    \"HISTORIA_IDENTIFICACION.TXT\",\n",
    "    new_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a79217",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    \"GRP_FML_COTIZANTE_ID\", \"GRP_FML_AFILIADO_ID\", \"TPS_PRN_ID\",\n",
    "    \"GRP_FML_FECHA_INICIO\", \"GRP_FML_FECHA_FIN\", \"ENT_ID\"\n",
    "]\n",
    "\n",
    "def load_txt_from_zip(zip_path: str,\n",
    "                      txt_name: str,\n",
    "                      columns: list[str],\n",
    "                      sep: str = \",\",\n",
    "                      encoding: str = \"UTF-8\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Abre zip_path, extrae txt_name y lo carga en un DataFrame con las columnas dadas.\n",
    "    \"\"\"\n",
    "    with ZipFile(zip_path) as z:\n",
    "        with z.open(txt_name) as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=sep,\n",
    "                header=None,\n",
    "                dtype=str,\n",
    "                encoding=encoding\n",
    "            )\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "# ahora, para cada archivo ZIP basta con:\n",
    "Df_H_GF_EPSC25 = load_txt_from_zip(\n",
    "    R_Historico_Grupo_Familia_EPS025,\n",
    "    \"HISTORIA_GRUPO_FAMILIAR_E.TXT\",\n",
    "    new_columns\n",
    ")\n",
    "\n",
    "Df_H_GF_EPS025 = load_txt_from_zip(\n",
    "    R_Historico_Grupo_Familia_EPSC25,\n",
    "    \"HISTORIA_GRUPO_FAMILIAR.TXT\",\n",
    "    new_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede683b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = pd.read_csv(R_Ms_SIE, sep=';', dtype=str, encoding='ANSI')\n",
    "Df_Cod_DANE = pd.read_csv(R_Municipios_DANE, sep=';', dtype=str, encoding='utf-8')\n",
    "\n",
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO_2\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80356f1",
   "metadata": {},
   "source": [
    "## 1.1 Limpiar Bases de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df82a8",
   "metadata": {},
   "source": [
    "### 1.1.1 Expedientes SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_Expedientes_SIE.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])\n",
    "print( DF_Expedientes_SIE['Proceso'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf221326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros antes de limpiar expedientes:\", DF_Expedientes_SIE.shape[0])\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE[DF_Expedientes_SIE['Estado Expediente'] == 'Cerrado']\n",
    "print(\"Número de registros con 'Cerrado':\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24717a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])\n",
    "# 1) Convierte a datetime\n",
    "DF_Expedientes_SIE['Fecha Grabado'] = pd.to_datetime(\n",
    "    DF_Expedientes_SIE['Fecha Grabado'],\n",
    "    format='%Y/%m/%d %H:%M',  # ajusta si tu formato es distinto\n",
    "    errors='coerce'\n",
    ")\n",
    "DF_Expedientes_SIE['Fecha Cierre'] = pd.to_datetime(\n",
    "    DF_Expedientes_SIE['Fecha Cierre'],\n",
    "    format='%Y/%m/%d %H:%M',  # ajusta si tu formato es distinto\n",
    "    errors='coerce'\n",
    ")\n",
    "# 2) Ordena de más antiguo a más reciente\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE.sort_values('Fecha Grabado')\n",
    "\n",
    "# 3) Elimina duplicados, quedándote con el último (el más reciente)\n",
    "DF_Expedientes_SIE = DF_Expedientes_SIE.drop_duplicates(\n",
    "    subset=['Proceso', 'Tipo Documento', 'Número Identificación'],\n",
    "    keep='last'\n",
    ")\n",
    "# 4) Re-formatea la fecha a DD/MM/YYYY\n",
    "DF_Expedientes_SIE['Fecha Grabado'] = DF_Expedientes_SIE['Fecha Grabado'].dt.strftime('%d/%m/%Y')\n",
    "DF_Expedientes_SIE['Fecha Cierre'] = DF_Expedientes_SIE['Fecha Cierre'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "print(\"Número total de registros:\", DF_Expedientes_SIE.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203ffde",
   "metadata": {},
   "source": [
    "### 1.1.2 Maestro ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e7537",
   "metadata": {},
   "source": [
    "### 1.1.3 Historicos Identificación ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c02fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "Df_H_I = pd.concat(\n",
    "    [Df_H_I_EPS025, Df_H_I_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_H_I_EPS025, Df_H_I_EPSC25\n",
    "\n",
    "# 3. Ya tienes un único DataFrame con todos los registros:\n",
    "print(Df_H_I.columns)\n",
    "\n",
    "print(Df_H_I.shape)   # filas totales, 6 columnas\n",
    "Df_H_I = Df_H_I.drop_duplicates(subset=[\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "print(Df_H_I.shape)   # filas totales, 6 columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f323ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registros totales en Df_H_I:\", len(Df_H_I))\n",
    "\n",
    "# 1) Tus columnas clave\n",
    "key_cols = [\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# 2) Anti-join: todos los de DF_ADRES que NO están en Df_H_I\n",
    "anti = DF_ADRES.merge(\n",
    "    Df_H_I[key_cols],\n",
    "    on=key_cols,\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "missing = anti.loc[anti[\"_merge\"] == \"left_only\", DF_ADRES.columns]\n",
    "\n",
    "# 3) Elimina columnas duplicadas (por si DF_ADRES traía nombres repetidos)\n",
    "missing = missing.loc[:, ~missing.columns.duplicated()]\n",
    "\n",
    "# 4) Asegura que todas las columnas de Df_H_I estén en `missing` (rellena con NA las que falten)\n",
    "for col in Df_H_I.columns:\n",
    "    if col not in missing.columns:\n",
    "        missing[col] = pd.NA\n",
    "\n",
    "# 5) Reordena `missing` para que las columnas queden en el mismo orden que Df_H_I\n",
    "missing = missing[Df_H_I.columns]\n",
    "\n",
    "# 6) Concatenamos los registros faltantes\n",
    "Df_H_I = pd.concat([Df_H_I, missing], ignore_index=True)\n",
    "\n",
    "# 7) Eliminamos cualquier duplicado en base a las 3 claves, dejando el primero\n",
    "Df_H_I = Df_H_I.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "\n",
    "print(\"Registros totales en Df_H_I tras la ampliación:\", len(Df_H_I))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bf6e9",
   "metadata": {},
   "source": [
    "### 1.1.4 Historicos Grupo Familiar ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar ambos DataFrames uno debajo del otro\n",
    "Df_H_GF = pd.concat(\n",
    "    [Df_H_GF_EPS025, Df_H_GF_EPS025],\n",
    "    ignore_index=True,  # reindexa de 0…n-1\n",
    "    sort=False          # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_H_GF_EPSC25, Df_H_GF_EPS025\n",
    "\n",
    "print(\"Registros totales en Df_H_GF:\", len(Df_H_GF))\n",
    "# 3. Creamos una columna temporal con la fecha como datetime para comparaciones\n",
    "#    Usamos dayfirst=True porque el formato es dd/mm/yyyy\n",
    "Df_H_GF['_fecha_dt'] = pd.to_datetime(\n",
    "    Df_H_GF['GRP_FML_FECHA_INICIO'],\n",
    "    format='%d/%m/%Y',\n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "# 4. Ordenamos por esa fecha de manera descendente (el más reciente primero)\n",
    "#    y eliminamos duplicados dejando únicamente el primer registro de cada grupo\n",
    "Df_H_GF = (\n",
    "    Df_H_GF\n",
    "    .sort_values('_fecha_dt', ascending=False)\n",
    "    .drop_duplicates(subset='GRP_FML_AFILIADO_ID', keep='first')\n",
    "    .drop(columns='_fecha_dt')  # limpiamos la columna auxiliar\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 5. Mostrar resultado\n",
    "print(\"Columnas resultantes:\", Df_H_GF.columns.tolist())\n",
    "print(\"Registros totales en Df_H_GF:\", len(Df_H_GF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491790e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como tu ID es numérico o string, ajusta el tipo según corresponda:\n",
    "buscado = '112855395'   # o sin comillas si tu columna es numérica: buscado = 112855395\n",
    "# Filtrar\n",
    "resultado = Df_H_GF.loc[Df_H_GF['GRP_FML_AFILIADO_ID'] == buscado]\n",
    "# Imprimir\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518702e",
   "metadata": {},
   "source": [
    "### 1.1.5 Maestro SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = Df_SIE.merge(\n",
    "    Df_H_I[[\"AFL_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]],\n",
    "    left_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "Df_SIE = Df_SIE.drop([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE = Df_SIE.drop(\n",
    "    columns=[\"primer_nombre_bdua\", \"segundo_nombre_bdua\", \"primer_apellido_bdua\", \"segundo_apellido_bdua\"],\n",
    "    errors=\"ignore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Parentesco_SIE\n",
    "mapping_parentesco = dict(zip(Df_Parentesco_SIE[\"Parentesco\"], Df_Parentesco_SIE[\"Cod_Resolu_762_2023\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"parentesco_codificado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"parentesco_codificado\"] = Df_SIE[\"parentesco_codificado\"].map(mapping_parentesco)\n",
    "\n",
    "del Df_Parentesco_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Estado_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Estado_SIE[\"Estado_SIE\"], Df_Estado_SIE[\"Cod_Resolu_762_2023\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"estado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"estado\"] = Df_SIE[\"estado\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Estado_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Regimen_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Regimen_SIE[\"regimen_SIE\"], Df_Regimen_SIE[\"Regimen_ADRES\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"regimen\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"regimen\"] = Df_SIE[\"regimen\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Regimen_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos la columna \"municipio\" de df_Municipios_SIE a \"ID_COD_municipio\"\n",
    "df_munis = df_Municipios_SIE.rename(columns={\"municipio\": \"ID_COD_municipio\"})[[\"descripcion\", \"ID_COD_municipio\"]]\n",
    "\n",
    "# Hacemos merge entre Df_SIE y df_munis usando Df_SIE[\"municipio\"] y df_munis[\"descripcion\"]\n",
    "Df_SIE = Df_SIE.merge(df_munis, left_on=\"municipio\", right_on=\"descripcion\", how=\"left\")\n",
    "\n",
    "# Eliminamos la columna \"descripcion\" que ya no es necesaria\n",
    "Df_SIE.drop(\"descripcion\", axis=1, inplace=True)\n",
    "\n",
    "# Reordenamos las columnas para que \"ID_COD_municipio\" quede justo a la derecha de \"municipio\"\n",
    "cols = list(Df_SIE.columns)\n",
    "idx = cols.index(\"municipio\")\n",
    "cols.remove(\"ID_COD_municipio\")\n",
    "cols.insert(idx + 1, \"ID_COD_municipio\")\n",
    "Df_SIE = Df_SIE[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"fecha_nacimiento\" a datetime usando el formato original, \n",
    "# y formatearla a dd/mm/yyyy; asigna \"Fecha no validad\" en caso de error.\n",
    "Df_SIE[\"fecha_nacimiento\"] = pd.to_datetime(\n",
    "    Df_SIE[\"fecha_nacimiento\"],\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\"\n",
    ").apply(lambda x: x.strftime(\"%d/%m/%Y\") if pd.notnull(x) else \"Fecha no validad\")\n",
    "\n",
    "# Contar cuántos registros tienen \"Fecha no validad\" y mostrar el resultado\n",
    "invalid_count = (Df_SIE[\"fecha_nacimiento\"] == \"Fecha no validad\").sum()\n",
    "print(\"Número de registros con fecha no válida:\", invalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3881a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un diccionario de mapeo a partir de Df_Tipo_Afiliados_SIE\n",
    "mapping_Estado_SIE = dict(zip(Df_Tipo_Afiliados_SIE[\"Tipo_afiliado_SIE\"], Df_Tipo_Afiliados_SIE[\"Tipo_afiliado_ADRES\"]))\n",
    "\n",
    "# Sustituimos los valores en la columna \"tipo_afiliado\" de Df_SIE usando el diccionario\n",
    "Df_SIE[\"tipo_afiliado\"] = Df_SIE[\"tipo_afiliado\"].map(mapping_Estado_SIE)\n",
    "\n",
    "del Df_Tipo_Afiliados_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640fe386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Lista de columnas que quieres “traer” desde DF_ADRES\n",
    "columnas = [\n",
    "    \"ENT_ID\",\n",
    "    \"TPS_IDN_ID_CF\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_CF\",\n",
    "    \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\",\n",
    "    \"AFL_MUNICIPIO_NACIMIENTO\",\n",
    "    \"AFL_DISCAPACIDAD\",\n",
    "    \"TPS_AFL_ID\",\n",
    "    \"TPS_PRN_ID\",\n",
    "    \"TPS_GRP_PBL_ID\",\n",
    "    \"TPS_NVL_SSB_ID\",\n",
    "    \"TPS_CND_BNF_ID\",\n",
    "    \"DPR_ID\",\n",
    "    \"MNC_ID\",\n",
    "    \"TPS_EST_AFL_ID\",\n",
    "    \"CND_AFL_FECHA_INICIO\",\n",
    "    \"PORTABILIDAD\",\n",
    "    \"COD_IPS_P\",\n",
    "    \"MTDLG_G_P\",\n",
    "    \"SUB_SISBEN_IV\",\n",
    "    \"MARCASISBENIV+MARCASISBENIII\",\n",
    "    \"CRUCE_BDEX_RNEC\",\n",
    "]\n",
    "\n",
    "# 2) Hacemos un copy para no tocar el original\n",
    "df_merged = Df_SIE.copy()\n",
    "\n",
    "# 3) Merge “left” por AFL_ID; todas las columnas nuevas vendrán con sufijo \"_adres\"\n",
    "df_merged = df_merged.merge(\n",
    "    DF_ADRES[['AFL_ID'] + columnas],\n",
    "    on='AFL_ID',\n",
    "    how='left',\n",
    "    suffixes=('', '_adres')\n",
    ")\n",
    "\n",
    "# 4) Para cada campo en nuestra lista:\n",
    "#    - Si existe la columna \"{col}_adres\", rellenamos NaN de la original con el valor de _adres\n",
    "#    - Luego eliminamos la columna auxiliar \"_adres\"\n",
    "for col in columnas:\n",
    "    col_adres = f\"{col}_adres\"\n",
    "    if col_adres in df_merged.columns:\n",
    "        # fillna en la columna original con los valores _adres\n",
    "        df_merged[col] = df_merged[col_adres].fillna(df_merged.get(col))\n",
    "        # eliminamos la auxiliar\n",
    "        df_merged.drop(columns=col_adres, inplace=True)\n",
    "\n",
    "# 5) Ya tienes el resultado “fusionado” en df_merged; si quieres, reasignas:\n",
    "Df_SIE = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Selecciona sólo las columnas que quieres traer\n",
    "cols_to_add = [\n",
    "    \"GRP_FML_AFILIADO_ID\",\n",
    "    \"GRP_FML_COTIZANTE_ID\",\n",
    "    \"TPS_PRN_ID\",\n",
    "    \"GRP_FML_FECHA_INICIO\",\n",
    "    \"GRP_FML_FECHA_FIN\"\n",
    "]\n",
    "\n",
    "# 2) Haz el merge de Df_SIE con Df_H_GF\n",
    "Df_SIE = Df_SIE.merge(\n",
    "    Df_H_GF[cols_to_add],\n",
    "    left_on=\"AFL_ID\",\n",
    "    right_on=\"GRP_FML_AFILIADO_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3) Como ya no necesitas la columna auxiliar, la quitas\n",
    "Df_SIE.drop(columns=\"GRP_FML_AFILIADO_ID\", inplace=True)\n",
    "\n",
    "# 4) Eliminar dataframer para limpiar memoria\n",
    "del Df_H_GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_SIE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14255ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_SIE[\"tipo_afiliado\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f35e12",
   "metadata": {},
   "source": [
    "# 2 Validaciones y Novedades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449a45b",
   "metadata": {},
   "source": [
    "## 2.1 Duplicados SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e672398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna \"Validar_Duplicidad\" según las condiciones solicitadas\n",
    "cond_duplicado = Df_SIE[\"AFL_ID\"].notna() & (Df_SIE[\"AFL_ID\"] != \"\") & (Df_SIE.duplicated(\"AFL_ID\", keep=False))\n",
    "cond_unico = Df_SIE[\"AFL_ID\"].notna() & (Df_SIE[\"AFL_ID\"] != \"\") & (~Df_SIE.duplicated(\"AFL_ID\", keep=False))\n",
    "cond_sin_id = Df_SIE[\"AFL_ID\"].isna() | (Df_SIE[\"AFL_ID\"] == \"\")\n",
    "\n",
    "Df_SIE[\"Validar_Duplicidad\"] = np.select(\n",
    "    [cond_duplicado, cond_unico, cond_sin_id],\n",
    "    [\"Registro Duplicado SIE\", \"Registro unnico SIE\", \"Registro Sin ID\"],\n",
    "    default=\"—sin clasificar—\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c770fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Asegurar que ENT_ID no tenga valores nulos\n",
    "Df_SIE[\"ENT_ID\"] = Df_SIE[\"ENT_ID\"].fillna(\"Sin dato\")\n",
    "\n",
    "# Agrupar por Validar_Duplicidad y ENT_ID\n",
    "conteo = Df_SIE.groupby([\"Validar_Duplicidad\", \"ENT_ID\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Filtrar solo las categorías de interés en filas\n",
    "categorias_interes = [\"Registro Duplicado SIE\", \"Registro Sin ID\"]\n",
    "conteo_filtrado = conteo.loc[categorias_interes]\n",
    "\n",
    "# Ordenar las columnas de forma personalizada si deseas\n",
    "orden_columnas = [\"EPS025\", \"EPSC25\", \"Sin dato\"]\n",
    "conteo_filtrado = conteo_filtrado[[col for col in orden_columnas if col in conteo_filtrado.columns]]\n",
    "\n",
    "# Colores dinámicos\n",
    "colores = cm.get_cmap('Set2', len(conteo_filtrado.columns)).colors\n",
    "\n",
    "# Crear gráfico\n",
    "ax = conteo_filtrado.plot(\n",
    "    kind='bar',\n",
    "    figsize=(12, 6),\n",
    "    color=colores,\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "# Títulos y etiquetas\n",
    "plt.title(\"Distribución de Registros Problemáticos por Tipo de ENT_ID\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Tipo de registro\", fontsize=13)\n",
    "plt.ylabel(\"Cantidad de registros\", fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"ENT_ID\", fontsize=10, title_fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Agregar etiquetas numéricas sobre las barras con precisión\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(\n",
    "            rect.get_x() + rect.get_width() / 2,  # centro de la barra\n",
    "            height + max(conteo_filtrado.values.flatten()) * 0.01,  # ligeramente encima\n",
    "            f'{int(height):,}',  # formateado con coma si quieres: f\"{height:,}\"\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontweight='bold',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7850eb6",
   "metadata": {},
   "source": [
    "## 2.2 Validar Estado SIE VS ADRES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa701432",
   "metadata": {},
   "source": [
    "### 2.2.1 N14 Activo Retirado SIE, Fallecido ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Retirar = Df_SIE[\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\") &\n",
    "    (Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"TPS_EST_AFL_ID\"].isin([\"RE\", \"SM\", \"SD\"]))\n",
    "]\n",
    "\n",
    "print(\"Número de registros:\", DF_Retirar.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas de DF_ADRES que se requieren para el nuevo dataframe\n",
    "cols_adres = [\n",
    "    \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\",\n",
    "    \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"CND_AFL_FECHA_INICIO\"\n",
    "]\n",
    "\n",
    "# Realizamos merge de DF_Retirar con DF_ADRES usando el id: \n",
    "# DF_Retirar[\"tipo_documento\"] corresponde a DF_ADRES[\"TPS_IDN_ID\"]\n",
    "# DF_Retirar[\"numero_identificacion\"] corresponde a DF_ADRES[\"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "df_merged = DF_Retirar.merge(\n",
    "    DF_ADRES[cols_adres],\n",
    "    left_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_adres\")\n",
    ")\n",
    "\n",
    "# Asegurarse de que \"CND_AFL_FECHA_INICIO\" sea una Serie unidimensional:\n",
    "if isinstance(df_merged[\"CND_AFL_FECHA_INICIO\"], pd.DataFrame):\n",
    "    cnd_fecha = df_merged[\"CND_AFL_FECHA_INICIO\"].iloc[:, 0]\n",
    "else:\n",
    "    cnd_fecha = df_merged[\"CND_AFL_FECHA_INICIO\"]\n",
    "\n",
    "# Construir el DataFrame df_N14 con los campos indicados.\n",
    "df_N14 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": list(range(1, len(df_merged) + 1)),\n",
    "    \"ENT_ID\": df_merged[\"ENT_ID\"].tolist(),\n",
    "    \"TPS_IDN_ID\": df_merged[\"TPS_IDN_ID\"].tolist(),\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_merged[\"HST_IDN_NUMERO_IDENTIFICACION\"].tolist(),\n",
    "    \"AFL_PRIMER_APELLIDO\": df_merged[\"AFL_PRIMER_APELLIDO\"].tolist(),\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_merged[\"AFL_SEGUNDO_APELLIDO\"].tolist(),\n",
    "    \"AFL_PRIMER_NOMBRE\": df_merged[\"AFL_PRIMER_NOMBRE\"].tolist(),\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_merged[\"AFL_SEGUNDO_NOMBRE\"].tolist(),\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_merged[\"AFL_FECHA_NACIMIENTO\"].tolist(),\n",
    "    \"DPR_ID\": df_merged[\"DPR_ID\"].tolist(),\n",
    "    \"MNC_ID\": df_merged[\"MNC_ID\"].tolist(),\n",
    "    \"NOVEDAD\": [\"N09\"] * len(df_merged),\n",
    "    \"FECHA_NOVEDAD\": cnd_fecha.tolist(),\n",
    "    \"COD_1_NOVEDAD\": [\"RE\"] * len(df_merged),\n",
    "    \"COD_2_NOVEDAD\": [\"4\"] * len(df_merged),\n",
    "    \"COD_3_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_4_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_5_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_6_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_7_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"Motivo\": [\"Activos SIE: RE, DS, ADRES \"] * len(df_merged),\n",
    "})\n",
    "\n",
    "print(\"Número de registros en DF_Retirar:\", DF_Retirar.shape[0])\n",
    "print(\"Número de registros en df_N14:\", df_N14.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36cebe8",
   "metadata": {},
   "source": [
    "### 2.2.2 N09 Activo Retirado SIE, Fallecido ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a107e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Fallecidos = Df_SIE[\n",
    "    (Df_SIE[\"estado\"].isin([\"AC\", \"RE\"])) &\n",
    "    #(Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\") &\n",
    "    #(Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"TPS_EST_AFL_ID\"].isin([\"AF\"]))\n",
    "]\n",
    "\n",
    "print(\"Número de registros:\", DF_Fallecidos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas de DF_ADRES que se requieren para el nuevo dataframe\n",
    "cols_adres = [\n",
    "    \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\",\n",
    "    \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"CND_AFL_FECHA_INICIO\"\n",
    "]\n",
    "\n",
    "# Realizamos merge de DF_Fallecidos con DF_ADRES usando el id: \n",
    "# DF_Fallecidos[\"tipo_documento\"] corresponde a DF_ADRES[\"TPS_IDN_ID\"]\n",
    "# DF_Fallecidos[\"numero_identificacion\"] corresponde a DF_ADRES[\"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "df_merged = DF_Fallecidos.merge(\n",
    "    DF_ADRES[cols_adres],\n",
    "    left_on=[\"tipo_documento\", \"numero_identificacion\"],\n",
    "    right_on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_adres\")\n",
    ")\n",
    "\n",
    "\n",
    "# Construir el DataFrame df_N09 con los campos indicados.\n",
    "df_N09 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": list(range(1, len(df_merged) + 1)),\n",
    "    \"ENT_ID\": df_merged[\"ENT_ID\"].tolist(),\n",
    "    \"TPS_IDN_ID\": df_merged[\"TPS_IDN_ID\"].tolist(),\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_merged[\"HST_IDN_NUMERO_IDENTIFICACION\"].tolist(),\n",
    "    \"AFL_PRIMER_APELLIDO\": df_merged[\"AFL_PRIMER_APELLIDO\"].tolist(),\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_merged[\"AFL_SEGUNDO_APELLIDO\"].tolist(),\n",
    "    \"AFL_PRIMER_NOMBRE\": df_merged[\"AFL_PRIMER_NOMBRE\"].tolist(),\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_merged[\"AFL_SEGUNDO_NOMBRE\"].tolist(),\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_merged[\"AFL_FECHA_NACIMIENTO\"].tolist(),\n",
    "    \"DPR_ID\": df_merged[\"DPR_ID\"].tolist(),\n",
    "    \"MNC_ID\": df_merged[\"MNC_ID\"].tolist(),\n",
    "    \"NOVEDAD\": [\"N09\"] * len(df_merged),\n",
    "    \"FECHA_NOVEDAD\": df_merged[\"CND_AFL_FECHA_INICIO\"].tolist(),\n",
    "    \"COD_1_NOVEDAD\": [\"\"] * len(df_merged),\n",
    "    \"COD_2_NOVEDAD\": [\"\"] * len(df_merged),\n",
    "    \"COD_3_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_4_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_5_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_6_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"COD_7_NOVEDAD\": [\"\" for _ in range(len(df_merged))],\n",
    "    \"Motivo\": [\"Activos SIE: AF ADRES \"] * len(df_merged),\n",
    "})\n",
    "\n",
    "print(\"Número de registros en DF_Fallecidos:\", DF_Fallecidos.shape[0])\n",
    "print(\"Número de registros en df_N09:\", df_N09.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f87fa",
   "metadata": {},
   "source": [
    "### 2.2.2 activo SIE, No existe en ADRES con nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b21f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    ((Df_SIE[\"AFL_ID\"].isna()) | (Df_SIE[\"AFL_ID\"] == \"\")) &\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (~Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\")\n",
    ")\n",
    "\n",
    "# 2. Define la lista de columnas que quieres conservar en el nuevo df\n",
    "cols = [\n",
    "    \"regimen\",\n",
    "    \"tipo_documento\",\n",
    "    \"numero_identificacion\",\n",
    "    \"primer_apellido\",\n",
    "    \"segundo_apellido\",\n",
    "    \"primer_nombre\",\n",
    "    \"segundo_nombre\",\n",
    "    \"fecha_nacimiento\",\n",
    "    \"municipio\",\n",
    "    \"AFL_ID\"\n",
    "]\n",
    "# 3. Aplica el filtro y selecciona solo las columnas deseadas\n",
    "DF_Retirar = Df_SIE.loc[mask, cols].copy()\n",
    "print(\"Número de registros en DF_Retirar:\", DF_Retirar.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1cfe0",
   "metadata": {},
   "source": [
    "Identifico que usuarios del SIE no cruzan con historico de identificación y por ende no tienen ID_BDUA y estan Activos en SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_sin_AFL_ID = Df_SIE[\n",
    "    ((Df_SIE[\"AFL_ID\"].isna()) | (Df_SIE[\"AFL_ID\"] == \"\")) &\n",
    "    (Df_SIE[\"estado\"] == \"AC\") &\n",
    "    (Df_SIE[\"regimen\"] == \"EPS025\") &\n",
    "    (~Df_SIE[\"ENT_ID\"].isin([\"EPS025\", \"EPSC25\"])) &\n",
    "    (Df_SIE[\"estado_traslado\"] == \"No Aplica\")\n",
    "    ]\n",
    "print(\"Número de registros en DF_Retirar:\", Df_sin_AFL_ID.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505a958",
   "metadata": {},
   "source": [
    "## 2.3 Validar I02[ Serial] SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd50024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define la lista de columnas que quieres conservar en el nuevo df\n",
    "cols = [\n",
    "    \"regimen\",\n",
    "    \"tipo_documento\",\n",
    "    \"numero_identificacion\",\n",
    "    \"primer_apellido\",\n",
    "    \"segundo_apellido\",\n",
    "    \"primer_nombre\",\n",
    "    \"segundo_nombre\",\n",
    "    \"fecha_nacimiento\",\n",
    "    \"municipio\",\n",
    "    \"AFL_ID\"\n",
    "]\n",
    "\n",
    "# 2. Construye la máscara de filtrado\n",
    "mask = (\n",
    "    # AFL_ID no es nulo\n",
    "    Df_SIE[\"AFL_ID\"].notna()\n",
    "    # AFL_ID no está vacío (por si hay cadenas vacías o solo espacios)\n",
    "    & (Df_SIE[\"AFL_ID\"].astype(str).str.strip() != \"\")\n",
    "    # serial_fosyga y AFL_ID son distintos\n",
    "    & (Df_SIE[\"serial_fosyga\"] != Df_SIE[\"AFL_ID\"])\n",
    ")\n",
    "\n",
    "# 3. Aplica el filtro y selecciona solo las columnas deseadas\n",
    "df_I02 = Df_SIE.loc[mask, cols].copy()\n",
    "\n",
    "# 4. (Opcional) Reinicia el índice del nuevo DataFrame\n",
    "df_I02.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Ya lo tienes:\n",
    "print(df_I02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0797677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Inserta 'NUM_SOLICITUD_NOVEDAD' como primera columna, con valores 1…n\n",
    "df_I02.insert(\n",
    "    loc=0,\n",
    "    column=\"NUM_SOLICITUD_NOVEDAD\",\n",
    "    value=range(1, len(df_I02) + 1)\n",
    ")\n",
    "\n",
    "# 2. Sustituye en 'regimen'\n",
    "df_I02[\"regimen\"] = df_I02[\"regimen\"].replace({\n",
    "    \"Subsidiado\": \"EPS025\",\n",
    "    \"Contributivo\": \"EPSC25\"\n",
    "})\n",
    "\n",
    "# 3. Renombra 'AFL_ID' a 'COD_1_NOVEDAD'\n",
    "df_I02 = df_I02.rename(columns={\"AFL_ID\": \"COD_1_NOVEDAD\"})\n",
    "\n",
    "# 4. Crea las nuevas columnas (todas al final por ahora)\n",
    "df_I02[\"DPR_ID\"]         = \"\"                             # vacía\n",
    "df_I02[\"MNC_ID\"]         = \"\"                             # vacía\n",
    "df_I02[\"NOVEDAD\"]        = \"I02\"                          # valor fijo\n",
    "df_I02[\"FECHA_NOVEDAD\"]  = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "# Columnas COD_2_NOVEDAD … COD_7_NOVEDAD vacías\n",
    "for i in range(2, 8):\n",
    "    df_I02[f\"COD_{i}_NOVEDAD\"] = \"\"\n",
    "\n",
    "# 5. Reordena para obtener exactamente las 20 columnas en el orden pedido\n",
    "df_I02 = df_I02[\n",
    "    [\n",
    "        \"NUM_SOLICITUD_NOVEDAD\",\n",
    "        \"regimen\",\n",
    "        \"tipo_documento\",\n",
    "        \"numero_identificacion\",\n",
    "        \"primer_apellido\",\n",
    "        \"segundo_apellido\",\n",
    "        \"primer_nombre\",\n",
    "        \"segundo_nombre\",\n",
    "        \"fecha_nacimiento\",\n",
    "        \"DPR_ID\",\n",
    "        \"MNC_ID\",\n",
    "        \"NOVEDAD\",\n",
    "        \"FECHA_NOVEDAD\",\n",
    "        \"COD_1_NOVEDAD\",\n",
    "        \"COD_2_NOVEDAD\",\n",
    "        \"COD_3_NOVEDAD\",\n",
    "        \"COD_4_NOVEDAD\",\n",
    "        \"COD_5_NOVEDAD\",\n",
    "        \"COD_6_NOVEDAD\",\n",
    "        \"COD_7_NOVEDAD\",\n",
    "        \"municipio\"\n",
    "    ]\n",
    "]\n",
    "# 6. Convierte a datetime (ajusta el formato si ya viene como cadena):\n",
    "df_I02['fecha_nacimiento'] = pd.to_datetime(\n",
    "    df_I02['fecha_nacimiento'],\n",
    "    format='%Y-%m-%d',      # o el formato que realmente tengas; quita este argumento si varía\n",
    "    errors='coerce'         # convierte lo que no encaje en NaT\n",
    ")\n",
    "# 7. Ahora formatea al string DD/MM/YYYY:\n",
    "df_I02['fecha_nacimiento'] = df_I02['fecha_nacimiento'].dt.strftime('%d/%m/%Y')\n",
    "# Listo: ahora `df_I02` tiene 20 columnas en el orden y con los valores solicitados.\n",
    "print(df_I02.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar merge entre df_I02 y df_Municipios_SIE\n",
    "# Suponemos que en df_I02 la columna que contiene el id es \"municipio\"\n",
    "df_I02 = df_I02.merge(\n",
    "    df_Municipios_SIE[[\"descripcion\", \"municipio\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"municipio\",\n",
    "    right_on=\"descripcion\"\n",
    ")\n",
    "\n",
    "# Por el merge, la columna de df_Municipios_SIE se renombrará a \"municipio_y\" y la original en df_I02 es \"municipio_x\".\n",
    "# Usamos la columna \"municipio_y\" para extraer el código.\n",
    "df_I02[\"DPR_ID\"] = df_I02[\"municipio_y\"].str[:2]    #Primeros dos dígitos: código del departamento\n",
    "df_I02[\"MNC_ID\"] = df_I02[\"municipio_y\"].str[2:]    #Últimos tres dígitos: código del municipio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d31143",
   "metadata": {},
   "source": [
    "## 2.4 Validar Grupos Familiares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asumimos que Df_SIE ya tiene estas cuatro columnas:\n",
    "#   SIE: tipo_documento_padre, numero_documento_padre\n",
    "#   ADRES: TPS_IDN_ID_CF, HST_IDN_NUMERO_IDENTIFICACION_CF\n",
    "\n",
    "# 1) Normaliza los “vacíos” para unificar: conviértelos a np.nan\n",
    "for c in [\"tipo_documento_padre\", \"numero_documento_padre\",\n",
    "          \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"]:\n",
    "    Df_SIE[c] = Df_SIE[c].replace(\"\", np.nan)\n",
    "\n",
    "# 2) Prepara las condiciones en orden\n",
    "condiciones = [\n",
    "    # todos vacíos → es el mismo afiliado, o sea, \"él\" es cabeza\n",
    "    Df_SIE[[\"tipo_documento_padre\", \"numero_documento_padre\",\n",
    "            \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"]]\n",
    "      .isna().all(axis=1),\n",
    "    # ambos lados tienen valor y coinciden\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna() &\n",
    "    (Df_SIE[\"tipo_documento_padre\"] == Df_SIE[\"TPS_IDN_ID_CF\"]) &\n",
    "    (Df_SIE[\"numero_documento_padre\"] == Df_SIE[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"]),\n",
    "    # ambos lados tienen valor pero NO coinciden\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna() &\n",
    "    ((Df_SIE[\"tipo_documento_padre\"] != Df_SIE[\"TPS_IDN_ID_CF\"]) |\n",
    "     (Df_SIE[\"numero_documento_padre\"] != Df_SIE[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"])),\n",
    "    # SIE tiene CF, ADRES no\n",
    "    Df_SIE[\"tipo_documento_padre\"].notna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].isna(),\n",
    "    # ADRES tiene CF, SIE no\n",
    "    Df_SIE[\"tipo_documento_padre\"].isna() &\n",
    "    Df_SIE[\"TPS_IDN_ID_CF\"].notna()\n",
    "]\n",
    "\n",
    "resultados = [\n",
    "    \"Es Cabeza de familia\",\n",
    "    \"Cabeza de familia igual en SIE y en ADRES\",\n",
    "    \"Cabeza de familia diferente en SIE y en ADRES\",\n",
    "    \"SIE con cabeza familiar, ADRES sin cabeza de familia\",\n",
    "    \"ADRES con cabeza de familia, SIE sin cabeza de familia\"\n",
    "]\n",
    "\n",
    "# 3) Crea la columna Validar_CF con np.select\n",
    "Df_SIE[\"Validar_CF\"] = np.select(condiciones, resultados, default=\"—sin clasificar—\")\n",
    "\n",
    "# 4) Imprimir cuántos registros hay por cada categoría en la columna Validar_CF\n",
    "conteos = Df_SIE[\"Validar_CF\"].value_counts()\n",
    "\n",
    "print(\"Distribución de la validación de cabezas de familia:\")\n",
    "for categoria, cantidad in conteos.items():\n",
    "    print(f\"  {categoria}: {cantidad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3078423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear tabla pivote para contar las inconsistencias por régimen\n",
    "counts = pd.pivot_table(\n",
    "    Df_SIE,\n",
    "    index=\"regimen\",\n",
    "    columns=\"Validar_CF\",\n",
    "    values=\"AFL_ID\",  # cualquier columna sirve para contar\n",
    "    aggfunc=\"count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Simplificar seleccionando solo las categorías de interés\n",
    "categorias_interes = [\n",
    "    \"Cabeza de familia diferente en SIE y en ADRES\",\n",
    "    \"SIE con cabeza familiar, ADRES sin cabeza de familia\",\n",
    "    \"ADRES con cabeza de familia, SIE sin cabeza de familia\"\n",
    "]\n",
    "counts = counts[categorias_interes]\n",
    "\n",
    "# 1) Calculamos proporciones\n",
    "pct = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# 2) Dibujamos barras apiladas al 100%\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "pct.plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    ax=ax,\n",
    "    color=[\"C0\",\"C2\",\"C1\"],\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "# 3) Etiquetas de porcentaje en cada segmento\n",
    "for i, regimen in enumerate(pct.index):\n",
    "    left = 0\n",
    "    for cat in pct.columns:\n",
    "        w = pct.loc[regimen, cat]\n",
    "        if w > 5:  # sólo anotar donde haya suficiente espacio\n",
    "            ax.text(\n",
    "                left + w/2, i,\n",
    "                f\"{w:.0f}%\",\n",
    "                va=\"center\", ha=\"center\",\n",
    "                color=\"white\", fontsize=9\n",
    "            )\n",
    "        left += w\n",
    "\n",
    "# 4) Ajustes\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_xlabel(\"Porcentaje de inconsistencias\")\n",
    "ax.set_title(\"Proporción de inconsistencias de Cabeza de Familia\\npor Régimen\")\n",
    "ax.legend(title=\"Tipo de inconsistencia\", bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01963812",
   "metadata": {},
   "source": [
    "## 2.3. Tipo de documento diferente\n",
    "Evoluciones efectivas en ADRES y no en SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a82150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrar los registros donde tipo_documento y TPS_IDN_ID son diferentes y TPS_IDN_ID no está vacío\n",
    "mask = (\n",
    "    (Df_SIE[\"tipo_documento\"] != Df_SIE[\"TPS_IDN_ID\"]) &\n",
    "    (Df_SIE[\"TPS_IDN_ID\"].notna()) &\n",
    "    (Df_SIE[\"Validar_Duplicidad\"].astype(str).str.strip() == \"Registro unnico SIE\") &\n",
    "    (Df_SIE[\"TPS_IDN_ID\"].astype(str).str.strip() != \"\")\n",
    ")\n",
    "\n",
    "# 2. Seleccionar y renombrar las columnas necesarias\n",
    "df_tmp = Df_SIE.loc[mask, [\n",
    "    \"ENT_ID\", \"tipo_documento\", \"numero_identificacion\", \"primer_apellido\",\n",
    "    \"segundo_apellido\", \"primer_nombre\", \"segundo_nombre\", \"fecha_nacimiento\",\n",
    "    \"ID_COD_municipio\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_FECHA_NACIMIENTO\"\n",
    "]].copy()\n",
    "\n",
    "# 3. Separar ID_COD_municipio en DPR_ID y MNC_ID\n",
    "df_tmp[\"DPR_ID\"] = df_tmp[\"ID_COD_municipio\"].str[:2]\n",
    "df_tmp[\"MNC_ID\"] = df_tmp[\"ID_COD_municipio\"].str[2:]\n",
    "\n",
    "# 4. Insertar columnas fijas\n",
    "df_tmp[\"NOVEDAD\"] = \"N01\"\n",
    "df_tmp[\"CND_AFL_FECHA_INICIO\"] = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# 5. Reordenar columnas según lo solicitado\n",
    "df_N01 = pd.DataFrame({\n",
    "    \"NUM_SOLICITUD_NOVEDAD\": range(1, len(df_tmp) + 1),\n",
    "    \"ENT_ID\": df_tmp[\"ENT_ID\"],\n",
    "    \"TPS_IDN_ID\": df_tmp[\"tipo_documento\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\": df_tmp[\"numero_identificacion\"],\n",
    "    \"AFL_PRIMER_APELLIDO\": df_tmp[\"primer_apellido\"],\n",
    "    \"AFL_SEGUNDO_APELLIDO\": df_tmp[\"segundo_apellido\"],\n",
    "    \"AFL_PRIMER_NOMBRE\": df_tmp[\"primer_nombre\"],\n",
    "    \"AFL_SEGUNDO_NOMBRE\": df_tmp[\"segundo_nombre\"],\n",
    "    \"AFL_FECHA_NACIMIENTO\": df_tmp[\"fecha_nacimiento\"],\n",
    "    \"DPR_ID\": df_tmp[\"DPR_ID\"],\n",
    "    \"MNC_ID\": df_tmp[\"MNC_ID\"],\n",
    "    \"NOVEDAD\": df_tmp[\"NOVEDAD\"],\n",
    "    \"CND_AFL_FECHA_INICIO\": df_tmp[\"CND_AFL_FECHA_INICIO\"],\n",
    "    \"COD_1_NOVEDAD\": df_tmp[\"TPS_IDN_ID\"],\n",
    "    \"COD_2_NOVEDAD\": df_tmp[\"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    \"COD_3_NOVEDAD\": df_tmp[\"AFL_FECHA_NACIMIENTO\"],\n",
    "    \"COD_4_NOVEDAD\": \"0\",\n",
    "    \"COD_5_NOVEDAD\": \"\",\n",
    "    \"COD_6_NOVEDAD\": \"\",\n",
    "    \"COD_7_NOVEDAD\": \"\",\n",
    "    \"Motivo\": \"Evolucion de documento SIE\"\n",
    "})\n",
    "\n",
    "# 6. (Opcional) Revisar el resultado\n",
    "print(df_N01.head())\n",
    "print(df_N01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e357443",
   "metadata": {},
   "source": [
    "# 2.3.1 Reportar evolucion a ADRES.\n",
    "Validando que en SIE el tipo de documento este bien pero no en ADRES. para identificar los afiliados y reportarlos ante ADRES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los pares de evolución válidos\n",
    "evoluciones_validas = {\n",
    "    (\"CN\", \"RC\"),\n",
    "    (\"RC\", \"TI\"),\n",
    "    (\"TI\", \"CC\"),\n",
    "    (\"PE\", \"PT\"),\n",
    "}\n",
    "\n",
    "# Verificar cuántos registros son inconsistentes\n",
    "print(f\"Total de registros: {len(df_N01)}\")\n",
    "\n",
    "# 1. Detectar registros con evolución actual VÁLIDA\n",
    "mask_evolucion_valida = df_N01.apply(\n",
    "    lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 2. Detectar registros con evolución actual INVÁLIDA\n",
    "mask_evolucion_invalida = ~mask_evolucion_valida\n",
    "\n",
    "# 3. De los inválidos, detectar cuáles se pueden corregir intercambiando\n",
    "mask_corregible_intercambiando = df_N01.apply(\n",
    "    lambda row: (row[\"COD_1_NOVEDAD\"], row[\"TPS_IDN_ID\"]) in evoluciones_validas,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 4. Máscara final: registros inválidos que se pueden corregir intercambiando\n",
    "mask_a_intercambiar = mask_evolucion_invalida & mask_corregible_intercambiando\n",
    "\n",
    "print(f\"Registros con evolución válida: {mask_evolucion_valida.sum()}\")\n",
    "print(f\"Registros con evolución inválida: {mask_evolucion_invalida.sum()}\")\n",
    "print(f\"Registros inválidos que se pueden corregir intercambiando: {mask_a_intercambiar.sum()}\")\n",
    "print(f\"Registros inválidos que NO se pueden corregir: {(mask_evolucion_invalida & ~mask_corregible_intercambiando).sum()}\")\n",
    "\n",
    "# 5. Intercambiar SOLO los registros que necesitan corrección\n",
    "if mask_a_intercambiar.any():\n",
    "    # Intercambiar valores\n",
    "    df_N01.loc[mask_a_intercambiar, [\"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]] = \\\n",
    "        df_N01.loc[mask_a_intercambiar, [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"COD_1_NOVEDAD\", \"COD_2_NOVEDAD\"]].values\n",
    "    \n",
    "    # Cambiar el motivo SOLO para estos registros\n",
    "    df_N01.loc[mask_a_intercambiar, \"Motivo\"] = \"Reportar novedad ante ADRES\"\n",
    "    \n",
    "    print(f\"Se intercambiaron {mask_a_intercambiar.sum()} registros para corregir evoluciones\")\n",
    "else:\n",
    "    print(\"No se encontraron registros que requieran intercambio\")\n",
    "\n",
    "# Verificar los motivos después del cambio\n",
    "print(\"\\nDistribución de motivos:\")\n",
    "print(df_N01[\"Motivo\"].value_counts())\n",
    "\n",
    "# Verificación final: mostrar algunos ejemplos\n",
    "print(\"\\nEjemplos de evoluciones después del procesamiento:\")\n",
    "print(\"Evoluciones válidas (primeros 5):\")\n",
    "validas_final = df_N01[df_N01.apply(lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas, axis=1)]\n",
    "if len(validas_final) > 0:\n",
    "    print(validas_final[[\"TPS_IDN_ID\", \"COD_1_NOVEDAD\", \"Motivo\"]].head())\n",
    "\n",
    "print(\"\\nEvoluciones que siguen siendo inválidas:\")\n",
    "invalidas_final = df_N01[~df_N01.apply(lambda row: (row[\"TPS_IDN_ID\"], row[\"COD_1_NOVEDAD\"]) in evoluciones_validas, axis=1)]\n",
    "if len(invalidas_final) > 0:\n",
    "    print(invalidas_final[[\"TPS_IDN_ID\", \"COD_1_NOVEDAD\", \"Motivo\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Crear tabla pivote\n",
    "pivot_motivos = pd.pivot_table(\n",
    "    df_N01,\n",
    "    index=\"ENT_ID\",\n",
    "    columns=\"Motivo\",\n",
    "    values=\"NUM_SOLICITUD_NOVEDAD\",\n",
    "    aggfunc=\"count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Gráfico de barras apiladas\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pivot_motivos.plot(kind='bar', stacked=True, ax=ax, color=['#1f77b4', '#ff7f0e'])\n",
    "ax.set_title('Distribución de Motivos por Régimen (Apilado)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Régimen (ENT_ID)', fontsize=12)\n",
    "ax.set_ylabel('Cantidad de Registros', fontsize=12)\n",
    "ax.legend(title='Motivo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Agregar totales encima de cada barra\n",
    "totales = pivot_motivos.sum(axis=1)\n",
    "for i, total in enumerate(totales):\n",
    "    ax.text(i, total + max(totales) * 0.01, str(total), \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc25a8",
   "metadata": {},
   "source": [
    "# 4. Guardar archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce41f4",
   "metadata": {},
   "source": [
    "## 4.9. Carpeta N01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ee043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N01\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N01\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N09.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N01_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea1013",
   "metadata": {},
   "source": [
    "## 4.9. Carpeta N09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N09\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N09\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N09.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N09_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebce3e0",
   "metadata": {},
   "source": [
    "## 4.1. Carpeta N14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b578c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"N14\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"N14\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"Motivo\"]\n",
    "# Agrupar por la columna \"DPR_ID\" y guardar un TXT por cada grupo\n",
    "for dpr, grupo in df_N14.groupby(\"DPR_ID\"):\n",
    "    filename = f\"N14_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7fc43",
   "metadata": {},
   "source": [
    "# 4.1. Carpeta I02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la subcarpeta \"I02\" dentro de R_Salida\n",
    "output_folder = os.path.join(R_Salida, \"I02\")\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "# Columnas a **no** incluir en el TXT\n",
    "skip = [\"municipio_x\", \"descripcion\", \"municipio_y\"]\n",
    "for dpr, grupo in df_I02.groupby(\"DPR_ID\"):\n",
    "    filename    = f\"I02_{dpr}.TXT\"\n",
    "    output_file = os.path.join(output_folder, filename)\n",
    "\n",
    "    # Opción A: drop por parámetro\n",
    "    grupo.drop(columns=skip).to_csv(\n",
    "        output_file,\n",
    "        sep=\",\",\n",
    "        index=False,\n",
    "        encoding=\"ANSI\",\n",
    "        header=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c47239",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(R_Salida, \"Reporte SIE.xlsx\")\n",
    "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "    Df_SIE.to_excel(writer, sheet_name=\"MS_SIE\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "    df_N01.to_excel(writer, sheet_name=\"N01\", index=False)\n",
    "    df_N09.to_excel(writer, sheet_name=\"N09\", index=False)\n",
    "    df_N14.to_excel(writer, sheet_name=\"N14\", index=False)\n",
    "    df_I02.to_excel(writer, sheet_name=\"I02\", index=False)\n",
    "    Df_sin_AFL_ID.to_excel(writer, sheet_name=\"Df_sin_AFL_ID\", index=False)\n",
    "print(f\"Archivo guardado: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
