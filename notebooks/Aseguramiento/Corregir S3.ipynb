{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bcde6e",
   "metadata": {},
   "source": [
    "# 1. Carga de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39330bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8a092",
   "metadata": {},
   "source": [
    "# 2. rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Oficce\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0021072025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0021072025.TXT\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02515072025.TXT\"\n",
    "\n",
    "# Ruta archivos salida Office\n",
    "Carpeta = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\07_Julio\\22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Home\n",
    "#R_Ms_ADRES_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0029052025.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0029052025.TXT\"\n",
    "#R_S3 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02520052025.TXT\"\n",
    "\n",
    "# Ruta archivos salida Office\n",
    "#Carpeta = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\06_Junio\\04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"15/07/2025\"\n",
    "Name = \"15-07-2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04cc76",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf878dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GlOSA\"]\n",
    "Df_S3 = pd.read_csv(R_S3, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_S3.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# Agregar columna \"Enviar\" con un valor inicial vacío\n",
    "Df_S3['Enviar'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a406d4",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# Seleccionar las columnas de DF_ADRES a transferir, junto con las columnas clave\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "\n",
    "# Hacemos un merge de Df_S3 con df_transfer mediante las columnas clave\n",
    "Df_S3 = Df_S3.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from_adres\")\n",
    ")\n",
    "\n",
    "# Renombrar la columna importada TPS_EST_AFL_ID (si se quiere conservar con ese nombre)\n",
    "Df_S3.rename(columns={\"TPS_EST_AFL_ID\": \"TPS_EST_AFL_ID_from_adres\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "# Agregar una nueva columna llamada \"Motivo\" con valores iniciales vacíos\n",
    "Df_S3['Motivo'] = ''\n",
    "# Proceso 1: Mover registros que cumplen la primera condición\n",
    "mask_1 = (Df_S3[\"ENT_ID_from_adres\"] == \"EPS025\") & (Df_S3[\"TPS_EST_AFL_ID_from_adres\"] == \"AC\")\n",
    "\n",
    "# Extraer los registros que cumplen la primera condición\n",
    "DF_No_Enviar_1 = Df_S3.loc[mask_1].copy()\n",
    "\n",
    "# Asignar el motivo correspondiente\n",
    "DF_No_Enviar_1[\"Motivo\"] = \"Activo Regimen Subsidiado\"\n",
    "\n",
    "# Eliminar estos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask_1].copy()\n",
    "\n",
    "# Proceso 2: Mover registros que cumplen la segunda condición\n",
    "mask_2 = Df_S3[\"TIPO_TRASLADO\"].isin([\"0\", \"1\", \"2\"])\n",
    "\n",
    "# Extraer los registros que cumplen la segunda condición\n",
    "DF_No_Enviar_2 = Df_S3.loc[mask_2].copy()\n",
    "\n",
    "# Asignar el motivo correspondiente\n",
    "DF_No_Enviar_2[\"Motivo\"] = \"Es un traslado de EPS\"\n",
    "\n",
    "# Eliminar estos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask_2].copy()\n",
    "\n",
    "# Combinar ambos DataFrames de registros no enviados\n",
    "DF_No_Enviar = pd.concat([DF_No_Enviar_1, DF_No_Enviar_2], ignore_index=True)\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe7bc9",
   "metadata": {},
   "source": [
    "# 5. Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar la columna \"No_Glosas\" contando las glosas separadas por \";\"\n",
    "Df_S3['No_Glosas'] = Df_S3['GlOSA'].apply(\n",
    "    lambda x: len(x.rstrip(';').split(';')) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# Imprimir los valores únicos de la nueva columna\n",
    "print(Df_S3['No_Glosas'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a94785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los primeros 6 caracteres de la columna 'GlOSA' y guardarlos en 'Glosa_Actual'\n",
    "Df_S3['Glosa_Actual'] = Df_S3['GlOSA'].str[:6]\n",
    "\n",
    "# Duplicar la columna 'GlOSA' en una nueva columna 'GlOSA_2'\n",
    "Df_S3['GlOSA_2'] = Df_S3['GlOSA']\n",
    "\n",
    "# Imprimir los valores únicos y la cantidad de registros de cada uno en la columna \"Glosa_Actual\"\n",
    "print(\"Valores únicos en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].unique())\n",
    "\n",
    "print(\"\\nCantidad de registros por cada valor en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69168a9d",
   "metadata": {},
   "source": [
    "# 5.1. Glosa GN0368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_GN0368(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0368 y ajusta la fecha en formato dd/mm/YYYY.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens por \";\" y filtrar vacíos\n",
    "    tokens = [t for t in glosa_str.split(\";\") if t]\n",
    "\n",
    "    # Buscar token GN0368(\n",
    "    token_gn = next((t for t in tokens if t.startswith(\"GN0368(\")), None)\n",
    "    if not token_gn:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha en formato dd/mm/YYYY dentro de GN0368(...)\n",
    "    m = re.search(r\"GN0368\\([^()]*?(\\d{2}/\\d{2}/\\d{4})\\)\", token_gn)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_str = m.group(1)\n",
    "    fecha_dt = datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "    # Sumar un día y formatear de nuevo como dd/mm/YYYY\n",
    "    nueva_fecha = (fecha_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminar el token GN0368 de la lista\n",
    "    restantes = [t for t in tokens if not t.startswith(\"GN0368(\")]\n",
    "    # Reconstruir GlOSA_2\n",
    "    row[\"GlOSA_2\"] = \";\".join(restantes) + (\";\" if restantes else \"\")\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and restantes:\n",
    "        row[\"Glosa_Actual\"] = restantes[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_GN0368, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b29bf",
   "metadata": {},
   "source": [
    "# 5.2. Glosa GN0369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_369(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0369(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # split y quitamos vacíos\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    # buscamos la glosa GN0369\n",
    "    token = next((tok for tok in tokens if tok.startswith(\"GN0369(\")), None)\n",
    "    if not token:\n",
    "        return row\n",
    "\n",
    "    # regex para extraer meses y fecha final\n",
    "    m = re.search(r\"GN0369\\([^|]*\\|(\\d+)\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    meses = int(m.group(1))\n",
    "    fecha_base = datetime.strptime(m.group(2), \"%d/%m/%Y\")\n",
    "\n",
    "    # sumamos meses y un día\n",
    "    nueva_fecha_dt = fecha_base + relativedelta(months=meses) + timedelta(days=1)\n",
    "    nueva_fecha = nueva_fecha_dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # actualizamos la columna de movilidad\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # eliminamos la glosa procesada\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0369(\")]\n",
    "    row[\"GlOSA_2\"] = ( \";\".join(tokens) + \";\" ) if tokens else \"\"\n",
    "\n",
    "    # ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1:\n",
    "        # si quedan otras glosas, tomamos la primera antes del \"(\"\n",
    "        primera = tokens[0]\n",
    "        row[\"Glosa_Actual\"] = primera.split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo al DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_369, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a1ce7",
   "metadata": {},
   "source": [
    "# 5.3. Glosa GN0421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_421(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separamos tokens y buscamos GN0421(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_421 = next((tok for tok in tokens if tok.startswith(\"GN0421(\")), None)\n",
    "    if not token_421:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha\n",
    "    m = re.search(r\"GN0421\\((\\d{2}/\\d{2}/\\d{4})\\)\", token_421)\n",
    "    if not m:\n",
    "        return row\n",
    "    fecha_base = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Sumamos un día\n",
    "    nueva_fecha = (fecha_base + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminamos el token procesado\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0421(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_421, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52b0ed",
   "metadata": {},
   "source": [
    "# 5.4. Glosa GN0084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0084(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Partimos tokens y buscamos GN0084(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_84 = next((tok for tok in tokens if tok.startswith(\"GN0084(\")), None)\n",
    "    if not token_84:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha tras el '|'\n",
    "    m = re.search(r\"GN0084\\([^|]+\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_84)\n",
    "    if m:\n",
    "        fecha_glosa = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "        # Intentamos parsear la fecha existente en FECHA_AFILIACION_MOVILIDAD\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\",\"\"), \"%d/%m/%Y\")\n",
    "        except Exception:\n",
    "            fecha_actual = None\n",
    "\n",
    "        # Solo si la fecha actual es anterior a la de la glosa, la actualizamos\n",
    "        if fecha_actual and fecha_actual < fecha_glosa:\n",
    "            nueva = (fecha_glosa + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva\n",
    "\n",
    "    # Eliminamos siempre el token GN0084\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0084(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo sobre tu DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0084, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb614f",
   "metadata": {},
   "source": [
    "# 5.5 Glosa GN0256\n",
    "1. No corresponde a Movilidad.\n",
    "2. GN0256;\n",
    "Esta glosa se presenta cuando una EPS quiere realizar un traslado y en el archivo registran en tipo de traslado 3 o 4, también cuando desean hacer movilidad ascendente o descendente y utilizan un tipo de traslado diferente a 3 o4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edac843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0256(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0256 y ajusta el valor de la columna TIPO_TRASLADO si corresponde.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0256\" not in glosa_str:\n",
    "        # No contiene la glosa GN0256, retorna la fila sin modificar\n",
    "        return row\n",
    "\n",
    "    # Inicializamos una bandera para verificar si se realizó algún cambio\n",
    "    cambio_realizado = False\n",
    "\n",
    "    # Condición 1: Ajustar TIPO_TRASLADO a \"1\" si ENT_ID_ORIGEN no es EPSC25 o EPS025 y TIPO_TRASLADO es \"3\", \"4\", \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") not in [\"EPSC25\", \"EPS025\"] and row.get(\"TIPO_TRASLADO\") in [\"3\", \"4\", \"5\"]:\n",
    "        row[\"TIPO_TRASLADO\"] = \"1\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Condición 2: Ajustar TIPO_TRASLADO a \"4\" si ENT_ID_ORIGEN es EPSC25 y TIPO_TRASLADO es \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") == \"EPSC25\" and row.get(\"TIPO_TRASLADO\") == \"5\":\n",
    "        row[\"TIPO_TRASLADO\"] = \"4\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Separar tokens por \";\" y eliminar el token GN0256\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0256\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual correctamente\n",
    "    if cambio_realizado:\n",
    "        if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "            row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "        else:\n",
    "            row[\"Glosa_Actual\"] = \"\"\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"Sin cambios GN0256\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0256, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5620c3d",
   "metadata": {},
   "source": [
    "# 5.6 Glosa GN0029\n",
    "1. Afiliado en trámite de traslado, en el Régimen Contributivo.\n",
    "2. GN0029(R4|EPS005|CCF055);\n",
    "Mientras que no se de respuesta a la solicitud de traslado, el registro es restringido con esta glsoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0029(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0029, eliminándola y actualizando las columnas 'Glosa_Actual' y 'GlOSA_2'.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0029(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y eliminar la glosa GN0029\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0029(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar 'Glosa_Actual'\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0029, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488fa68",
   "metadata": {},
   "source": [
    "# 5.7 Glosa GN0302\n",
    "1. La fecha de inicio de novedad se encuentra en BDEX para el periodo solicitado.\n",
    "2. GN0302(FMS001[1|10/05/2022|31/12/2999]);\n",
    "3. GN0302(CodigoEntidad[Consecutivo|FechaInicioAfiliacion|FechaFinAfiliacion]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0302(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0302 y realiza las actualizaciones necesarias.\n",
    "    Si la fecha final es 31/12/2999, marca el registro para moverlo a DF_No_Enviar.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0302(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0302\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_302 = next((tok for tok in tokens if tok.startswith(\"GN0302(\")), None)\n",
    "    if not token_302:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha de inicio\n",
    "    m = re.search(r\"GN0302\\([^|]*\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\|\", token_302)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_inicio = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Verificar si la fecha final es 31/12/2999\n",
    "    if \"31/12/2999\" in token_302:\n",
    "        row[\"Motivo\"] = \"Activo Regimen Especial\"\n",
    "    else:\n",
    "        # Ajustar fecha de movilidad si aplica\n",
    "        nueva_fecha = fecha_inicio + timedelta(days=1)\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\", \"\"), \"%d/%m/%Y\")\n",
    "            if fecha_actual < nueva_fecha:\n",
    "                row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha.strftime(\"%d/%m/%Y\")\n",
    "        except:\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # Eliminar GN0302 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0302(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0302:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0302', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0302, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"Activo Regimen Especial\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"Activo Regimen Especial\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0302:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0302', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ac90f",
   "metadata": {},
   "source": [
    "# Glosa GN0259\n",
    "1. El registro presenta afiliación como pensionado. // NR: \" \"\n",
    "2. GN0297(01/09/2013|01/12/2999);\n",
    "3. GN0297(FechaInicioPension|FechaFinPension)\n",
    "4. El usuario es reportado como Pensionado por el MSPS en la BD RUAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1014c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0297(row):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0297 y marca los registros que presentan afiliación como pensionado.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0297(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0297\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_297 = next((tok for tok in tokens if tok.startswith(\"GN0297(\")), None)\n",
    "    if not token_297:\n",
    "        return row\n",
    "\n",
    "    # Marcar el registro como pensionado\n",
    "    row[\"Motivo\"] = \"El registro presenta afiliación como pensionado\"\n",
    "\n",
    "    # Eliminar GN0297 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0297(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0297:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0297', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0297, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"El registro presenta afiliación como pensionado\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"El registro presenta afiliación como pensionado\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0297:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0297', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae22f37",
   "metadata": {},
   "source": [
    "# Glosa GN0501\n",
    "1. El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\n",
    "2. R1, S1 - NS, NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0501(row):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0501 y marca los registros que tienen novedades en el SAT.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0501(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0501\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_501 = next((tok for tok in tokens if tok.startswith(\"GN0501(\")), None)\n",
    "    if not token_501:\n",
    "        return row\n",
    "\n",
    "    # Marcar el registro con el motivo SAT\n",
    "    row[\"Motivo\"] = \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"\n",
    "\n",
    "    # Eliminar GN0501 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0501(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0501:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0501', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0501, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0501:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0501', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff4c41",
   "metadata": {},
   "source": [
    "# Glosa GN0161\n",
    "1. La solicitud de trámite de traslado a la misma entidad, solo es válido si se está activando al usuario. EL usuario debe estar en estado RE, DE.\n",
    "2. GN0161(C|EPS005|01/09/2016|11|001|C|AC|01/09/2016);\n",
    "3. GN0161(Regimen|Entidad|FechaInicioAfil|Dpto|Mun|TipoAfiliado|Estado|FechaInicioCondicion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c21ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0161(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0161 y actualiza la columna 'Motivo' con el mensaje correspondiente.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0161(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0161\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_161 = next((tok for tok in tokens if tok.startswith(\"GN0161(\")), None)\n",
    "    if not token_161:\n",
    "        return row\n",
    "\n",
    "    # Extraer FechaInicioCondicion\n",
    "    m = re.search(r\"GN0161\\([^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_161)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_inicio_condicion = m.group(1)\n",
    "\n",
    "    # Actualizar la columna 'Motivo'\n",
    "    row[\"Motivo\"] = f\"Activo subsidiado proximamente {fecha_inicio_condicion}\"\n",
    "\n",
    "    # Eliminar GN0161 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0161(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0161, axis=1)\n",
    "\n",
    "# Mover registros con motivo asignado a DF_No_Enviar\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] != \"\"].copy()\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] == \"\"].copy()\n",
    "\n",
    "# Verificación después del proceso\n",
    "print(\"Registros con GN0161 procesados y movidos a DF_No_Enviar.\")\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a5596",
   "metadata": {},
   "source": [
    "# Glosa GN0017\n",
    "1. Afiliado ya existente pero los datos no concuerdan con la información de tipo genero enviada con la BDUA.\n",
    "2. GN0017(M);\n",
    "3. GN0017(GeneroBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0017(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0017 actualizando los valores de las columnas\n",
    "    'TPS_GNR_ID' y 'TPS_GNR_ID_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0017(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0017(...)\n",
    "    m = re.search(r\"GN0017\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"TPS_GNR_ID\"]   = nuevo_nombre\n",
    "    row[\"TPS_GNR_ID_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar la glosa GN0017 de la lista de glosas,\n",
    "    # filtrando también las cadenas vacías:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0017(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo “;” al final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "    \n",
    "\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0017, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfe688",
   "metadata": {},
   "source": [
    "# Glosa GN0034\n",
    "1. Primer Apellido diferente al registrado en la BDUA.\n",
    "2. GN0034(MARTINEZ);    \n",
    "3. GN0034(PrimerApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f628bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0034(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0034 actualizando los valores de las columnas\n",
    "    'AFL_PRIMER_APELLIDO' y 'AFL_PRIMER_APELLIDO_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0034(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0034(...)\n",
    "    m = re.search(r\"GN0034\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_apellido = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"AFL_PRIMER_APELLIDO\"]   = nuevo_apellido\n",
    "    row[\"AFL_PRIMER_APELLIDO_2\"] = nuevo_apellido\n",
    "\n",
    "    # Eliminar la glosa GN0034 de la lista de glosas,\n",
    "    # filtrando también tokens vacíos:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0034(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2: si hay tokens, unir y poner un solo ';' al final\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0034, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c6f66",
   "metadata": {},
   "source": [
    "# Glosa GN0035\n",
    "1. Segundo Apellido diferente al registrado en la BDUA.\n",
    "2. GN0035(SUAREZ);\n",
    "3. GN0035(SegundoApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a232552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0035(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0035 (incluso si está vacía),\n",
    "    actualizando 'AFL_SEGUNDO_APELLIDO' y 'AFL_SEGUNDO_APELLIDO_2'\n",
    "    y luego eliminando la glosa de GlOSA_2.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0035(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Ahora permitimos también paréntesis vacíos\n",
    "    m = re.search(r\"GN0035\\(([^)]*)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    # Esto será '' si la glosa estaba vacía\n",
    "    nuevo_apellido = m.group(1)\n",
    "\n",
    "    # Asignamos el valor (vacío o no)\n",
    "    row[\"AFL_SEGUNDO_APELLIDO\"]   = nuevo_apellido\n",
    "    row[\"AFL_SEGUNDO_APELLIDO_2\"] = nuevo_apellido\n",
    "\n",
    "    # Eliminamos la glosa (filtrando también tokens vacíos)\n",
    "    tokens = [\n",
    "        tok\n",
    "        for tok in glosa_str.split(\";\")\n",
    "        if tok and not tok.startswith(\"GN0035(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruimos GlOSA_2 (un solo ';' al final si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizamos Glosa_Actual según tu lógica\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0035, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097598c1",
   "metadata": {},
   "source": [
    "# Glosa GN0036\n",
    "1. Primer Nombre diferente al registrado en la BDUA.\n",
    "2. GN0036(MARCELA);\n",
    "3. GN0036(PrimerNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0036(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0036 actualizando los valores de las columnas\n",
    "    'AFL_PRIMER_NOMBRE' y 'AFL_PRIMER_NOMBRE_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0036(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0036(...)\n",
    "    m = re.search(r\"GN0036\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"AFL_PRIMER_NOMBRE\"]   = nuevo_nombre\n",
    "    row[\"AFL_PRIMER_NOMBRE_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar GN0036 de la lista de glosas, filtrando vacíos:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0036(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo ';' al final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0036, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb338d9d",
   "metadata": {},
   "source": [
    "# Glosa GN0037\n",
    "1. Segundo nombre diferente al registrado en la BDUA.\n",
    "2. GN0037(CARLOS);\n",
    "3. GN0037(SegundoNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3023d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_0037(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0037 actualizando los valores de las columnas\n",
    "    'AFL_SEGUNDO_NOMBRE' y 'AFL_SEGUNDO_NOMBRE_2' con el valor de la glosa\n",
    "    (incluso si está vacío), y luego elimina la glosa de GlOSA_2.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0037(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Ahora permitimos también paréntesis vacíos:\n",
    "    m = re.search(r\"GN0037\\(([^)]*)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    # Esto será '' si la glosa estaba vacía\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas (quedarán vacías si nuevo_nombre == '')\n",
    "    row[\"AFL_SEGUNDO_NOMBRE\"]   = nuevo_nombre\n",
    "    row[\"AFL_SEGUNDO_NOMBRE_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar la glosa GN0037 de la lista, filtrando también los tokens vacíos\n",
    "    tokens = [\n",
    "        tok\n",
    "        for tok in glosa_str.split(\";\")\n",
    "        if tok and not tok.startswith(\"GN0037(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo ';' final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual según tu lógica\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0037, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5323",
   "metadata": {},
   "source": [
    "# 6. Guardar informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e872a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(Carpeta) / f\"Corrección Glosas-{Name}.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    Df_S3.to_excel(writer, sheet_name=\"Df_S3\", index=False)\n",
    "    DF_No_Enviar.to_excel(writer, sheet_name=\"DF_No_Enviar\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
