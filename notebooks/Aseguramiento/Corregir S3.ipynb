{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Carga de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 2. rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"20/01/2026\"\n",
    "Name = \"20-01-2026\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Oficce\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2026\\EPSC25MC0016012026.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2026\\EPS025MS0016012026.TXT\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2026\\S3EPS02514012026.TXT\"\n",
    "\n",
    "# Ruta archivos salida Office\n",
    "Carpeta = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2026\\01_Enero\\20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Home\n",
    "#R_Ms_ADRES_EPSC25 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2026\\EPSC25MC0005012026.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2026\\EPS025MS0005012026.TXT\"\n",
    "#R_S3 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02516122025.TXT\"\n",
    "\n",
    "# Ruta archivos salida home\n",
    "#Carpeta = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2026\\01_Enero\\06\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GlOSA\"]\n",
    "Df_S3 = pd.read_csv(R_S3, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_S3.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# Agregar columna \"Enviar\" con un valor inicial vacío\n",
    "Df_S3['Enviar'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# Seleccionar las columnas de DF_ADRES a transferir, junto con las columnas clave\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "\n",
    "# Hacemos un merge de Df_S3 con df_transfer mediante las columnas clave\n",
    "Df_S3 = Df_S3.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from_adres\")\n",
    ")\n",
    "\n",
    "# Renombrar la columna importada TPS_EST_AFL_ID (si se quiere conservar con ese nombre)\n",
    "Df_S3.rename(columns={\"TPS_EST_AFL_ID\": \"TPS_EST_AFL_ID_from_adres\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "# Agregar una nueva columna llamada \"Motivo\" con valores iniciales vacíos\n",
    "Df_S3['Motivo'] = ''\n",
    "# Proceso 1: Mover registros que cumplen la primera condición\n",
    "mask_1 = (Df_S3[\"ENT_ID_from_adres\"] == \"EPS025\") & (Df_S3[\"TPS_EST_AFL_ID_from_adres\"] == \"AC\")\n",
    "\n",
    "# Extraer los registros que cumplen la primera condición\n",
    "DF_No_Enviar_1 = Df_S3.loc[mask_1].copy()\n",
    "\n",
    "# Asignar el motivo correspondiente\n",
    "DF_No_Enviar_1[\"Motivo\"] = \"Activo Regimen Subsidiado\"\n",
    "\n",
    "# Eliminar estos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask_1].copy()\n",
    "\n",
    "# Proceso 2: Mover registros que cumplen la segunda condición\n",
    "mask_2 = Df_S3[\"TIPO_TRASLADO\"].isin([\"0\", \"1\", \"2\"])\n",
    "\n",
    "# Extraer los registros que cumplen la segunda condición\n",
    "DF_No_Enviar_2 = Df_S3.loc[mask_2].copy()\n",
    "\n",
    "# Asignar el motivo correspondiente\n",
    "DF_No_Enviar_2[\"Motivo\"] = \"Es un traslado de EPS\"\n",
    "\n",
    "# Eliminar estos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask_2].copy()\n",
    "\n",
    "# Combinar ambos DataFrames de registros no enviados\n",
    "DF_No_Enviar = pd.concat([DF_No_Enviar_1, DF_No_Enviar_2], ignore_index=True)\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 5. Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar la columna \"No_Glosas\" contando las glosas separadas por \";\"\n",
    "Df_S3['No_Glosas'] = Df_S3['GlOSA'].apply(\n",
    "    lambda x: len(x.rstrip(';').split(';')) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# Imprimir los valores únicos de la nueva columna\n",
    "print(Df_S3['No_Glosas'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los primeros 6 caracteres de la columna 'GlOSA' y guardarlos en 'Glosa_Actual'\n",
    "Df_S3['Glosa_Actual'] = Df_S3['GlOSA'].str[:6]\n",
    "\n",
    "# Duplicar la columna 'GlOSA' en una nueva columna 'GlOSA_2'\n",
    "Df_S3['GlOSA_2'] = Df_S3['GlOSA']\n",
    "\n",
    "# Imprimir los valores únicos y la cantidad de registros de cada uno en la columna \"Glosa_Actual\"\n",
    "print(\"Valores únicos en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].unique())\n",
    "\n",
    "print(\"\\nCantidad de registros por cada valor en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# 5.1. Glosa GN0368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_GN0368(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0368 y ajusta la fecha en formato dd/mm/YYYY.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens por \";\" y filtrar vacíos\n",
    "    tokens = [t for t in glosa_str.split(\";\") if t]\n",
    "\n",
    "    # Buscar token GN0368(\n",
    "    token_gn = next((t for t in tokens if t.startswith(\"GN0368(\")), None)\n",
    "    if not token_gn:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha en formato dd/mm/YYYY dentro de GN0368(...)\n",
    "    m = re.search(r\"GN0368\\([^()]*?(\\d{2}/\\d{2}/\\d{4})\\)\", token_gn)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_str = m.group(1)\n",
    "    fecha_dt = datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "    # Sumar un día y formatear de nuevo como dd/mm/YYYY\n",
    "    nueva_fecha = (fecha_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminar el token GN0368 de la lista\n",
    "    restantes = [t for t in tokens if not t.startswith(\"GN0368(\")]\n",
    "    # Reconstruir GlOSA_2\n",
    "    row[\"GlOSA_2\"] = \";\".join(restantes) + (\";\" if restantes else \"\")\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and restantes:\n",
    "        row[\"Glosa_Actual\"] = restantes[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_GN0368, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 5.2. Glosa GN0369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_369(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0369(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # split y quitamos vacíos\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    # buscamos la glosa GN0369\n",
    "    token = next((tok for tok in tokens if tok.startswith(\"GN0369(\")), None)\n",
    "    if not token:\n",
    "        return row\n",
    "\n",
    "    # regex para extraer meses y fecha final\n",
    "    m = re.search(r\"GN0369\\([^|]*\\|(\\d+)\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    meses = int(m.group(1))\n",
    "    fecha_base = datetime.strptime(m.group(2), \"%d/%m/%Y\")\n",
    "\n",
    "    # sumamos meses y un día\n",
    "    nueva_fecha_dt = fecha_base + relativedelta(months=meses) + timedelta(days=1)\n",
    "    nueva_fecha = nueva_fecha_dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # actualizamos la columna de movilidad\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # eliminamos la glosa procesada\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0369(\")]\n",
    "    row[\"GlOSA_2\"] = ( \";\".join(tokens) + \";\" ) if tokens else \"\"\n",
    "\n",
    "    # ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1:\n",
    "        # si quedan otras glosas, tomamos la primera antes del \"(\"\n",
    "        primera = tokens[0]\n",
    "        row[\"Glosa_Actual\"] = primera.split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo al DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_369, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# 5.3. Glosa GN0421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_421(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separamos tokens y buscamos GN0421(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_421 = next((tok for tok in tokens if tok.startswith(\"GN0421(\")), None)\n",
    "    if not token_421:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha\n",
    "    m = re.search(r\"GN0421\\((\\d{2}/\\d{2}/\\d{4})\\)\", token_421)\n",
    "    if not m:\n",
    "        return row\n",
    "    fecha_base = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Sumamos un día\n",
    "    nueva_fecha = (fecha_base + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminamos el token procesado\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0421(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_421, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# 5.4. Glosa GN0084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0084(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Partimos tokens y buscamos GN0084(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_84 = next((tok for tok in tokens if tok.startswith(\"GN0084(\")), None)\n",
    "    if not token_84:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha tras el '|'\n",
    "    m = re.search(r\"GN0084\\([^|]+\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_84)\n",
    "    if m:\n",
    "        fecha_glosa = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "        # Intentamos parsear la fecha existente en FECHA_AFILIACION_MOVILIDAD\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\",\"\"), \"%d/%m/%Y\")\n",
    "        except Exception:\n",
    "            fecha_actual = None\n",
    "\n",
    "        # Solo si la fecha actual es anterior a la de la glosa, la actualizamos\n",
    "        if fecha_actual and fecha_actual < fecha_glosa:\n",
    "            nueva = (fecha_glosa + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva\n",
    "\n",
    "    # Eliminamos siempre el token GN0084\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0084(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo sobre tu DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0084, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 5.5 Glosa GN0256\n",
    "1. No corresponde a Movilidad.\n",
    "2. GN0256;\n",
    "Esta glosa se presenta cuando una EPS quiere realizar un traslado y en el archivo registran en tipo de traslado 3 o 4, también cuando desean hacer movilidad ascendente o descendente y utilizan un tipo de traslado diferente a 3 o4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0256(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0256 y ajusta el valor de la columna TIPO_TRASLADO si corresponde.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0256\" not in glosa_str:\n",
    "        # No contiene la glosa GN0256, retorna la fila sin modificar\n",
    "        return row\n",
    "\n",
    "    # Inicializamos una bandera para verificar si se realizó algún cambio\n",
    "    cambio_realizado = False\n",
    "\n",
    "    # Condición 1: Ajustar TIPO_TRASLADO a \"1\" si ENT_ID_ORIGEN no es EPSC25 o EPS025 y TIPO_TRASLADO es \"3\", \"4\", \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") not in [\"EPSC25\", \"EPS025\"] and row.get(\"TIPO_TRASLADO\") in [\"3\", \"4\", \"5\"]:\n",
    "        row[\"TIPO_TRASLADO\"] = \"1\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Condición 2: Ajustar TIPO_TRASLADO a \"4\" si ENT_ID_ORIGEN es EPSC25 y TIPO_TRASLADO es \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") == \"EPSC25\" and row.get(\"TIPO_TRASLADO\") == \"5\":\n",
    "        row[\"TIPO_TRASLADO\"] = \"4\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Separar tokens por \";\" y eliminar el token GN0256\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0256\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual correctamente\n",
    "    if cambio_realizado:\n",
    "        if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "            row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "        else:\n",
    "            row[\"Glosa_Actual\"] = \"\"\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"Sin cambios GN0256\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0256, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# 5.6 Glosa GN0029\n",
    "1. Afiliado en trámite de traslado, en el Régimen Contributivo.\n",
    "2. GN0029(R4|EPS005|CCF055);\n",
    "Mientras que no se de respuesta a la solicitud de traslado, el registro es restringido con esta glsoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0029(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0029, eliminándola y actualizando las columnas 'Glosa_Actual' y 'GlOSA_2'.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0029(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y eliminar la glosa GN0029\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0029(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar 'Glosa_Actual'\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0029, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# 5.7 Glosa GN0302\n",
    "1. La fecha de inicio de novedad se encuentra en BDEX para el periodo solicitado.\n",
    "2. GN0302(FMS001[1|10/05/2022|31/12/2999]);\n",
    "3. GN0302(CodigoEntidad[Consecutivo|FechaInicioAfiliacion|FechaFinAfiliacion]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0302(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0302 y realiza las actualizaciones necesarias.\n",
    "    Si la fecha final es 31/12/2999, marca el registro para moverlo a DF_No_Enviar.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0302(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0302\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_302 = next((tok for tok in tokens if tok.startswith(\"GN0302(\")), None)\n",
    "    if not token_302:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha de inicio\n",
    "    m = re.search(r\"GN0302\\([^|]*\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\|\", token_302)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_inicio = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Verificar si la fecha final es 31/12/2999\n",
    "    if \"31/12/2999\" in token_302:\n",
    "        row[\"Motivo\"] = \"Activo Regimen Especial\"\n",
    "    else:\n",
    "        # Ajustar fecha de movilidad si aplica\n",
    "        nueva_fecha = fecha_inicio + timedelta(days=1)\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\", \"\"), \"%d/%m/%Y\")\n",
    "            if fecha_actual < nueva_fecha:\n",
    "                row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha.strftime(\"%d/%m/%Y\")\n",
    "        except:\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # Eliminar GN0302 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0302(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0302:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0302', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0302, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"Activo Regimen Especial\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"Activo Regimen Especial\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0302:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0302', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Glosa GN0259\n",
    "1. El registro presenta afiliación como pensionado. // NR: \" \"\n",
    "2. GN0297(01/09/2013|01/12/2999);\n",
    "3. GN0297(FechaInicioPension|FechaFinPension)\n",
    "4. El usuario es reportado como Pensionado por el MSPS en la BD RUAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0297(row):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0297 y marca los registros que presentan afiliación como pensionado.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0297(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0297\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_297 = next((tok for tok in tokens if tok.startswith(\"GN0297(\")), None)\n",
    "    if not token_297:\n",
    "        return row\n",
    "\n",
    "    # Marcar el registro como pensionado\n",
    "    row[\"Motivo\"] = \"El registro presenta afiliación como pensionado\"\n",
    "\n",
    "    # Eliminar GN0297 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0297(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0297:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0297', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0297, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"El registro presenta afiliación como pensionado\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"El registro presenta afiliación como pensionado\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0297:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0297', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Glosa GN0501\n",
    "1. El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\n",
    "2. R1, S1 - NS, NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0501(row):\n",
    "    \"\"\"\n",
    "    Procesa la glosa GN0501 y marca los registros que tienen novedades en el SAT.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0501(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0501\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_501 = next((tok for tok in tokens if tok.startswith(\"GN0501(\")), None)\n",
    "    if not token_501:\n",
    "        return row\n",
    "\n",
    "    # Marcar el registro con el motivo SAT\n",
    "    row[\"Motivo\"] = \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"\n",
    "\n",
    "    # Eliminar GN0501 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0501(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "    \n",
    "    # Actualizar Glosa_Actual\n",
    "    row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0] if tokens else \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Verificación antes de procesar\n",
    "print(\"Antes de procesar:\")\n",
    "print(\"Registros con GN0501:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0501', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0501, axis=1)\n",
    "\n",
    "# Identificar registros a mover\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] == \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"].copy()\n",
    "\n",
    "# Agregar registros a DF_No_Enviar\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    \n",
    "    # Eliminar registros movidos de Df_S3\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] != \"El afiliado cotizante o cabeza de familia ha realizado o se encuentra asociado a alguna novedad realizada en el SAT. Ha de continuar con el uso del SAT\"].copy()\n",
    "\n",
    "# Verificación después de procesar\n",
    "print(\"\\nDespués de procesar:\")\n",
    "print(\"Registros con GN0501:\", len(Df_S3[Df_S3['GlOSA_2'].str.contains('GN0501', na=False)]))\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Glosa GN0161\n",
    "1. La solicitud de trámite de traslado a la misma entidad, solo es válido si se está activando al usuario. EL usuario debe estar en estado RE, DE.\n",
    "2. GN0161(C|EPS005|01/09/2016|11|001|C|AC|01/09/2016);\n",
    "3. GN0161(Regimen|Entidad|FechaInicioAfil|Dpto|Mun|TipoAfiliado|Estado|FechaInicioCondicion);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0161(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0161 y actualiza la columna 'Motivo' con el mensaje correspondiente.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0161(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y buscar GN0161\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    token_161 = next((tok for tok in tokens if tok.startswith(\"GN0161(\")), None)\n",
    "    if not token_161:\n",
    "        return row\n",
    "\n",
    "    # Extraer FechaInicioCondicion\n",
    "    m = re.search(r\"GN0161\\([^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_161)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_inicio_condicion = m.group(1)\n",
    "\n",
    "    # Actualizar la columna 'Motivo'\n",
    "    row[\"Motivo\"] = f\"Activo subsidiado proximamente {fecha_inicio_condicion}\"\n",
    "\n",
    "    # Eliminar GN0161 de la lista de glosas\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0161(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0161, axis=1)\n",
    "\n",
    "# Mover registros con motivo asignado a DF_No_Enviar\n",
    "registros_a_mover = Df_S3[Df_S3[\"Motivo\"] != \"\"].copy()\n",
    "if not registros_a_mover.empty:\n",
    "    DF_No_Enviar = pd.concat([DF_No_Enviar, registros_a_mover], ignore_index=True)\n",
    "    Df_S3 = Df_S3[Df_S3[\"Motivo\"] == \"\"].copy()\n",
    "\n",
    "# Verificación después del proceso\n",
    "print(\"Registros con GN0161 procesados y movidos a DF_No_Enviar.\")\n",
    "print(\"Total registros en Df_S3:\", len(Df_S3))\n",
    "print(\"Total registros en DF_No_Enviar:\", len(DF_No_Enviar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Glosa GN0017\n",
    "1. Afiliado ya existente pero los datos no concuerdan con la información de tipo genero enviada con la BDUA.\n",
    "2. GN0017(M);\n",
    "3. GN0017(GeneroBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0017(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0017 actualizando los valores de las columnas\n",
    "    'TPS_GNR_ID' y 'TPS_GNR_ID_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0017(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0017(...)\n",
    "    m = re.search(r\"GN0017\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"TPS_GNR_ID\"]   = nuevo_nombre\n",
    "    row[\"TPS_GNR_ID_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar la glosa GN0017 de la lista de glosas,\n",
    "    # filtrando también las cadenas vacías:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0017(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo “;” al final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "    \n",
    "\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0017, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Glosa GN0034\n",
    "1. Primer Apellido diferente al registrado en la BDUA.\n",
    "2. GN0034(MARTINEZ);    \n",
    "3. GN0034(PrimerApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0034(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0034 actualizando los valores de las columnas\n",
    "    'AFL_PRIMER_APELLIDO' y 'AFL_PRIMER_APELLIDO_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0034(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0034(...)\n",
    "    m = re.search(r\"GN0034\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_apellido = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"AFL_PRIMER_APELLIDO\"]   = nuevo_apellido\n",
    "    row[\"AFL_PRIMER_APELLIDO_2\"] = nuevo_apellido\n",
    "\n",
    "    # Eliminar la glosa GN0034 de la lista de glosas,\n",
    "    # filtrando también tokens vacíos:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0034(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2: si hay tokens, unir y poner un solo ';' al final\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0034, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Glosa GN0035\n",
    "1. Segundo Apellido diferente al registrado en la BDUA.\n",
    "2. GN0035(SUAREZ);\n",
    "3. GN0035(SegundoApeBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0035(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0035 (incluso si está vacía),\n",
    "    actualizando 'AFL_SEGUNDO_APELLIDO' y 'AFL_SEGUNDO_APELLIDO_2'\n",
    "    y luego eliminando la glosa de GlOSA_2.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0035(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Ahora permitimos también paréntesis vacíos\n",
    "    m = re.search(r\"GN0035\\(([^)]*)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    # Esto será '' si la glosa estaba vacía\n",
    "    nuevo_apellido = m.group(1)\n",
    "\n",
    "    # Asignamos el valor (vacío o no)\n",
    "    row[\"AFL_SEGUNDO_APELLIDO\"]   = nuevo_apellido\n",
    "    row[\"AFL_SEGUNDO_APELLIDO_2\"] = nuevo_apellido\n",
    "\n",
    "    # Eliminamos la glosa (filtrando también tokens vacíos)\n",
    "    tokens = [\n",
    "        tok\n",
    "        for tok in glosa_str.split(\";\")\n",
    "        if tok and not tok.startswith(\"GN0035(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruimos GlOSA_2 (un solo ';' al final si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizamos Glosa_Actual según tu lógica\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0035, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# Glosa GN0036\n",
    "1. Primer Nombre diferente al registrado en la BDUA.\n",
    "2. GN0036(MARCELA);\n",
    "3. GN0036(PrimerNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0036(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0036 actualizando los valores de las columnas\n",
    "    'AFL_PRIMER_NOMBRE' y 'AFL_PRIMER_NOMBRE_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0036(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer el valor dentro de GN0036(...)\n",
    "    m = re.search(r\"GN0036\\(([^)]+)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con el nuevo nombre\n",
    "    row[\"AFL_PRIMER_NOMBRE\"]   = nuevo_nombre\n",
    "    row[\"AFL_PRIMER_NOMBRE_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar GN0036 de la lista de glosas, filtrando vacíos:\n",
    "    tokens = [\n",
    "        tok \n",
    "        for tok in glosa_str.split(\";\") \n",
    "        if tok and not tok.startswith(\"GN0036(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo ';' al final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0036, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Glosa GN0037\n",
    "1. Segundo nombre diferente al registrado en la BDUA.\n",
    "2. GN0037(CARLOS);\n",
    "3. GN0037(SegundoNomBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_0037(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0037 actualizando los valores de las columnas\n",
    "    'AFL_SEGUNDO_NOMBRE' y 'AFL_SEGUNDO_NOMBRE_2' con el valor de la glosa\n",
    "    (incluso si está vacío), y luego elimina la glosa de GlOSA_2.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0037(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Ahora permitimos también paréntesis vacíos:\n",
    "    m = re.search(r\"GN0037\\(([^)]*)\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    # Esto será '' si la glosa estaba vacía\n",
    "    nuevo_nombre = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas (quedarán vacías si nuevo_nombre == '')\n",
    "    row[\"AFL_SEGUNDO_NOMBRE\"]   = nuevo_nombre\n",
    "    row[\"AFL_SEGUNDO_NOMBRE_2\"] = nuevo_nombre\n",
    "\n",
    "    # Eliminar la glosa GN0037 de la lista, filtrando también los tokens vacíos\n",
    "    tokens = [\n",
    "        tok\n",
    "        for tok in glosa_str.split(\";\")\n",
    "        if tok and not tok.startswith(\"GN0037(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo ';' final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual según tu lógica\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0037, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Glosa GN0049\n",
    "1. Fecha de nacimiento diferente al registrado en la BDUA.\n",
    "2. GN0049(12/10/2001);\n",
    "3. GN0049(FechNacBDUA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0049(row):\n",
    "    \"\"\"\n",
    "    Corrige la glosa GN0049 actualizando los valores de las columnas\n",
    "    'AFL_FECHA_NACIMIENTO' y 'AFL_FECHA_NACIMIENTO_2' con el valor de la glosa.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0049(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Extraer la fecha dentro de GN0049(...) con formato dd/mm/yyyy\n",
    "    m = re.search(r\"GN0049\\((\\d{2}/\\d{2}/\\d{4})\\)\", glosa_str)\n",
    "    if not m:\n",
    "        return row\n",
    "    \n",
    "    nueva_fecha = m.group(1)\n",
    "\n",
    "    # Actualizar las columnas con la nueva fecha\n",
    "    row[\"AFL_FECHA_NACIMIENTO\"]   = nueva_fecha\n",
    "    row[\"AFL_FECHA_NACIMIENTO_2\"] = nueva_fecha\n",
    "\n",
    "    # Eliminar la glosa GN0049 de la lista, filtrando también tokens vacíos\n",
    "    tokens = [\n",
    "        tok\n",
    "        for tok in glosa_str.split(\";\")\n",
    "        if tok and not tok.startswith(\"GN0049(\")\n",
    "    ]\n",
    "\n",
    "    # Reconstruir GlOSA_2 con un solo ';' final (si hay tokens)\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual según la lógica establecida\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0049, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# Glosa GN0060\n",
    "1. Edad del afiliado inconsistente con tipo y número de identificación.\n",
    "2. GN0060(267|84|228);\n",
    "3. GN0060(EdadEnMeses|EdadEnMesesMinimaPermitidaParaDocumento|EdadEnMesesMaximaPermitidaParaDocumento);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_gn0060(df):\n",
    "    \"\"\"\n",
    "    Función para limpiar la glosa GN0060 - Edad del afiliado inconsistente con tipo y número de identificación.\n",
    "    \n",
    "    Evoluciona el tipo de documento:\n",
    "    - RC -> TI\n",
    "    - TI -> CC\n",
    "    \n",
    "    Para otros casos, marca para revisión manual.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Contador inicial\n",
    "    registros_gn0060_inicial = df[df['GlOSA_2'].str.contains('GN0060', na=False)].shape[0]\n",
    "    print(f\"Registros con GN0060 antes del procesamiento: {registros_gn0060_inicial}\")\n",
    "    \n",
    "    # Filtrar registros con glosa GN0060\n",
    "    mask_gn0060 = df['GlOSA_2'].str.contains('GN0060', na=False)\n",
    "    \n",
    "    if mask_gn0060.any():\n",
    "        # Evolución RC -> TI\n",
    "        mask_rc = mask_gn0060 & (df['TPS_IDN_ID'] == 'RC')\n",
    "        df.loc[mask_rc, 'TPS_IDN_ID_2'] = 'TI'\n",
    "        \n",
    "        # Evolución TI -> CC\n",
    "        mask_ti = mask_gn0060 & (df['TPS_IDN_ID'] == 'TI')\n",
    "        df.loc[mask_ti, 'TPS_IDN_ID_2'] = 'CC'\n",
    "        \n",
    "        # Para RC y TI (casos que SÍ se pueden corregir)\n",
    "        mask_corregibles = mask_rc | mask_ti\n",
    "        \n",
    "        # Eliminar la glosa GN0060 para los casos corregibles\n",
    "        df.loc[mask_corregibles, 'GlOSA_2'] = df.loc[mask_corregibles, 'GlOSA_2'].str.replace(r'GN0060[^;]*;?', '', regex=True)\n",
    "        \n",
    "        # Limpiar GlOSA_2 de puntos y comas sobrantes\n",
    "        df.loc[mask_corregibles, 'GlOSA_2'] = df.loc[mask_corregibles, 'GlOSA_2'].str.replace(r'^;+|;+$', '', regex=True)\n",
    "        df.loc[mask_corregibles, 'GlOSA_2'] = df.loc[mask_corregibles, 'GlOSA_2'].str.replace(r';+', ';', regex=True)\n",
    "        \n",
    "        # Actualizar Glosa_Actual para casos corregibles basándose en si quedan glosas\n",
    "        for idx in df[mask_corregibles].index:\n",
    "            glosa_restante = df.loc[idx, 'GlOSA_2'].strip()\n",
    "            if df.loc[idx, 'No_Glosas'] > 1 and glosa_restante:\n",
    "                # Si quedan glosas, tomar la primera\n",
    "                primera_glosa = glosa_restante.split(';')[0]\n",
    "                df.loc[idx, 'Glosa_Actual'] = primera_glosa.split('(')[0] if '(' in primera_glosa else primera_glosa\n",
    "            else:\n",
    "                # No quedan glosas\n",
    "                df.loc[idx, 'Glosa_Actual'] = ''\n",
    "        \n",
    "        # Casos fuera de RC/TI - marcar para revisión (NO se pueden corregir)\n",
    "        mask_otros = mask_gn0060 & ~(df['TPS_IDN_ID'].isin(['RC', 'TI']))\n",
    "        df.loc[mask_otros, 'Glosa_Actual'] = 'Revisar evolución para determinar si se reporta o no'\n",
    "        \n",
    "        # Contadores finales\n",
    "        registros_gn0060_final = df[df['GlOSA_2'].str.contains('GN0060', na=False)].shape[0]\n",
    "        registros_evolucionados = mask_rc.sum() + mask_ti.sum()\n",
    "        registros_revision = mask_otros.sum()\n",
    "        \n",
    "        print(f\"Registros RC evolucionados a TI: {mask_rc.sum()}\")\n",
    "        print(f\"Registros TI evolucionados a CC: {mask_ti.sum()}\")\n",
    "        print(f\"Registros marcados para revisión: {registros_revision}\")\n",
    "        print(f\"Registros con GN0060 después del procesamiento: {registros_gn0060_final}\")\n",
    "        print(f\"Total de registros procesados: {registros_evolucionados + registros_revision}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar la función\n",
    "Df_S3 = limpiar_glosa_gn0060(Df_S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# 6. Guardar informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "print(\"Número de registros en DF_No_Enviar:\", DF_No_Enviar.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(Carpeta) / f\"Corrección Glosas-{Name}.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    Df_S3.to_excel(writer, sheet_name=\"Df_S3\", index=False)\n",
    "    DF_No_Enviar.to_excel(writer, sheet_name=\"DF_No_Enviar\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
