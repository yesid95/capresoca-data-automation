{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bcde6e",
   "metadata": {},
   "source": [
    "# 1. Carga de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39330bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8a092",
   "metadata": {},
   "source": [
    "# 2. rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Oficce\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0019052025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0003062025.TXT\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02520052025.TXT\"\n",
    "\n",
    "# Ruta archivos salida Office\n",
    "Carpeta = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\06_Junio\\04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada Home\n",
    "#R_Ms_ADRES_EPSC25 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0029052025.TXT\"\n",
    "#R_Ms_ADRES_EPS025 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0029052025.TXT\"\n",
    "#R_S3 = r\"D:\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02520052025.TXT\"\n",
    "\n",
    "# Ruta archivos salida Office\n",
    "#Carpeta = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\06_Junio\\04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"04/06/2025\"\n",
    "Name = \"04-06-2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04cc76",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf878dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GlOSA\"]\n",
    "Df_S3 = pd.read_csv(R_S3, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_S3.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# Agregar columna \"Enviar\" con un valor inicial vacío\n",
    "Df_S3['Enviar'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a406d4",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# Seleccionar las columnas de DF_ADRES a transferir, junto con las columnas clave\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "\n",
    "# Hacemos un merge de Df_S3 con df_transfer mediante las columnas clave\n",
    "Df_S3 = Df_S3.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from_adres\")\n",
    ")\n",
    "\n",
    "# Si se desea actualizar el valor de ENT_ID en Df_S3 usando el de DF_ADRES,\n",
    "# se puede hacer lo siguiente. De lo contrario, se conservará el valor original:\n",
    "Df_S3[\"ENT_ID\"] = Df_S3[\"ENT_ID_from_adres\"].combine_first(Df_S3[\"ENT_ID\"])\n",
    "\n",
    "# Renombrar la columna importada TPS_EST_AFL_ID (si se quiere conservar con ese nombre)\n",
    "Df_S3.rename(columns={\"TPS_EST_AFL_ID\": \"TPS_EST_AFL_ID_from_adres\"}, inplace=True)\n",
    "\n",
    "# Opcional: se puede eliminar la columna auxiliar ENT_ID_from_adres ya que su valor fue transferido\n",
    "Df_S3.drop(columns=[\"ENT_ID_from_adres\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# 1. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_S3[\"ENT_ID\"] == \"EPS025\") &\n",
    "    (Df_S3[\"TPS_EST_AFL_ID_from_adres\"] == \"AC\")\n",
    ") | Df_S3[\"TIPO_TRASLADO\"].isin([\"0\", \"1\", \"2\"])\n",
    "\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_No_Enviar = Df_S3.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask].copy()\n",
    "\n",
    "print(\"Número de registros en DF_No_Enivar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe7bc9",
   "metadata": {},
   "source": [
    "# 5. Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar la columna \"No_Glosas\" contando las glosas separadas por \";\"\n",
    "Df_S3['No_Glosas'] = Df_S3['GlOSA'].apply(\n",
    "    lambda x: len(x.rstrip(';').split(';')) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# Imprimir los valores únicos de la nueva columna\n",
    "print(Df_S3['No_Glosas'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a94785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los primeros 6 caracteres de la columna 'GlOSA' y guardarlos en 'Glosa_Actual'\n",
    "Df_S3['Glosa_Actual'] = Df_S3['GlOSA'].str[:6]\n",
    "\n",
    "# Duplicar la columna 'GlOSA' en una nueva columna 'GlOSA_2'\n",
    "Df_S3['GlOSA_2'] = Df_S3['GlOSA']\n",
    "\n",
    "# Imprimir los valores únicos y la cantidad de registros de cada uno en la columna \"Glosa_Actual\"\n",
    "print(\"Valores únicos en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].unique())\n",
    "\n",
    "print(\"\\nCantidad de registros por cada valor en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69168a9d",
   "metadata": {},
   "source": [
    "# 5.1. Glosa GN0368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_GN0368(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0368 y ajusta la fecha en formato dd/mm/YYYY.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens por \";\" y filtrar vacíos\n",
    "    tokens = [t for t in glosa_str.split(\";\") if t]\n",
    "\n",
    "    # Buscar token GN0368(\n",
    "    token_gn = next((t for t in tokens if t.startswith(\"GN0368(\")), None)\n",
    "    if not token_gn:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha en formato dd/mm/YYYY dentro de GN0368(...)\n",
    "    m = re.search(r\"GN0368\\([^()]*?(\\d{2}/\\d{2}/\\d{4})\\)\", token_gn)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_str = m.group(1)\n",
    "    fecha_dt = datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "    # Sumar un día y formatear de nuevo como dd/mm/YYYY\n",
    "    nueva_fecha = (fecha_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminar el token GN0368 de la lista\n",
    "    restantes = [t for t in tokens if not t.startswith(\"GN0368(\")]\n",
    "    # Reconstruir GlOSA_2\n",
    "    row[\"GlOSA_2\"] = \";\".join(restantes) + (\";\" if restantes else \"\")\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and restantes:\n",
    "        row[\"Glosa_Actual\"] = restantes[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_GN0368, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b29bf",
   "metadata": {},
   "source": [
    "# 5.2. Glosa GN0369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_369(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0369(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # split y quitamos vacíos\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    # buscamos la glosa GN0369\n",
    "    token = next((tok for tok in tokens if tok.startswith(\"GN0369(\")), None)\n",
    "    if not token:\n",
    "        return row\n",
    "\n",
    "    # regex para extraer meses y fecha final\n",
    "    m = re.search(r\"GN0369\\([^|]*\\|(\\d+)\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    meses = int(m.group(1))\n",
    "    fecha_base = datetime.strptime(m.group(2), \"%d/%m/%Y\")\n",
    "\n",
    "    # sumamos meses y un día\n",
    "    nueva_fecha_dt = fecha_base + relativedelta(months=meses) + timedelta(days=1)\n",
    "    nueva_fecha = nueva_fecha_dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # actualizamos la columna de movilidad\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # eliminamos la glosa procesada\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0369(\")]\n",
    "    row[\"GlOSA_2\"] = ( \";\".join(tokens) + \";\" ) if tokens else \"\"\n",
    "\n",
    "    # ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1:\n",
    "        # si quedan otras glosas, tomamos la primera antes del \"(\"\n",
    "        primera = tokens[0]\n",
    "        row[\"Glosa_Actual\"] = primera.split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo al DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_369, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a1ce7",
   "metadata": {},
   "source": [
    "# 5.3. Glosa GN0421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_421(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separamos tokens y buscamos GN0421(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_421 = next((tok for tok in tokens if tok.startswith(\"GN0421(\")), None)\n",
    "    if not token_421:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha\n",
    "    m = re.search(r\"GN0421\\((\\d{2}/\\d{2}/\\d{4})\\)\", token_421)\n",
    "    if not m:\n",
    "        return row\n",
    "    fecha_base = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Sumamos un día\n",
    "    nueva_fecha = (fecha_base + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminamos el token procesado\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0421(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_421, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52b0ed",
   "metadata": {},
   "source": [
    "# 5.4. Glosa GN0084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0084(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Partimos tokens y buscamos GN0084(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_84 = next((tok for tok in tokens if tok.startswith(\"GN0084(\")), None)\n",
    "    if not token_84:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha tras el '|'\n",
    "    m = re.search(r\"GN0084\\([^|]+\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_84)\n",
    "    if m:\n",
    "        fecha_glosa = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "        # Intentamos parsear la fecha existente en FECHA_AFILIACION_MOVILIDAD\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\",\"\"), \"%d/%m/%Y\")\n",
    "        except Exception:\n",
    "            fecha_actual = None\n",
    "\n",
    "        # Solo si la fecha actual es anterior a la de la glosa, la actualizamos\n",
    "        if fecha_actual and fecha_actual < fecha_glosa:\n",
    "            nueva = (fecha_glosa + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva\n",
    "\n",
    "    # Eliminamos siempre el token GN0084\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0084(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo sobre tu DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0084, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb614f",
   "metadata": {},
   "source": [
    "# 5.5 Glosa GN0256\n",
    "1. No corresponde a Movilidad.\n",
    "2. GN0256;\n",
    "Esta glosa se presenta cuando una EPS quiere realizar un traslado y en el archivo registran en tipo de traslado 3 o 4, también cuando desean hacer movilidad ascendente o descendente y utilizan un tipo de traslado diferente a 3 o4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edac843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0256(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0256 y ajusta el valor de la columna TIPO_TRASLADO si corresponde.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0256\" not in glosa_str:\n",
    "        # No contiene la glosa GN0256, retorna la fila sin modificar\n",
    "        return row\n",
    "\n",
    "    # Inicializamos una bandera para verificar si se realizó algún cambio\n",
    "    cambio_realizado = False\n",
    "\n",
    "    # Condición 1: Ajustar TIPO_TRASLADO a \"1\" si ENT_ID_ORIGEN no es EPSC25 o EPS025 y TIPO_TRASLADO es \"3\", \"4\", \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") not in [\"EPSC25\", \"EPS025\"] and row.get(\"TIPO_TRASLADO\") in [\"3\", \"4\", \"5\"]:\n",
    "        row[\"TIPO_TRASLADO\"] = \"1\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Condición 2: Ajustar TIPO_TRASLADO a \"4\" si ENT_ID_ORIGEN es EPSC25 y TIPO_TRASLADO es \"5\"\n",
    "    if row.get(\"ENT_ID_ORIGEN\") == \"EPSC25\" and row.get(\"TIPO_TRASLADO\") == \"5\":\n",
    "        row[\"TIPO_TRASLADO\"] = \"4\"\n",
    "        cambio_realizado = True\n",
    "\n",
    "    # Separar tokens por \";\" y eliminar el token GN0256\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0256\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) if tokens else \"\"\n",
    "\n",
    "    # Actualizar Glosa_Actual correctamente\n",
    "    if cambio_realizado:\n",
    "        if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "            row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "        else:\n",
    "            row[\"Glosa_Actual\"] = \"\"\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"Sin cambios GN0256\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0256, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5620c3d",
   "metadata": {},
   "source": [
    "# 5 GN0029\n",
    "1. Afiliado en trámite de traslado, en el Régimen Contributivo.\n",
    "2. GN0029(R4|EPS005|CCF055);\n",
    "Mientras que no se de respuesta a la solicitud de traslado, el registro es restringido con esta glsoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0029(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0029, eliminándola y actualizando las columnas 'Glosa_Actual' y 'GlOSA_2'.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0029(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens y eliminar la glosa GN0029\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok and not tok.startswith(\"GN0029(\")]\n",
    "    row[\"GlOSA_2\"] = \";\".join(tokens) + \";\" if tokens else \"\"\n",
    "\n",
    "    # Actualizar 'Glosa_Actual'\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0029, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488fa68",
   "metadata": {},
   "source": [
    "# 5.7 Glosa GN0302\n",
    "1. La fecha de inicio de novedad se encuentra en BDEX para el periodo solicitado.\n",
    "2. GN0302(FMS001[1|10/05/2022|31/12/2999]);\n",
    "3. GN0302(CodigoEntidad[Consecutivo|FechaInicioAfiliacion|FechaFinAfiliacion]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_glosa_GN0302(\n",
    "    df_s3: pd.DataFrame,\n",
    "    df_no_enviar: pd.DataFrame,\n",
    "    fecha_referencia: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Procesa todas las filas de `df_s3` que contengan la glosa GN0302, de modo que:\n",
    "    \n",
    "      1. Si el año de la fecha dentro de GN0302 es mayor que el año de `fecha_referencia`,\n",
    "         mueve la fila a `df_no_enviar` (creando la columna \"motivo\" si es necesario) con:\n",
    "             motivo = \"Activo en el regimen especial\"\n",
    "         y posteriormente elimina esa fila de `df_s3`.\n",
    "    \n",
    "      2. Si la fecha dentro de GN0302 es del mismo año o menor que el año de `fecha_referencia`:\n",
    "         a. Extrae la fecha de GN0302 (formato \"DD/MM/YYYY\"), le suma un día, y\n",
    "            la compara con la fecha existente en la columna \"FECHA_AFILIACION_MOVILIDAD\":\n",
    "              • Si la fecha calculada (GN0302 + 1 día) es posterior a la fecha existente\n",
    "                en \"FECHA_AFILIACION_MOVILIDAD\", actualiza dicha columna con la nueva fecha.\n",
    "              • En caso contrario, deja en `df_s3` el valor que ya estaba en \"FECHA_AFILIACION_MOVILIDAD\".\n",
    "         b. Elimina el token completo \"GN0302(...)\" de la columna \"GlOSA_2\".\n",
    "         c. Recalcula la columna \"Glosa_Actual\":\n",
    "              • Si hay más de una glosa pendiente (columna \"No_Glosas\" > 1) y todavía queda\n",
    "                al menos un token de glosa en \"GlOSA_2\", asigna a \"Glosa_Actual\" el prefijo\n",
    "                (todo lo que va antes de \"(\" ) del primer token restante.\n",
    "              • En caso contrario (No_Glosas <= 1 o ya no quedan tokens), asigna cadena vacía a \"Glosa_Actual\".\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    df_s3 : pd.DataFrame\n",
    "        DataFrame principal que contiene las filas con posibles glosas GN0302.\n",
    "        Debe tener al menos estas columnas:\n",
    "          - \"GlOSA_2\" (string con uno o varios tokens de glosas separados por \";\")\n",
    "          - \"No_Glosas\" (int, número de glosas pendientes)\n",
    "          - \"FECHA_AFILIACION_MOVILIDAD\" (string en formato \"DD/MM/YYYY\", puede estar vacío)\n",
    "          - \"Glosa_Actual\" (string, nombre de la glosa vigente; se actualizará si corresponde)\n",
    "    \n",
    "    df_no_enviar : pd.DataFrame\n",
    "        DataFrame donde se almacenarán las filas que se detecten con fecha GN0302 de año\n",
    "        mayor que el año de `fecha_referencia`. Si no existe la columna \"motivo\", se creará.\n",
    "        Puede venir vacío o contener ya registros previos.\n",
    "    \n",
    "    fecha_referencia : str\n",
    "        Cadena con la fecha de comparación en formato \"DD/MM/YYYY\" (p. ej. \"04/06/2025\").\n",
    "        Sólo se utiliza para comparar los años: año_de_GN0302 vs año_de_fecha_referencia.\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        - El primer elemento es `df_s3` actualizado, con las filas movidas a `df_no_enviar` eliminadas,\n",
    "          las fechas actualizadas (cuando corresponde) y las glosas GN0302 quitadas de \"GlOSA_2\"/\"Glosa_Actual\".\n",
    "        - El segundo elemento es `df_no_enviar` con las filas añadidas (cada fila movida lleva su \"motivo\").\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------\n",
    "    # 1) Convertir fecha_referencia a datetime para comparar años\n",
    "    # -------------------------------------------------\n",
    "    fecha_ref_dt = datetime.strptime(fecha_referencia, \"%d/%m/%Y\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) Asegurarnos de que df_no_enviar tenga columna \"motivo\"\n",
    "    # -------------------------------------------------\n",
    "    if \"motivo\" not in df_no_enviar.columns:\n",
    "        df_no_enviar[\"motivo\"] = \"\"\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) Preparar lista para marcar índices a eliminar de df_s3\n",
    "    # -------------------------------------------------\n",
    "    indices_para_remover = []\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4) Recorrer cada fila de df_s3 con iterrows()\n",
    "    # -------------------------------------------------\n",
    "    for idx, fila in df_s3.iterrows():\n",
    "        glosa_str = fila.get(\"GlOSA_2\", \"\")\n",
    "        # 4.1) Si \"GlOSA_2\" está vacío o no contiene \"GN0302(\", no hay nada que hacer\n",
    "        if pd.isna(glosa_str) or \"GN0302(\" not in str(glosa_str):\n",
    "            continue\n",
    "\n",
    "        # 4.2) Partir el string de GlOSA_2 en tokens separados por \";\"\n",
    "        todos_tokens = [tok for tok in str(glosa_str).split(\";\") if tok.strip()]\n",
    "        token_gn = next((tok for tok in todos_tokens if tok.startswith(\"GN0302(\")), None)\n",
    "        if token_gn is None:\n",
    "            # Aunque contenga \"GN0302(\", no coincide el formato; se salta\n",
    "            continue\n",
    "\n",
    "        # 4.3) Extraer la fecha de fin de afiliación dentro de GN0302\n",
    "        #      Patrón: GN0302(someText|someText|DD/MM/YYYY|...)\n",
    "        patron_fecha = r\"GN0302\\([^|]*\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\"\n",
    "        match_fecha = re.search(patron_fecha, token_gn)\n",
    "        if not match_fecha:\n",
    "            # No se encontró la fecha en el formato esperado, saltamos\n",
    "            continue\n",
    "\n",
    "        fecha_gn_dt = datetime.strptime(match_fecha.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 5) CASO: año de GN0302 > año de fecha_ref_dt  --> MOVER A df_no_enviar\n",
    "        # -------------------------------------------------\n",
    "        if fecha_gn_dt.year > fecha_ref_dt.year:\n",
    "            # a. Hacer copia completa de la fila para no modificar el original\n",
    "            fila_copia = fila.copy()\n",
    "            fila_copia[\"motivo\"] = \"Activo en el regimen especial\"\n",
    "\n",
    "            # b. Añadir la fila a df_no_enviar\n",
    "            df_no_enviar = df_no_enviar.append(fila_copia, ignore_index=True)\n",
    "\n",
    "            # c. Marcar este índice de df_s3 para borrarlo luego\n",
    "            indices_para_remover.append(idx)\n",
    "            # d. Saltar cualquier otra modificación de esta fila en df_s3\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 6) CASO: año de GN0302 <= año de fecha_ref_dt --> AJUSTAR FECHA Y QUITAR TOKEN\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        # 6.1) Calcular la \"nueva fecha\" que es fecha_gn_dt + 1 día\n",
    "        nueva_fecha_dt = fecha_gn_dt + timedelta(days=1)\n",
    "        nueva_fecha_str = nueva_fecha_dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        # 6.2) Leer la fecha actual existente en \"FECHA_AFILIACION_MOVILIDAD\"\n",
    "        fecha_existente_str = fila.get(\"FECHA_AFILIACION_MOVILIDAD\", \"\")\n",
    "        # Si la celda está vacía o no es formato válido, asumimos que no hay fecha válida\n",
    "        try:\n",
    "            fecha_existente_dt = datetime.strptime(fecha_existente_str, \"%d/%m/%Y\")\n",
    "        except Exception:\n",
    "            fecha_existente_dt = None\n",
    "\n",
    "        # 6.3) Comparar: si la nueva_fecha_dt es posterior a fecha_existente_dt (o si no existía fecha válida),\n",
    "        #      actualizamos la columna \"FECHA_AFILIACION_MOVILIDAD\" con nueva_fecha_str.\n",
    "        if (fecha_existente_dt is None) or (nueva_fecha_dt > fecha_existente_dt):\n",
    "            df_s3.at[idx, \"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha_str\n",
    "        # En caso contrario (la fecha existente es igual o posterior a nueva_fecha), se mantiene la existente.\n",
    "\n",
    "        # 6.4) Eliminar el token GN0302(...) de la lista de glosas en \"GlOSA_2\"\n",
    "        tokens_sin_gn = [tok for tok in todos_tokens if not tok.startswith(\"GN0302(\")]\n",
    "        nuevo_glosa2 = \"\"\n",
    "        if tokens_sin_gn:\n",
    "            nuevo_glosa2 = \";\".join(tokens_sin_gn) + \";\"\n",
    "        df_s3.at[idx, \"GlOSA_2\"] = nuevo_glosa2\n",
    "\n",
    "        # 6.5) Recalcular \"Glosa_Actual\":\n",
    "        #      Si No_Glosas > 1 y aún hay tokens, usar el prefijo (antes de \"(\") del primer token.\n",
    "        no_glosas = fila.get(\"No_Glosas\", 0)\n",
    "        if (no_glosas > 1) and tokens_sin_gn:\n",
    "            primer_token = tokens_sin_gn[0]\n",
    "            glosa_actual = primer_token.split(\"(\")[0]\n",
    "            df_s3.at[idx, \"Glosa_Actual\"] = glosa_actual\n",
    "        else:\n",
    "            # Si no quedan glosas o No_Glosas <= 1, dejamos vacío\n",
    "            df_s3.at[idx, \"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 7) Tras recorrer todo, eliminamos de df_s3 las filas marcadas\n",
    "    # -------------------------------------------------\n",
    "    if indices_para_remover:\n",
    "        df_s3 = df_s3.drop(index=indices_para_remover)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 8) Reindexar df_s3 para que los índices queden consecutivos\n",
    "    # -------------------------------------------------\n",
    "    df_s3 = df_s3.reset_index(drop=True)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 9) Devolver ambos DataFrames actualizados\n",
    "    # -------------------------------------------------\n",
    "    return df_s3, df_no_enviar\n",
    "\n",
    "\n",
    "\n",
    "# Llamado a la función:\n",
    "Df_S3, DF_No_Enviar = procesar_glosa_GN0302(\n",
    "    df_s3=Df_S3,\n",
    "    df_no_enviar=DF_No_Enviar,\n",
    "    fecha_referencia=Fecha\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ac90f",
   "metadata": {},
   "source": [
    "# Se debe corregir la glosa GN0302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5323",
   "metadata": {},
   "source": [
    "# 6. Guardar informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e872a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(Carpeta) / f\"resultados-{Name}.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    Df_S3.to_excel(writer, sheet_name=\"Df_S3\", index=False)\n",
    "    DF_No_Enviar.to_excel(writer, sheet_name=\"DF_No_Enviar\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
