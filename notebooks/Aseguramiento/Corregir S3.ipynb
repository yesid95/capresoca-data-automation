{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bcde6e",
   "metadata": {},
   "source": [
    "# 1. Carga de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39330bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8a092",
   "metadata": {},
   "source": [
    "# 2. rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta archivos entrada\n",
    "R_Ms_ADRES_EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0019052025.TXT\"\n",
    "R_Ms_ADRES_EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0019052025.TXT\"\n",
    "R_S3 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Procesos BDUA EPS\\S3\\2025-2\\S3EPS02513052025.TXT\"\n",
    "\n",
    "# Ruta archivos salida\n",
    "Carpeta = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\05_Mayo\\20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fecha = \"20/05/2025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04cc76",
   "metadata": {},
   "source": [
    "# 3. Carga Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf878dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\", \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\", \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\", \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO\", \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\", \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\"]\n",
    "\n",
    "Df_EPS025 = pd.read_csv(R_Ms_ADRES_EPS025, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPS025.columns = new_columns\n",
    "\n",
    "Df_EPSC25 = pd.read_csv(R_Ms_ADRES_EPSC25, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_EPSC25.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\", \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\", \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\", \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\", \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\", \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\", \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_MDL_SBS_ID\", \"ENT_ID_ORIGEN\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\", \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\", \"GlOSA\"]\n",
    "Df_S3 = pd.read_csv(R_S3, sep=',', header=None, dtype=str, encoding='ANSI')\n",
    "Df_S3.columns = new_columns\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# Agregar columna \"Enviar\" con un valor inicial vacío\n",
    "Df_S3['Enviar'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a406d4",
   "metadata": {},
   "source": [
    "# 4. Limpier datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar uno debajo del otro\n",
    "DF_ADRES = pd.concat(\n",
    "    [Df_EPS025, Df_EPSC25],\n",
    "    ignore_index=True,   # reindexa de 0…n-1\n",
    "    sort=False           # evita warnings si el orden de columnas coincide\n",
    ")\n",
    "\n",
    "# 2. (Opcional) borrar los DataFrames originales para liberar memoria\n",
    "del Df_EPS025, Df_EPSC25\n",
    "\n",
    "# Seleccionar las columnas de DF_ADRES a transferir, junto con las columnas clave\n",
    "cols_transfer = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"ENT_ID\", \"TPS_EST_AFL_ID\"]\n",
    "df_transfer = DF_ADRES[cols_transfer].drop_duplicates()\n",
    "\n",
    "# Hacemos un merge de Df_S3 con df_transfer mediante las columnas clave\n",
    "Df_S3 = Df_S3.merge(\n",
    "    df_transfer,\n",
    "    on=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from_adres\")\n",
    ")\n",
    "\n",
    "# Si se desea actualizar el valor de ENT_ID en Df_S3 usando el de DF_ADRES,\n",
    "# se puede hacer lo siguiente. De lo contrario, se conservará el valor original:\n",
    "Df_S3[\"ENT_ID\"] = Df_S3[\"ENT_ID_from_adres\"].combine_first(Df_S3[\"ENT_ID\"])\n",
    "\n",
    "# Renombrar la columna importada TPS_EST_AFL_ID (si se quiere conservar con ese nombre)\n",
    "Df_S3.rename(columns={\"TPS_EST_AFL_ID\": \"TPS_EST_AFL_ID_from_adres\"}, inplace=True)\n",
    "\n",
    "# Opcional: se puede eliminar la columna auxiliar ENT_ID_from_adres ya que su valor fue transferido\n",
    "Df_S3.drop(columns=[\"ENT_ID_from_adres\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])\n",
    "\n",
    "# 1. Definimos la máscara con la lógica (cond1 AND cond2)  OR  cond3\n",
    "mask = (\n",
    "    (Df_S3[\"ENT_ID\"] == \"EPS025\") &\n",
    "    (Df_S3[\"TPS_EST_AFL_ID_from_adres\"] == \"AC\")\n",
    ") | Df_S3[\"TIPO_TRASLADO\"].isin([\"0\", \"1\", \"2\"])\n",
    "\n",
    "# 2. Extraemos los registros a enviar\n",
    "DF_No_Enviar = Df_S3.loc[mask].copy()\n",
    "\n",
    "# 3. Eliminamos esos mismos registros del DataFrame original\n",
    "Df_S3 = Df_S3.loc[~mask].copy()\n",
    "\n",
    "print(\"Número de registros en DF_No_Enivar:\", DF_No_Enviar.shape[0])\n",
    "print(\"Número de registros en Df_S3:\", Df_S3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe7bc9",
   "metadata": {},
   "source": [
    "# 5. Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar la columna \"No_Glosas\" contando las glosas separadas por \";\"\n",
    "Df_S3['No_Glosas'] = Df_S3['GlOSA'].apply(\n",
    "    lambda x: len(x.rstrip(';').split(';')) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# Imprimir los valores únicos de la nueva columna\n",
    "print(Df_S3['No_Glosas'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a94785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los primeros 6 caracteres de la columna 'GlOSA' y guardarlos en 'Glosa_Actual'\n",
    "Df_S3['Glosa_Actual'] = Df_S3['GlOSA'].str[:6]\n",
    "\n",
    "# Duplicar la columna 'GlOSA' en una nueva columna 'GlOSA_2'\n",
    "Df_S3['GlOSA_2'] = Df_S3['GlOSA']\n",
    "\n",
    "# Imprimir los valores únicos y la cantidad de registros de cada uno en la columna \"Glosa_Actual\"\n",
    "print(\"Valores únicos en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].unique())\n",
    "\n",
    "print(\"\\nCantidad de registros por cada valor en Glosa_Actual:\")\n",
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69168a9d",
   "metadata": {},
   "source": [
    "# 5.1. Glosa GN0368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def limpiar_glosa_GN0368(row):\n",
    "    \"\"\"\n",
    "    Limpia la glosa GN0368 y ajusta la fecha en formato dd/mm/YYYY.\n",
    "    \"\"\"\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separar tokens por \";\" y filtrar vacíos\n",
    "    tokens = [t for t in glosa_str.split(\";\") if t]\n",
    "\n",
    "    # Buscar token GN0368(\n",
    "    token_gn = next((t for t in tokens if t.startswith(\"GN0368(\")), None)\n",
    "    if not token_gn:\n",
    "        return row\n",
    "\n",
    "    # Extraer fecha en formato dd/mm/YYYY dentro de GN0368(...)\n",
    "    m = re.search(r\"GN0368\\([^()]*?(\\d{2}/\\d{2}/\\d{4})\\)\", token_gn)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    fecha_str = m.group(1)\n",
    "    fecha_dt = datetime.strptime(fecha_str, \"%d/%m/%Y\")\n",
    "    # Sumar un día y formatear de nuevo como dd/mm/YYYY\n",
    "    nueva_fecha = (fecha_dt + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminar el token GN0368 de la lista\n",
    "    restantes = [t for t in tokens if not t.startswith(\"GN0368(\")]\n",
    "    # Reconstruir GlOSA_2\n",
    "    row[\"GlOSA_2\"] = \";\".join(restantes) + (\";\" if restantes else \"\")\n",
    "\n",
    "    # Actualizar Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and restantes:\n",
    "        row[\"Glosa_Actual\"] = restantes[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Aplicación sobre el DataFrame\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_GN0368, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b29bf",
   "metadata": {},
   "source": [
    "# 5.2. Glosa GN0369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_369(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str or \"GN0369(\" not in glosa_str:\n",
    "        return row\n",
    "\n",
    "    # split y quitamos vacíos\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok.strip()]\n",
    "    # buscamos la glosa GN0369\n",
    "    token = next((tok for tok in tokens if tok.startswith(\"GN0369(\")), None)\n",
    "    if not token:\n",
    "        return row\n",
    "\n",
    "    # regex para extraer meses y fecha final\n",
    "    m = re.search(r\"GN0369\\([^|]*\\|(\\d+)\\|[^|]*\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token)\n",
    "    if not m:\n",
    "        return row\n",
    "\n",
    "    meses = int(m.group(1))\n",
    "    fecha_base = datetime.strptime(m.group(2), \"%d/%m/%Y\")\n",
    "\n",
    "    # sumamos meses y un día\n",
    "    nueva_fecha_dt = fecha_base + relativedelta(months=meses) + timedelta(days=1)\n",
    "    nueva_fecha = nueva_fecha_dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # actualizamos la columna de movilidad\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # eliminamos la glosa procesada\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0369(\")]\n",
    "    row[\"GlOSA_2\"] = ( \";\".join(tokens) + \";\" ) if tokens else \"\"\n",
    "\n",
    "    # ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1:\n",
    "        # si quedan otras glosas, tomamos la primera antes del \"(\"\n",
    "        primera = tokens[0]\n",
    "        row[\"Glosa_Actual\"] = primera.split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo al DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_369, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a1ce7",
   "metadata": {},
   "source": [
    "# 5.3. Glosa GN0421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_421(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Separamos tokens y buscamos GN0421(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_421 = next((tok for tok in tokens if tok.startswith(\"GN0421(\")), None)\n",
    "    if not token_421:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha\n",
    "    m = re.search(r\"GN0421\\((\\d{2}/\\d{2}/\\d{4})\\)\", token_421)\n",
    "    if not m:\n",
    "        return row\n",
    "    fecha_base = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "\n",
    "    # Sumamos un día\n",
    "    nueva_fecha = (fecha_base + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "    row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "\n",
    "    # Eliminamos el token procesado\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0421(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_421, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52b0ed",
   "metadata": {},
   "source": [
    "# 5.4. Glosa GN0084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_glosa_0084(row):\n",
    "    glosa_str = row.get(\"GlOSA_2\", \"\")\n",
    "    if not glosa_str:\n",
    "        return row\n",
    "\n",
    "    # Partimos tokens y buscamos GN0084(\n",
    "    tokens = [tok for tok in glosa_str.split(\";\") if tok]\n",
    "    token_84 = next((tok for tok in tokens if tok.startswith(\"GN0084(\")), None)\n",
    "    if not token_84:\n",
    "        return row\n",
    "\n",
    "    # Extraemos la fecha tras el '|'\n",
    "    m = re.search(r\"GN0084\\([^|]+\\|(\\d{2}/\\d{2}/\\d{4})\\)\", token_84)\n",
    "    if m:\n",
    "        fecha_glosa = datetime.strptime(m.group(1), \"%d/%m/%Y\")\n",
    "        # Intentamos parsear la fecha existente en FECHA_AFILIACION_MOVILIDAD\n",
    "        try:\n",
    "            fecha_actual = datetime.strptime(row.get(\"FECHA_AFILIACION_MOVILIDAD\",\"\"), \"%d/%m/%Y\")\n",
    "        except Exception:\n",
    "            fecha_actual = None\n",
    "\n",
    "        # Solo si la fecha actual es anterior a la de la glosa, la actualizamos\n",
    "        if fecha_actual and fecha_actual < fecha_glosa:\n",
    "            nueva = (fecha_glosa + timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva\n",
    "\n",
    "    # Eliminamos siempre el token GN0084\n",
    "    tokens = [tok for tok in tokens if not tok.startswith(\"GN0084(\")]\n",
    "    row[\"GlOSA_2\"] = (\";\".join(tokens) + \";\") if tokens else \"\"\n",
    "\n",
    "    # Ajustamos Glosa_Actual\n",
    "    if row.get(\"No_Glosas\", 0) > 1 and tokens:\n",
    "        row[\"Glosa_Actual\"] = tokens[0].split(\"(\")[0]\n",
    "    else:\n",
    "        row[\"Glosa_Actual\"] = \"\"\n",
    "\n",
    "    return row\n",
    "\n",
    "# Para aplicarlo sobre tu DataFrame:\n",
    "Df_S3 = Df_S3.apply(limpiar_glosa_0084, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5323",
   "metadata": {},
   "source": [
    "# 6. Guardar informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e872a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = Path(Carpeta) / \"resultados.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    Df_S3.to_excel(writer, sheet_name=\"Df_S3\", index=False)\n",
    "    DF_No_Enviar.to_excel(writer, sheet_name=\"DF_No_Enviar\", index=False)\n",
    "    DF_ADRES.to_excel(writer, sheet_name=\"DF_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Df_S3['Glosa_Actual'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
