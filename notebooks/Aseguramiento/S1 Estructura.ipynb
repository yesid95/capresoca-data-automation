{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modulos, Clases y funciones Externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar de Python\n",
    "import datetime  # Manejo de fechas y horas\n",
    "import pandas as pd  # Manipulaci√≥n y an√°lisis de datos en DataFrames\n",
    "import numpy as np  # Operaciones num√©ricas y manejo eficiente de arrays\n",
    "import sys  # Acceso a variables y funciones del sistema\n",
    "import re  # Expresiones regulares para procesamiento de texto\n",
    "import os  # Operaciones del sistema de archivos\n",
    "\n",
    "# A√±adir la carpeta ra√≠z del proyecto al sys.path para importar m√≥dulos personalizados\n",
    "#sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "#sys.path.append(os.path.abspath(r\"C:\\Users\\crist\\Documents\\Proyectos Python\\capresoca-data-automation\"))  # Ruta alternativa (comentada)\n",
    "\n",
    "# Importar funci√≥n y clase personalizada del proyecto\n",
    "#from src.file_loader import cargar_maestros_ADRES  # Funci√≥n para cargar archivos maestros ADRES\n",
    "#from src.data_cleaning import BduaReportProcessor      # Clase para limpiar y normalizar poblaci√≥n Maestro ADRES\n",
    "#from src.data_cleaning import DataCleaner # Clase para limpiar y normalizar DataFrames de Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Rutas y varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ‚öôÔ∏è PAR√ÅMETROS DE CONFIGURACI√ìN MANUAL\n",
    "# ============================================================================\n",
    "# üîß ACTUALIZA ESTOS VALORES CADA EJECUCI√ìN:\n",
    "\n",
    "# 1. FECHA Y CARPETA\n",
    "ANO = \"2026\"\n",
    "MES_NUMERO = \"02\"  # Dos d√≠gitos: \"01\", \"02\", etc.\n",
    "MES_NOMBRE = \"Febrero\"  # Nombre del mes\n",
    "DIA = \"10\"\n",
    "\n",
    "# Variables de fecha usadas en el flujo\n",
    "fecha_archivo = f\"{DIA}-{MES_NUMERO}-{ANO}\"  # \"03-02-2026\"\n",
    "\n",
    "# 2. ARCHIVOS DIN√ÅMICOS (cambian cada ejecuci√≥n)\n",
    "ARCHIVO_MS_ADRES_EPSC25 = \"EPSC25MC0006022026.TXT\"  # Maestro Contributivo\n",
    "ARCHIVO_MS_ADRES_EPS025 = \"EPS025MS0006022026.TXT\"  # Maestro Subsidiado\n",
    "ARCHIVO_PILA_VALIDADA = f\"Dataframe Pila {fecha_archivo}.xlsx\"\n",
    "ARCHIVO_S1_SALIDA = f\"S1 Estructura {fecha_archivo}.xlsx\"\n",
    "\n",
    "# 3. FECHAS DE CONTROL (objetos datetime)\n",
    "# Fecha m√≠nima permitida para movilidad (primer d√≠a del mes anterior al reporte)\n",
    "FECHA_MINIMA_ANO = 2025\n",
    "FECHA_MINIMA_MES = 12\n",
    "FECHA_MINIMA_DIA = 1\n",
    "\n",
    "# Fecha del reporte actual\n",
    "FECHA_REPORTE_ANO = int(ANO)\n",
    "FECHA_REPORTE_MES = int(MES_NUMERO)\n",
    "FECHA_REPORTE_DIA = int(DIA)\n",
    "\n",
    "# Fecha del pr√≥ximo reporte (para validaci√≥n)\n",
    "FECHA_PROXI_REPORTE_ANO = 2026\n",
    "FECHA_PROXI_REPORTE_MES = 2\n",
    "FECHA_PROXI_REPORTE_DIA = 17\n",
    "\n",
    "# ============================================================================\n",
    "# DETECCI√ìN AUTOM√ÅTICA DE RUTAS (UNIVERSAL)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Obtener usuario del sistema autom√°ticamente\n",
    "usuario = os.environ.get('USERNAME') or os.environ.get('USER')\n",
    "user_home = os.path.expanduser(\"~\")\n",
    "\n",
    "# Rutas base\n",
    "ONEDRIVE_BASE = os.path.join(\n",
    "    user_home, \n",
    "    \"OneDrive - 891856000_CAPRESOCA E P S\", \n",
    "    \"Capresoca\", \n",
    "    \"AlmostClear\"\n",
    ")\n",
    "\n",
    "ESCRITORIO = os.path.join(\n",
    "    user_home,\n",
    "    \"OneDrive - 891856000_CAPRESOCA E P S\",\n",
    "    \"Escritorio\"\n",
    ")\n",
    "\n",
    "# Ruta del proyecto (universal)\n",
    "PROYECTO_RAIZ = os.path.join(user_home, \"Documents\", \"Proyectos Python\", \"capresoca-data-automation\")\n",
    "sys.path.append(os.path.abspath(PROYECTO_RAIZ))\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTAR M√ìDULOS PERSONALIZADOS\n",
    "# ============================================================================\n",
    "\n",
    "from src.file_loader import cargar_maestros_ADRES  # Funci√≥n para cargar archivos maestros ADRES\n",
    "from src.data_cleaning import BduaReportProcessor  # Clase para limpiar y normalizar poblaci√≥n Maestro ADRES\n",
    "from src.data_cleaning import DataCleaner          # Clase para limpiar y normalizar DataFrames de Pandas\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTRUCCI√ìN DE RUTAS AUTOM√ÅTICAS\n",
    "# ============================================================================\n",
    "\n",
    "# --- RUTAS DE ENTRADA ---\n",
    "R_Maestro__EPSC25 = os.path.join(ONEDRIVE_BASE, \"Procesos BDUA\", \"Contributivo\", \"Maestro\", ANO, ARCHIVO_MS_ADRES_EPSC25)\n",
    "R_Maestro__EPS025 = os.path.join(ONEDRIVE_BASE, \"Procesos BDUA\", \"Subsidiados\", \"Maestro\", \"MS\", ANO, ARCHIVO_MS_ADRES_EPS025)\n",
    "\n",
    "# --- RUTA DE ENTRADA/SALIDA (CARPETA DE TRABAJO) ---\n",
    "Carpeta = os.path.join(ESCRITORIO, \"Yesid Rinc√≥n Z\", \"Traslados\", \"Procesos BDUA\", ANO, f\"{MES_NUMERO}_{MES_NOMBRE}\", DIA)\n",
    "R_Pila_Validada = os.path.join(Carpeta, ARCHIVO_PILA_VALIDADA)\n",
    "S_Excel = os.path.join(Carpeta, ARCHIVO_S1_SALIDA)\n",
    "\n",
    "# Crear carpeta de trabajo si no existe\n",
    "os.makedirs(Carpeta, exist_ok=True)\n",
    "\n",
    "# --- OBJETOS DE FECHA PARA VALIDACI√ìN ---\n",
    "fecha_Minima = datetime.datetime(FECHA_MINIMA_ANO, FECHA_MINIMA_MES, FECHA_MINIMA_DIA)\n",
    "fecha_reporte = datetime.datetime(FECHA_REPORTE_ANO, FECHA_REPORTE_MES, FECHA_REPORTE_DIA)\n",
    "fecha_proxi_reporte = datetime.datetime(FECHA_PROXI_REPORTE_ANO, FECHA_PROXI_REPORTE_MES, FECHA_PROXI_REPORTE_DIA)\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACI√ìN Y RESUMEN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üë§ USUARIO: {usuario}\")\n",
    "print(f\"üìÅ OneDrive Base: {ONEDRIVE_BASE}\")\n",
    "print(f\"üìÅ Proyecto: {PROYECTO_RAIZ}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìã PAR√ÅMETROS CONFIGURADOS:\")\n",
    "print(f\"   üìÖ Fecha archivo: {fecha_archivo}\")\n",
    "print(f\"   üìÇ A√±o: {ANO} | Mes: {MES_NUMERO}_{MES_NOMBRE} | D√≠a: {DIA}\")\n",
    "print(f\"   üìÑ Maestro EPSC25: {ARCHIVO_MS_ADRES_EPSC25}\")\n",
    "print(f\"   üìÑ Maestro EPS025: {ARCHIVO_MS_ADRES_EPS025}\")\n",
    "print(f\"   üìä Pila Validada: {ARCHIVO_PILA_VALIDADA}\")\n",
    "print(f\"   üìÑ Salida S1: {ARCHIVO_S1_SALIDA}\")\n",
    "\n",
    "print(f\"\\nüìÜ FECHAS DE CONTROL:\")\n",
    "print(f\"   üìÖ Fecha m√≠nima: {fecha_Minima.strftime('%d/%m/%Y')}\")\n",
    "print(f\"   üìÖ Fecha reporte: {fecha_reporte.strftime('%d/%m/%Y')}\")\n",
    "print(f\"   üìÖ Pr√≥ximo reporte: {fecha_proxi_reporte.strftime('%d/%m/%Y')}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üîç VALIDACI√ìN DE RUTAS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rutas_validar = {\n",
    "    \"üìÑ Maestro EPSC25\": R_Maestro__EPSC25,\n",
    "    \"üìÑ Maestro EPS025\": R_Maestro__EPS025,\n",
    "    \"üìä Pila Validada\": R_Pila_Validada,\n",
    "    \"üìÇ Carpeta de trabajo\": Carpeta\n",
    "}\n",
    "\n",
    "errores = []\n",
    "for nombre, ruta in rutas_validar.items():\n",
    "    existe = os.path.exists(ruta)\n",
    "    icono = \"‚úÖ\" if existe else \"‚ùå\"\n",
    "    print(f\"{icono} {nombre}\")\n",
    "    print(f\"   {ruta}\")\n",
    "    if not existe and \"Carpeta de trabajo\" not in nombre:\n",
    "        errores.append(nombre)\n",
    "\n",
    "if errores:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(errores)} ruta(s) no encontrada(s)\")\n",
    "    print(\"\\nüí° Verifica:\")\n",
    "    print(\"   1. Que OneDrive est√© sincronizado\")\n",
    "    print(\"   2. Que los par√°metros al inicio est√©n actualizados\")\n",
    "    print(\"   3. Que los archivos existan en las carpetas\")\n",
    "    print(\"   4. Que el archivo 'Dataframe Pila' se haya generado previamente\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Todas las rutas validadas correctamente\")\n",
    "\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home\n",
    "#R_Pila_Validada = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2026\\01_Enero\\06\\Dataframe Pila 06-01-2026.xlsx\"\n",
    "#R_Maestro__EPSC25 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2026\\EPSC25MC0005012026.TXT\"\n",
    "#R_Maestro__EPS025 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2026\\EPS025MS0005012026.TXT\"\n",
    "\n",
    "#fecha_archivo = \"06-01-2026\"\n",
    "#S_Excel = fr\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rinc√≥n Z\\Traslados\\Procesos BDUA\\2026\\01_Enero\\06\\S1 Estructura {fecha_archivo}.xlsx\"\n",
    "\n",
    "# Crear el objeto de fecha\n",
    "#fecha_Minima = datetime.datetime(2025, 11, 1)\n",
    "#fecha_reporte = datetime.datetime(2026, 1, 6)\n",
    "#fecha_proxi_reporte = datetime.datetime(2026, 1, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Cargue Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar los maestros\n",
    "maestro_ADRES = cargar_maestros_ADRES(R_Maestro__EPS025, R_Maestro__EPSC25)\n",
    "Pila_Validada = pd.read_excel(R_Pila_Validada, sheet_name=\"Sheet1\", header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Listado censal o Sisben ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicar la columna \"MARCASISBENIV+MARCASISBENIII_2\" y nombrarla \"MARCASISBENIV+MARCASISBENIII\"\n",
    "maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII_2\"] = \\\n",
    "    maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instanciar el procesador: Se crea un objeto pasando el DataFrame.\n",
    "#    La jerarqu√≠a de poblaci√≥n ya est√° definida por defecto dentro de la clase.\n",
    "processor = BduaReportProcessor(df=maestro_ADRES)\n",
    "\n",
    "# 2. Ejecutar la limpieza y asignarla de vuelta.\n",
    "#    El m√©todo retorna un DataFrame completamente nuevo con la columna actualizada.\n",
    "maestro_ADRES = processor.prioritize_population_markers(\n",
    "    col_name=\"MARCASISBENIV+MARCASISBENIII\"\n",
    ")\n",
    "\n",
    "# ¬°Listo! 'maestro_ADRES' ahora contiene los datos limpios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Contrucci√≥n S1 movilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Pila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas del nuevo DataFrame\n",
    "new_columns = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\",\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\",\n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "    \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "]\n",
    "\n",
    "# Filtrar registros donde 'Movilidad' contiene 'S1'\n",
    "mask = Pila_Validada[\"Movilidad\"].str.contains(\"S1\", na=False)\n",
    "filtered = Pila_Validada.loc[mask]\n",
    "\n",
    "# Crear el nuevo DataFrame S1 con valores vac√≠os\n",
    "S1 = pd.DataFrame('', index=filtered.index, columns=new_columns)\n",
    "\n",
    "# Asignar los valores requeridos\n",
    "S1.loc[:, \"TPS_IDN_ID\"] = filtered[\"Tipo Documento Cotizante\"]\n",
    "S1.loc[:, \"HST_IDN_NUMERO_IDENTIFICACION\"] = filtered[\"N¬∞ Identificaci√≥n Cotizante\"]\n",
    "S1.loc[:, \"FECHA_AFILIACION_MOVILIDAD\"] = filtered[\"Fecha envio\"]\n",
    "S1.loc[:, \"CND_AFL_SBS_SUBGRUPO_SIV\"] = filtered[\"Poblaci√≥n_2\"]\n",
    "\n",
    "# Mostrar cantidad de registros, columnas y porcentaje de vac√≠os por columna\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "print(\"Porcentaje de vac√≠os por columna:\")\n",
    "print(S1.isin(['', None, np.nan]).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"Where\"] = \"Pila\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Beneficairios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: mostrar nombres de columnas antes de cualquier operaci√≥n\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns antes de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns antes de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Limpiar espacios invisibles en los nombres de columna\n",
    "# ----------------------------------------------------\n",
    "S1.columns            = S1.columns.str.strip()\n",
    "maestro_ADRES.columns = maestro_ADRES.columns.str.strip()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: volver a mostrar nombres de columnas despu√©s de strip()\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns despu√©s de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns despu√©s de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 0) Cantidad de registros y columnas en S1\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Preparar s√≥lo las claves de S1 (cabezas de familia),\n",
    "#    incluyendo FECHA_AFILIACION_MOVILIDAD para luego propagarla\n",
    "# ----------------------------------------------------\n",
    "s1_keys = (\n",
    "    S1[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"FECHA_AFILIACION_MOVILIDAD\"]]\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Emparejar S1 (cabezas) con maestro_ADRES (beneficiarios)\n",
    "#    - Usamos sufijos para distinguir columnas duplicadas\n",
    "# ----------------------------------------------------\n",
    "beneficiarios = (\n",
    "    maestro_ADRES\n",
    "    .merge(\n",
    "        s1_keys,\n",
    "        left_on=[\"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "        right_on=[\"TPS_IDN_ID\",    \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_master\", \"_s1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Debug: ver columnas que resultaron de este merge\n",
    "print(\">> beneficiarios columns:\")\n",
    "print(beneficiarios.columns.tolist())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Construir los nuevos registros con las columnas solicitadas,\n",
    "#    propagando FECHA_AFILIACION_MOVILIDAD desde la familia cabeza\n",
    "# ----------------------------------------------------\n",
    "nuevos = pd.DataFrame({\n",
    "    # ID del beneficiario (de maestro_ADRES con sufijo _master)\n",
    "    \"TPS_IDN_ID\":                        beneficiarios[\"TPS_IDN_ID_master\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\":     beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_master\"],\n",
    "    # Subgrupo seg√∫n MARCASISBENIV+MARCASISBENIII\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\":          beneficiarios[\"MARCASISBENIV+MARCASISBENIII\"],\n",
    "    # IDs de la cabeza de familia\n",
    "    \"TPS_IDN_CF_ID\":                     beneficiarios[\"TPS_IDN_ID_CF\"],\n",
    "    \"HST_IDN_NUMERO_CF_IDENTIFICACION\":  beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "    # TPS_PRN_ID del beneficiario\n",
    "    \"TPS_PRN_ID\":                        beneficiarios[\"TPS_PRN_ID\"],\n",
    "    # Marcamos estos nuevos registros con la etiqueta fija\n",
    "    \"Where\":                             \"Beneficiarios Pila\",\n",
    "    # Propagamos la misma fecha de movilidad que ten√≠a la cabeza de familia\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\":        beneficiarios[\"FECHA_AFILIACION_MOVILIDAD\"],\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) A√±adir los nuevos registros al final de S1\n",
    "# ----------------------------------------------------\n",
    "S1 = pd.concat([S1, nuevos], ignore_index=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Mostrar resumen final\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros despu√©s del concat: {S1.shape[0]}\")\n",
    "print(f\"Columnas despu√©s del concat:  {S1.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Fechas de Beneficairios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Sisben Beneficiarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar el mapeo de los valores de maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "# a S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] solo donde S1[\"Where\"] == \"Beneficiarios Pila\"\n",
    "\n",
    "# Crear un diccionario para b√∫squeda r√°pida por ID\n",
    "maestro_map = maestro_ADRES.set_index([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "\n",
    "# Actualizar S1 usando .apply para mantener el nombre de las columnas\n",
    "def actualizar_subgrupo(row):\n",
    "    if row.get(\"Where\") == \"Beneficiarios Pila\":\n",
    "        key = (row[\"TPS_IDN_ID\"], row[\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "        valor = maestro_map.get(key, row[\"CND_AFL_SBS_SUBGRUPO_SIV\"])\n",
    "        return valor\n",
    "    return row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "\n",
    "S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = S1.apply(actualizar_subgrupo, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Datos del Maestro al S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Defino c√≥mo se llaman en maestro ‚Üí c√≥mo quiero que queden en S1\n",
    "column_mapping = {\n",
    "    \"AFL_PAIS_NACIMIENTO\":     \"PAIS_NACIMIENTO\",\n",
    "    \"AFL_MUNICIPIO_NACIMIENTO\":\"LUGAR_NACIMIENTO\",\n",
    "    \"AFL_NACIONALIDAD\":        \"NACIONALIDAD\",\n",
    "    \"AFL_SEXO_IDENTIFICACION\": \"SEXO_IDENTIFICACION\",\n",
    "    \"AFL_DISCAPACIDAD\":        \"TIPO_DISCAPACIDAD\",\n",
    "    \"DPR_ID\":                  \"DPR_ID\",\n",
    "    \"MNC_ID\":                  \"MNC_ID\",\n",
    "    \"ZNS_ID\":                  \"ZNS_ID\",\n",
    "    \"TPS_AFL_ID\":              \"TPS_AFL_ID\",\n",
    "    # (y las que ya ten√≠as antes‚Ä¶)\n",
    "    \"AFL_PRIMER_APELLIDO\":    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\":   \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\":      \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\":     \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\":   \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\":             \"TPS_GNR_ID\",\n",
    "}\n",
    "\n",
    "join_keys = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# 2) Preparo el maestro: me quedo solo con las columnas que necesito y\n",
    "#    las renombro para que coincidan con S1:\n",
    "maestro_subset = (\n",
    "    maestro_ADRES[list(column_mapping.keys()) + join_keys]\n",
    "    .rename(columns=column_mapping)\n",
    ")\n",
    "\n",
    "# 3) Hago el merge contra S1; pandas a√±adir√° un sufijo solo a las columnas\n",
    "#    del maestro (porque ya existen en S1):\n",
    "S1_merged = S1.merge(\n",
    "    maestro_subset,\n",
    "    on=join_keys,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_maestro\")\n",
    ")\n",
    "\n",
    "# 4) Ahora, para cada columna de destino 'col', relleno S1[col] solo donde\n",
    "#    est√© vac√≠o usando la versi√≥n '_maestro', y luego elimino esa columna:\n",
    "for col in column_mapping.values():\n",
    "    maestro_col = f\"{col}_maestro\"\n",
    "    if maestro_col in S1_merged:\n",
    "        S1_merged[col] = S1_merged[col].replace(\"\", pd.NA) \\\n",
    "                                       .fillna(S1_merged[maestro_col]) \\\n",
    "                                       .fillna(\"\")  # opcional volver a cadena vac√≠a\n",
    "        S1_merged.drop(columns=maestro_col, inplace=True)\n",
    "\n",
    "# 5) S1_merged es tu S1 final, con el mismo esquema de columnas y\n",
    "#    solo rellenando lo que estaba vac√≠o:\n",
    "S1 = S1_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar los valores de las columnas base a las columnas \"_2\" en S1\n",
    "cols_base = [\n",
    "    \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\"\n",
    "]\n",
    "cols_2 = [\n",
    "    \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\",\n",
    "    \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\",\n",
    "    \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\",\n",
    "    \"AFL_FECHA_NACIMIENTO_2\",\n",
    "    \"TPS_GNR_ID_2\"\n",
    "]\n",
    "\n",
    "for base, col2 in zip(cols_base, cols_2):\n",
    "    S1[col2] = S1[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Sisben S1 \n",
    "1. Tipo de poblaci√≥n \"TPS_GRP_PBL_ID\"\n",
    "2. Nivel de sisben \"TPS_NVL_SSB_ID\"\n",
    "3. Metodologia del sisben \"CND_AFL_SBS_METODOLOGIA\"\n",
    "4. Grupo sisben \"CND_AFL_SBS_SUBGRUPO_SIV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_nvl_ssb(subgrupo):\n",
    "    if pd.isna(subgrupo):\n",
    "        return \"\"\n",
    "    lc_match = re.match(r\"LC\\((\\d+)\\)\", subgrupo)\n",
    "    if lc_match:\n",
    "        return lc_match.group(1)\n",
    "    siv_match = re.match(r\"SIV\\([A-Z0-9]+\\)\", subgrupo)\n",
    "    if siv_match:\n",
    "        return \"5\"\n",
    "    return \"\"\n",
    "\n",
    "S1[\"TPS_GRP_PBL_ID\"] = S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"].apply(extract_nvl_ssb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignaci√≥n condicional seg√∫n las reglas descritas\n",
    "def assign_values(row):\n",
    "    grp_pbl_id = row[\"TPS_GRP_PBL_ID\"]\n",
    "    subgrupo = row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "    if grp_pbl_id != \"5\":\n",
    "        row[\"TPS_NVL_SSB_ID\"] = \"N\"\n",
    "        row[\"CND_AFL_SBS_METODOLOGIA\"] = \"3\"\n",
    "        row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    else:\n",
    "        # Extraer la letra y n√∫mero de SIV(xxx)\n",
    "        match = re.match(r\"SIV\\(([A-Z])(\\d{2})\\)\", subgrupo)\n",
    "        if match:\n",
    "            letra = match.group(1)\n",
    "            numero = match.group(2)\n",
    "            if letra in [\"A\", \"B\"]:\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"1\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            elif letra == \"C\":\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"2\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = f\"{letra}{numero}\"\n",
    "        else:\n",
    "            # Si no coincide con el patr√≥n, dejar vac√≠o\n",
    "            row[\"TPS_NVL_SSB_ID\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_METODOLOGIA\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    return row\n",
    "\n",
    "S1 = S1.apply(assign_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Tipo y Fechas de movilidad\n",
    "1. Se asigna el tipo de movilidad, acorde al corte del proceso a reportar \"TIPO_TRASLADO\"\n",
    "2. se valida y se corigue la fecha de ser mayor a la de la fecha del reporte \"FECHA_AFILIACION_MOVILIDAD\"  y menor a la fecha del sigueinte reporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S1[\"FECHA_AFILIACION_MOVILIDAD\"].head())\n",
    "print(S1[\"FECHA_AFILIACION_MOVILIDAD\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Asignar valor 3 por defecto\n",
    "S1[\"TIPO_TRASLADO\"] = \"3\"\n",
    "\n",
    "# 2. Convertir la columna de fechas a datetime si a√∫n no lo es\n",
    "if not pd.api.types.is_datetime64_any_dtype(S1[\"FECHA_AFILIACION_MOVILIDAD\"]):\n",
    "    try:\n",
    "        S1[\"FECHA_AFILIACION_MOVILIDAD\"] = pd.to_datetime(\n",
    "            S1[\"FECHA_AFILIACION_MOVILIDAD\"], errors=\"raise\", format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    except Exception:\n",
    "        # Si no tiene hora o viene en otro formato v√°lido ISO\n",
    "        S1[\"FECHA_AFILIACION_MOVILIDAD\"] = pd.to_datetime(\n",
    "            S1[\"FECHA_AFILIACION_MOVILIDAD\"], errors=\"coerce\", infer_datetime_format=True\n",
    "        )\n",
    "\n",
    "# Validar que todas las fechas se hayan convertido correctamente\n",
    "errores_fecha = S1[\"FECHA_AFILIACION_MOVILIDAD\"].isna().sum()\n",
    "if errores_fecha > 0:\n",
    "    print(f\"‚ùå {errores_fecha} registros con fechas inv√°lidas despu√©s de la conversi√≥n.\")\n",
    "    print(S1[S1[\"FECHA_AFILIACION_MOVILIDAD\"].isna()].head())\n",
    "else:\n",
    "    print(\"‚úÖ Todas las fechas fueron convertidas correctamente a datetime.\")\n",
    "\n",
    "# 3. Instanciar el limpiador (si aplica l√≥gica adicional)\n",
    "cleaner = DataCleaner(df=S1)\n",
    "S1 = cleaner.process_date_column(column_name=\"FECHA_AFILIACION_MOVILIDAD\")\n",
    "\n",
    "# 4. Inicializar columna de validaci√≥n\n",
    "S1[\"Validaci√≥n fecha\"] = \"\"\n",
    "\n",
    "# 5. L√≥gica de validaci√≥n de fecha por registro\n",
    "def validar_fecha(row):\n",
    "    fecha = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "    if pd.isnull(fecha):\n",
    "        return row\n",
    "\n",
    "    if fecha < fecha_Minima:\n",
    "        row[\"FECHA_AFILIACION_MOVILIDAD\"] = fecha_Minima\n",
    "        row[\"TIPO_TRASLADO\"] = \"3\"\n",
    "        row[\"Validaci√≥n fecha\"] = \"\"\n",
    "    elif fecha > fecha_reporte:\n",
    "        if fecha < fecha_proxi_reporte:\n",
    "            nueva_fecha = fecha - pd.DateOffset(months=1)\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "            row[\"TIPO_TRASLADO\"] = \"4\"\n",
    "            row[\"Validaci√≥n fecha\"] = \"\"\n",
    "        else:\n",
    "            row[\"TIPO_TRASLADO\"] = \"\"\n",
    "            row[\"Validaci√≥n fecha\"] = \"proceso para proximos reportes\"\n",
    "    else:\n",
    "        row[\"TIPO_TRASLADO\"] = \"3\"\n",
    "        row[\"Validaci√≥n fecha\"] = \"\"\n",
    "    return row\n",
    "\n",
    "# 6. Aplicar la funci√≥n fila por fila\n",
    "S1 = S1.apply(validar_fecha, axis=1)\n",
    "\n",
    "# 7. Convertir a formato string DD/MM/YYYY para exportar\n",
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] = S1[\"FECHA_AFILIACION_MOVILIDAD\"].apply(\n",
    "    lambda x: x.strftime(\"%d/%m/%Y\") if pd.notnull(x) else \"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Valores Absolutos\n",
    "Valores que debe tener la estructura de manera automatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"ENT_ID\"] = \"EPS025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Municipio Nacimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar c√≥digo de municipio de nacimiento en LUGAR_NACIMIENTO si est√° vac√≠o y PAIS_NACIMIENTO es \"COL\"\n",
    "mask = (S1[\"LUGAR_NACIMIENTO\"].isin([\"\", None, pd.NA])) & (S1[\"PAIS_NACIMIENTO\"] == \"COL\")\n",
    "\n",
    "def municipio_codigo(row):\n",
    "    if mask.loc[row.name]:\n",
    "        dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "        mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "        return dpr + mnc\n",
    "    return row[\"LUGAR_NACIMIENTO\"]\n",
    "\n",
    "S1[\"LUGAR_NACIMIENTO\"] = S1.apply(municipio_codigo, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Guardamos Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(S_Excel, engine=\"openpyxl\") as writer:\n",
    "    S1.to_excel(writer, sheet_name=\"S1\", index=False)\n",
    "    maestro_ADRES.to_excel(writer, sheet_name=\"maestro_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", S_Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "maestro_ADRES.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
