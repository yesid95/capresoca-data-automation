{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modulos, Clases y funciones Externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías estándar de Python\n",
    "import datetime  # Manejo de fechas y horas\n",
    "import pandas as pd  # Manipulación y análisis de datos en DataFrames\n",
    "import numpy as np  # Operaciones numéricas y manejo eficiente de arrays\n",
    "import sys  # Acceso a variables y funciones del sistema\n",
    "import re  # Expresiones regulares para procesamiento de texto\n",
    "import os  # Operaciones del sistema de archivos\n",
    "\n",
    "# Añadir la carpeta raíz del proyecto al sys.path para importar módulos personalizados\n",
    "#sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "sys.path.append(os.path.abspath(r\"C:\\Users\\crist\\Documents\\Proyectos Python\\capresoca-data-automation\"))  # Ruta alternativa (comentada)\n",
    "\n",
    "# Importar función y clase personalizada del proyecto\n",
    "from src.file_loader import cargar_maestros_ADRES  # Función para cargar archivos maestros ADRES\n",
    "from src.data_cleaning import BduaReportProcessor      # Clase para limpiar y normalizar población Maestro ADRES\n",
    "from src.data_cleaning import DataCleaner # Clase para limpiar y normalizar DataFrames de Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Rutas y varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFICCE\n",
    "#R_Pila_Validada = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\02\\Dataframe Pila 02-12-2025.xlsx\"\n",
    "#R_Maestro__EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0027112025.TXT\"\n",
    "#R_Maestro__EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-02\\EPS025MS0027112025.TXT\"\n",
    "\n",
    "#fecha_archivo = \"02-12-2025\"\n",
    "\n",
    "#S_Excel = fr\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\02\\S1 Estructura {fecha_archivo}.xlsx\"\n",
    "\n",
    "# Crear el objeto de fecha\n",
    "#fecha_Minima = datetime.datetime(2025, 10, 1)\n",
    "#fecha_reporte = datetime.datetime(2025, 12, 2)\n",
    "#fecha_proxi_reporte = datetime.datetime(2025, 12, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home\n",
    "R_Pila_Validada = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\10\\Dataframe Pila 10-12-2025.xlsx\"\n",
    "R_Maestro__EPSC25 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0009122025.TXT\"\n",
    "R_Maestro__EPS025 = r\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-02\\EPS025MS0009122025.TXT\"\n",
    "\n",
    "fecha_archivo = \"10-12-2025\"\n",
    "\n",
    "S_Excel = fr\"C:\\Users\\crist\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\12_Diciembre\\10\\S1 Estructura {fecha_archivo}.xlsx\"\n",
    "\n",
    "# Crear el objeto de fecha\n",
    "fecha_Minima = datetime.datetime(2025, 10, 1)\n",
    "fecha_reporte = datetime.datetime(2025, 12, 10)\n",
    "fecha_proxi_reporte = datetime.datetime(2025, 12, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Cargue Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar los maestros\n",
    "maestro_ADRES = cargar_maestros_ADRES(R_Maestro__EPS025, R_Maestro__EPSC25)\n",
    "Pila_Validada = pd.read_excel(R_Pila_Validada, sheet_name=\"Sheet1\", header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Listado censal o Sisben ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicar la columna \"MARCASISBENIV+MARCASISBENIII_2\" y nombrarla \"MARCASISBENIV+MARCASISBENIII\"\n",
    "maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII_2\"] = \\\n",
    "    maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instanciar el procesador: Se crea un objeto pasando el DataFrame.\n",
    "#    La jerarquía de población ya está definida por defecto dentro de la clase.\n",
    "processor = BduaReportProcessor(df=maestro_ADRES)\n",
    "\n",
    "# 2. Ejecutar la limpieza y asignarla de vuelta.\n",
    "#    El método retorna un DataFrame completamente nuevo con la columna actualizada.\n",
    "maestro_ADRES = processor.prioritize_population_markers(\n",
    "    col_name=\"MARCASISBENIV+MARCASISBENIII\"\n",
    ")\n",
    "\n",
    "# ¡Listo! 'maestro_ADRES' ahora contiene los datos limpios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Contrucción S1 movilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Pila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas del nuevo DataFrame\n",
    "new_columns = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\",\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\",\n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "    \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "]\n",
    "\n",
    "# Filtrar registros donde 'Movilidad' contiene 'S1'\n",
    "mask = Pila_Validada[\"Movilidad\"].str.contains(\"S1\", na=False)\n",
    "filtered = Pila_Validada.loc[mask]\n",
    "\n",
    "# Crear el nuevo DataFrame S1 con valores vacíos\n",
    "S1 = pd.DataFrame('', index=filtered.index, columns=new_columns)\n",
    "\n",
    "# Asignar los valores requeridos\n",
    "S1.loc[:, \"TPS_IDN_ID\"] = filtered[\"Tipo Documento Cotizante\"]\n",
    "S1.loc[:, \"HST_IDN_NUMERO_IDENTIFICACION\"] = filtered[\"N° Identificación Cotizante\"]\n",
    "S1.loc[:, \"FECHA_AFILIACION_MOVILIDAD\"] = filtered[\"Fecha envio\"]\n",
    "S1.loc[:, \"CND_AFL_SBS_SUBGRUPO_SIV\"] = filtered[\"Población_2\"]\n",
    "\n",
    "# Mostrar cantidad de registros, columnas y porcentaje de vacíos por columna\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "print(\"Porcentaje de vacíos por columna:\")\n",
    "print(S1.isin(['', None, np.nan]).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"Where\"] = \"Pila\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Beneficairios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: mostrar nombres de columnas antes de cualquier operación\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns antes de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns antes de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Limpiar espacios invisibles en los nombres de columna\n",
    "# ----------------------------------------------------\n",
    "S1.columns            = S1.columns.str.strip()\n",
    "maestro_ADRES.columns = maestro_ADRES.columns.str.strip()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: volver a mostrar nombres de columnas después de strip()\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns después de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns después de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 0) Cantidad de registros y columnas en S1\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Preparar sólo las claves de S1 (cabezas de familia),\n",
    "#    incluyendo FECHA_AFILIACION_MOVILIDAD para luego propagarla\n",
    "# ----------------------------------------------------\n",
    "s1_keys = (\n",
    "    S1[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"FECHA_AFILIACION_MOVILIDAD\"]]\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Emparejar S1 (cabezas) con maestro_ADRES (beneficiarios)\n",
    "#    - Usamos sufijos para distinguir columnas duplicadas\n",
    "# ----------------------------------------------------\n",
    "beneficiarios = (\n",
    "    maestro_ADRES\n",
    "    .merge(\n",
    "        s1_keys,\n",
    "        left_on=[\"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "        right_on=[\"TPS_IDN_ID\",    \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_master\", \"_s1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Debug: ver columnas que resultaron de este merge\n",
    "print(\">> beneficiarios columns:\")\n",
    "print(beneficiarios.columns.tolist())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Construir los nuevos registros con las columnas solicitadas,\n",
    "#    propagando FECHA_AFILIACION_MOVILIDAD desde la familia cabeza\n",
    "# ----------------------------------------------------\n",
    "nuevos = pd.DataFrame({\n",
    "    # ID del beneficiario (de maestro_ADRES con sufijo _master)\n",
    "    \"TPS_IDN_ID\":                        beneficiarios[\"TPS_IDN_ID_master\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\":     beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_master\"],\n",
    "    # Subgrupo según MARCASISBENIV+MARCASISBENIII\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\":          beneficiarios[\"MARCASISBENIV+MARCASISBENIII\"],\n",
    "    # IDs de la cabeza de familia\n",
    "    \"TPS_IDN_CF_ID\":                     beneficiarios[\"TPS_IDN_ID_CF\"],\n",
    "    \"HST_IDN_NUMERO_CF_IDENTIFICACION\":  beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "    # TPS_PRN_ID del beneficiario\n",
    "    \"TPS_PRN_ID\":                        beneficiarios[\"TPS_PRN_ID\"],\n",
    "    # Marcamos estos nuevos registros con la etiqueta fija\n",
    "    \"Where\":                             \"Beneficiarios Pila\",\n",
    "    # Propagamos la misma fecha de movilidad que tenía la cabeza de familia\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\":        beneficiarios[\"FECHA_AFILIACION_MOVILIDAD\"],\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Añadir los nuevos registros al final de S1\n",
    "# ----------------------------------------------------\n",
    "S1 = pd.concat([S1, nuevos], ignore_index=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Mostrar resumen final\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros después del concat: {S1.shape[0]}\")\n",
    "print(f\"Columnas después del concat:  {S1.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Fechas de Beneficairios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Sisben Beneficiarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar el mapeo de los valores de maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "# a S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] solo donde S1[\"Where\"] == \"Beneficiarios Pila\"\n",
    "\n",
    "# Crear un diccionario para búsqueda rápida por ID\n",
    "maestro_map = maestro_ADRES.set_index([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "\n",
    "# Actualizar S1 usando .apply para mantener el nombre de las columnas\n",
    "def actualizar_subgrupo(row):\n",
    "    if row.get(\"Where\") == \"Beneficiarios Pila\":\n",
    "        key = (row[\"TPS_IDN_ID\"], row[\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "        valor = maestro_map.get(key, row[\"CND_AFL_SBS_SUBGRUPO_SIV\"])\n",
    "        return valor\n",
    "    return row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "\n",
    "S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = S1.apply(actualizar_subgrupo, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Datos del Maestro al S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Defino cómo se llaman en maestro → cómo quiero que queden en S1\n",
    "column_mapping = {\n",
    "    \"AFL_PAIS_NACIMIENTO\":     \"PAIS_NACIMIENTO\",\n",
    "    \"AFL_MUNICIPIO_NACIMIENTO\":\"LUGAR_NACIMIENTO\",\n",
    "    \"AFL_NACIONALIDAD\":        \"NACIONALIDAD\",\n",
    "    \"AFL_SEXO_IDENTIFICACION\": \"SEXO_IDENTIFICACION\",\n",
    "    \"AFL_DISCAPACIDAD\":        \"TIPO_DISCAPACIDAD\",\n",
    "    \"DPR_ID\":                  \"DPR_ID\",\n",
    "    \"MNC_ID\":                  \"MNC_ID\",\n",
    "    \"ZNS_ID\":                  \"ZNS_ID\",\n",
    "    \"TPS_AFL_ID\":              \"TPS_AFL_ID\",\n",
    "    # (y las que ya tenías antes…)\n",
    "    \"AFL_PRIMER_APELLIDO\":    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\":   \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\":      \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\":     \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\":   \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\":             \"TPS_GNR_ID\",\n",
    "}\n",
    "\n",
    "join_keys = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# 2) Preparo el maestro: me quedo solo con las columnas que necesito y\n",
    "#    las renombro para que coincidan con S1:\n",
    "maestro_subset = (\n",
    "    maestro_ADRES[list(column_mapping.keys()) + join_keys]\n",
    "    .rename(columns=column_mapping)\n",
    ")\n",
    "\n",
    "# 3) Hago el merge contra S1; pandas añadirá un sufijo solo a las columnas\n",
    "#    del maestro (porque ya existen en S1):\n",
    "S1_merged = S1.merge(\n",
    "    maestro_subset,\n",
    "    on=join_keys,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_maestro\")\n",
    ")\n",
    "\n",
    "# 4) Ahora, para cada columna de destino 'col', relleno S1[col] solo donde\n",
    "#    esté vacío usando la versión '_maestro', y luego elimino esa columna:\n",
    "for col in column_mapping.values():\n",
    "    maestro_col = f\"{col}_maestro\"\n",
    "    if maestro_col in S1_merged:\n",
    "        S1_merged[col] = S1_merged[col].replace(\"\", pd.NA) \\\n",
    "                                       .fillna(S1_merged[maestro_col]) \\\n",
    "                                       .fillna(\"\")  # opcional volver a cadena vacía\n",
    "        S1_merged.drop(columns=maestro_col, inplace=True)\n",
    "\n",
    "# 5) S1_merged es tu S1 final, con el mismo esquema de columnas y\n",
    "#    solo rellenando lo que estaba vacío:\n",
    "S1 = S1_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar los valores de las columnas base a las columnas \"_2\" en S1\n",
    "cols_base = [\n",
    "    \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\"\n",
    "]\n",
    "cols_2 = [\n",
    "    \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\",\n",
    "    \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\",\n",
    "    \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\",\n",
    "    \"AFL_FECHA_NACIMIENTO_2\",\n",
    "    \"TPS_GNR_ID_2\"\n",
    "]\n",
    "\n",
    "for base, col2 in zip(cols_base, cols_2):\n",
    "    S1[col2] = S1[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Sisben S1 \n",
    "1. Tipo de población \"TPS_GRP_PBL_ID\"\n",
    "2. Nivel de sisben \"TPS_NVL_SSB_ID\"\n",
    "3. Metodologia del sisben \"CND_AFL_SBS_METODOLOGIA\"\n",
    "4. Grupo sisben \"CND_AFL_SBS_SUBGRUPO_SIV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_nvl_ssb(subgrupo):\n",
    "    if pd.isna(subgrupo):\n",
    "        return \"\"\n",
    "    lc_match = re.match(r\"LC\\((\\d+)\\)\", subgrupo)\n",
    "    if lc_match:\n",
    "        return lc_match.group(1)\n",
    "    siv_match = re.match(r\"SIV\\([A-Z0-9]+\\)\", subgrupo)\n",
    "    if siv_match:\n",
    "        return \"5\"\n",
    "    return \"\"\n",
    "\n",
    "S1[\"TPS_GRP_PBL_ID\"] = S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"].apply(extract_nvl_ssb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignación condicional según las reglas descritas\n",
    "def assign_values(row):\n",
    "    grp_pbl_id = row[\"TPS_GRP_PBL_ID\"]\n",
    "    subgrupo = row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "    if grp_pbl_id != \"5\":\n",
    "        row[\"TPS_NVL_SSB_ID\"] = \"N\"\n",
    "        row[\"CND_AFL_SBS_METODOLOGIA\"] = \"3\"\n",
    "        row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    else:\n",
    "        # Extraer la letra y número de SIV(xxx)\n",
    "        match = re.match(r\"SIV\\(([A-Z])(\\d{2})\\)\", subgrupo)\n",
    "        if match:\n",
    "            letra = match.group(1)\n",
    "            numero = match.group(2)\n",
    "            if letra in [\"A\", \"B\"]:\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"1\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            elif letra == \"C\":\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"2\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = f\"{letra}{numero}\"\n",
    "        else:\n",
    "            # Si no coincide con el patrón, dejar vacío\n",
    "            row[\"TPS_NVL_SSB_ID\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_METODOLOGIA\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    return row\n",
    "\n",
    "S1 = S1.apply(assign_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Tipo y Fechas de movilidad\n",
    "1. Se asigna el tipo de movilidad, acorde al corte del proceso a reportar \"TIPO_TRASLADO\"\n",
    "2. se valida y se corigue la fecha de ser mayor a la de la fecha del reporte \"FECHA_AFILIACION_MOVILIDAD\"  y menor a la fecha del sigueinte reporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S1[\"FECHA_AFILIACION_MOVILIDAD\"].head())\n",
    "print(S1[\"FECHA_AFILIACION_MOVILIDAD\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Asignar valor 3 por defecto\n",
    "S1[\"TIPO_TRASLADO\"] = \"3\"\n",
    "\n",
    "# 2. Convertir la columna de fechas a datetime si aún no lo es\n",
    "if not pd.api.types.is_datetime64_any_dtype(S1[\"FECHA_AFILIACION_MOVILIDAD\"]):\n",
    "    try:\n",
    "        S1[\"FECHA_AFILIACION_MOVILIDAD\"] = pd.to_datetime(\n",
    "            S1[\"FECHA_AFILIACION_MOVILIDAD\"], errors=\"raise\", format=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "    except Exception:\n",
    "        # Si no tiene hora o viene en otro formato válido ISO\n",
    "        S1[\"FECHA_AFILIACION_MOVILIDAD\"] = pd.to_datetime(\n",
    "            S1[\"FECHA_AFILIACION_MOVILIDAD\"], errors=\"coerce\", infer_datetime_format=True\n",
    "        )\n",
    "\n",
    "# Validar que todas las fechas se hayan convertido correctamente\n",
    "errores_fecha = S1[\"FECHA_AFILIACION_MOVILIDAD\"].isna().sum()\n",
    "if errores_fecha > 0:\n",
    "    print(f\"❌ {errores_fecha} registros con fechas inválidas después de la conversión.\")\n",
    "    print(S1[S1[\"FECHA_AFILIACION_MOVILIDAD\"].isna()].head())\n",
    "else:\n",
    "    print(\"✅ Todas las fechas fueron convertidas correctamente a datetime.\")\n",
    "\n",
    "# 3. Instanciar el limpiador (si aplica lógica adicional)\n",
    "cleaner = DataCleaner(df=S1)\n",
    "S1 = cleaner.process_date_column(column_name=\"FECHA_AFILIACION_MOVILIDAD\")\n",
    "\n",
    "# 4. Inicializar columna de validación\n",
    "S1[\"Validación fecha\"] = \"\"\n",
    "\n",
    "# 5. Lógica de validación de fecha por registro\n",
    "def validar_fecha(row):\n",
    "    fecha = row[\"FECHA_AFILIACION_MOVILIDAD\"]\n",
    "    if pd.isnull(fecha):\n",
    "        return row\n",
    "\n",
    "    if fecha < fecha_Minima:\n",
    "        row[\"FECHA_AFILIACION_MOVILIDAD\"] = fecha_Minima\n",
    "        row[\"TIPO_TRASLADO\"] = \"3\"\n",
    "        row[\"Validación fecha\"] = \"\"\n",
    "    elif fecha > fecha_reporte:\n",
    "        if fecha < fecha_proxi_reporte:\n",
    "            nueva_fecha = fecha - pd.DateOffset(months=1)\n",
    "            row[\"FECHA_AFILIACION_MOVILIDAD\"] = nueva_fecha\n",
    "            row[\"TIPO_TRASLADO\"] = \"4\"\n",
    "            row[\"Validación fecha\"] = \"\"\n",
    "        else:\n",
    "            row[\"TIPO_TRASLADO\"] = \"\"\n",
    "            row[\"Validación fecha\"] = \"proceso para proximos reportes\"\n",
    "    else:\n",
    "        row[\"TIPO_TRASLADO\"] = \"3\"\n",
    "        row[\"Validación fecha\"] = \"\"\n",
    "    return row\n",
    "\n",
    "# 6. Aplicar la función fila por fila\n",
    "S1 = S1.apply(validar_fecha, axis=1)\n",
    "\n",
    "# 7. Convertir a formato string DD/MM/YYYY para exportar\n",
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] = S1[\"FECHA_AFILIACION_MOVILIDAD\"].apply(\n",
    "    lambda x: x.strftime(\"%d/%m/%Y\") if pd.notnull(x) else \"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"FECHA_AFILIACION_MOVILIDAD\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Valores Absolutos\n",
    "Valores que debe tener la estructura de manera automatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"ENT_ID\"] = \"EPS025\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Municipio Nacimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar código de municipio de nacimiento en LUGAR_NACIMIENTO si está vacío y PAIS_NACIMIENTO es \"COL\"\n",
    "mask = (S1[\"LUGAR_NACIMIENTO\"].isin([\"\", None, pd.NA])) & (S1[\"PAIS_NACIMIENTO\"] == \"COL\")\n",
    "\n",
    "def municipio_codigo(row):\n",
    "    if mask.loc[row.name]:\n",
    "        dpr = str(row[\"DPR_ID\"]).zfill(2)\n",
    "        mnc = str(row[\"MNC_ID\"]).zfill(3)\n",
    "        return dpr + mnc\n",
    "    return row[\"LUGAR_NACIMIENTO\"]\n",
    "\n",
    "S1[\"LUGAR_NACIMIENTO\"] = S1.apply(municipio_codigo, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Guardamos Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.ExcelWriter(S_Excel, engine=\"openpyxl\") as writer:\n",
    "    S1.to_excel(writer, sheet_name=\"S1\", index=False)\n",
    "    maestro_ADRES.to_excel(writer, sheet_name=\"maestro_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", S_Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "maestro_ADRES.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
