{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3884943",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PoblacionCleaner' from 'src.data_cleaning' (c:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\src\\data_cleaning.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[166]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#sys.path.append(os.path.abspath(\"D:\\Proyectos Python\\capresoca-data-automation\"))  # Ruta alternativa (comentada)\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Importar función y clase personalizada del proyecto\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cargar_maestros_ADRES  \u001b[38;5;66;03m# Función para cargar archivos maestros ADRES\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_cleaning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PoblacionCleaner     \u001b[38;5;66;03m# Clase para limpiar y normalizar población\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Instanciar el limpiador de población para su uso posterior\u001b[39;00m\n\u001b[32m     19\u001b[39m cleaner = PoblacionCleaner()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PoblacionCleaner' from 'src.data_cleaning' (c:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\src\\data_cleaning.py)"
     ]
    }
   ],
   "source": [
    "# Librerías estándar de Python\n",
    "import datetime  # Manejo de fechas y horas\n",
    "import pandas as pd  # Manipulación y análisis de datos en DataFrames\n",
    "import numpy as np  # Operaciones numéricas y manejo eficiente de arrays\n",
    "import sys  # Acceso a variables y funciones del sistema\n",
    "import re  # Expresiones regulares para procesamiento de texto\n",
    "import os  # Operaciones del sistema de archivos\n",
    "import src  # Permite importar módulos del proyecto (si existe un __init__.py)\n",
    "\n",
    "# Añadir la carpeta raíz del proyecto al sys.path para importar módulos personalizados\n",
    "sys.path.append(os.path.abspath(\"c:/Users/osmarrincon/Documents/capresoca-data-automation\"))\n",
    "#sys.path.append(os.path.abspath(\"D:\\Proyectos Python\\capresoca-data-automation\"))  # Ruta alternativa (comentada)\n",
    "\n",
    "# Importar función y clase personalizada del proyecto\n",
    "from src.file_loader import cargar_maestros_ADRES  # Función para cargar archivos maestros ADRES\n",
    "from src.data_cleaning import DataCleaner      # Clase para limpiar y normalizar población\n",
    "\n",
    "# Instanciar el limpiador de población para su uso posterior\n",
    "cleaner = PoblacionCleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703f3c6",
   "metadata": {},
   "source": [
    "# Rutas y varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34700c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Pila_Validada = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\07_Julio\\15\\Dataframe 15-07-2025.xlsx\"\n",
    "R_Maestro__EPSC25 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Contributivo\\Maestro\\2025-2\\EPSC25MC0014072025.TXT\"\n",
    "R_Maestro__EPS025 = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\2025-2\\EPS025MS0014072025.TXT\"\n",
    "\n",
    "S_Excel = r\"C:\\Users\\osmarrincon\\OneDrive - 891856000_CAPRESOCA E P S\\Escritorio\\Yesid Rincón Z\\Traslados\\Procesos BDUA\\2025\\07_Julio\\15\\Prueba.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b99dab",
   "metadata": {},
   "source": [
    "# Cargue Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151626aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y combinar los maestros\n",
    "maestro_ADRES = cargar_maestros_ADRES(R_Maestro__EPS025, R_Maestro__EPSC25)\n",
    "Pila_Validada = pd.read_excel(R_Pila_Validada, sheet_name=\"Sheet1\", header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eca49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicar la columna \"MARCASISBENIV+MARCASISBENIII_2\" y nombrarla \"MARCASISBENIV+MARCASISBENIII\"\n",
    "maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII_2\"] = \\\n",
    "    maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jerarquia = ['9', '17', '28', '2', '1']\n",
    "\n",
    "def limpiar_poblacion(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "\n",
    "    original = valor  # Para posible trazabilidad\n",
    "\n",
    "    # 1. Reemplazar si es exactamente SIV(Dnn)\n",
    "    if re.fullmatch(r\"SIV\\(D\\d{2}\\)\", valor.strip()):\n",
    "        return \"Sisben D\"\n",
    "\n",
    "    # 2. Eliminar sufijos -SIV(...) y -SIII(...)\n",
    "    valor = re.sub(r\"-SIV\\([^\\)]*\\)\", \"\", valor)\n",
    "    valor = re.sub(r\"-SIII\\([^\\)]*\\)\", \"\", valor)\n",
    "\n",
    "    # 3. Si es solo SIII(...), eliminar todo\n",
    "    if re.fullmatch(r\"SIII\\([^\\)]*\\)\", valor.strip()):\n",
    "        return \"\"\n",
    "\n",
    "    # 4. Eliminar prefijo \"LC(0)-\"\n",
    "    if valor.startswith(\"LC(0)-\"):\n",
    "        valor = valor.replace(\"LC(0)-\", \"\")\n",
    "\n",
    "    # 5. Si queda solo \"LC(0)\", eliminar\n",
    "    if valor.strip() == \"LC(0)\":\n",
    "        return \"\"\n",
    "\n",
    "    # 6. Normalizar LC(...) con jerarquía\n",
    "    def jerarquia_lc(match):\n",
    "        contenido = match.group(1)\n",
    "        valores = [v.strip() for v in contenido.split('|')]\n",
    "\n",
    "        if len(valores) == 1:\n",
    "            return f\"LC({valores[0]})\"\n",
    "\n",
    "        for prioridad in jerarquia:\n",
    "            if prioridad in valores:\n",
    "                return f\"LC({prioridad})\"\n",
    "\n",
    "        return \"\"  # Eliminar si no hay valor válido\n",
    "\n",
    "    valor = re.sub(r\"LC\\(([^\\)]*)\\)\", jerarquia_lc, valor)\n",
    "\n",
    "    return valor.strip()\n",
    "\n",
    "# Aplicar la función\n",
    "maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"] = maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"].apply(limpiar_poblacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2a8ac",
   "metadata": {},
   "source": [
    "# contrucción S1 movilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9aaf5",
   "metadata": {},
   "source": [
    "## Pila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas del nuevo DataFrame\n",
    "new_columns = [\n",
    "    \"ENT_ID\", \"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\", \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\", \"AFL_PRIMER_APELLIDO_2\", \"AFL_SEGUNDO_APELLIDO_2\", \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\", \"AFL_FECHA_NACIMIENTO_2\", \"TPS_GNR_ID_2\", \"DPR_ID\", \"MNC_ID\", \"ZNS_ID\",\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\", \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"TIPO_TRASLADO\", \"CND_AFL_SBS_METODOLOGIA\",\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\", \"CON_DISCAPACIDAD\", \"TPS_IDN_CF_ID\", \"HST_IDN_NUMERO_CF_IDENTIFICACION\",\n",
    "    \"TPS_PRN_ID\", \"TPS_AFL_ID\", \"TPS_ETN_ID\", \"NOM_RESGUARDO_INDIGENA\",\n",
    "    \"PAIS_NACIMIENTO\", \"LUGAR_NACIMIENTO\", \"NACIONALIDAD\", \"SEXO_IDENTIFICACION\", \"TIPO_DISCAPACIDAD\"\n",
    "]\n",
    "\n",
    "# Filtrar registros donde 'Movilidad' contiene 'S1'\n",
    "mask = Pila_Validada[\"Movilidad\"].str.contains(\"S1\", na=False)\n",
    "filtered = Pila_Validada.loc[mask]\n",
    "\n",
    "# Crear el nuevo DataFrame S1 con valores vacíos\n",
    "S1 = pd.DataFrame('', index=filtered.index, columns=new_columns)\n",
    "\n",
    "# Asignar los valores requeridos\n",
    "S1.loc[:, \"TPS_IDN_ID\"] = filtered[\"Tipo Documento Cotizante\"]\n",
    "S1.loc[:, \"HST_IDN_NUMERO_IDENTIFICACION\"] = filtered[\"N° Identificación Cotizante\"]\n",
    "S1.loc[:, \"FECHA_AFILIACION_MOVILIDAD\"] = filtered[\"Fecha envio\"]\n",
    "S1.loc[:, \"CND_AFL_SBS_SUBGRUPO_SIV\"] = filtered[\"Población_2\"]\n",
    "\n",
    "# Mostrar cantidad de registros, columnas y porcentaje de vacíos por columna\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "print(\"Porcentaje de vacíos por columna:\")\n",
    "print(S1.isin(['', None, np.nan]).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1[\"Where\"] = \"Pila\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78f9eb",
   "metadata": {},
   "source": [
    "## Beneficairios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fe5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: mostrar nombres de columnas antes de cualquier operación\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns antes de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns antes de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Limpiar espacios invisibles en los nombres de columna\n",
    "# ----------------------------------------------------\n",
    "S1.columns            = S1.columns.str.strip()\n",
    "maestro_ADRES.columns = maestro_ADRES.columns.str.strip()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Debug: volver a mostrar nombres de columnas después de strip()\n",
    "# ----------------------------------------------------\n",
    "print(\">> S1 columns después de strip():\")\n",
    "print([repr(c) for c in S1.columns.tolist()])\n",
    "print(\">> maestro_ADRES columns después de strip():\")\n",
    "print([repr(c) for c in maestro_ADRES.columns.tolist()])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 0) Cantidad de registros y columnas en S1\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros: {S1.shape[0]}, Columnas: {S1.shape[1]}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Preparar sólo las claves de S1 (cabezas de familia),\n",
    "#    incluyendo FECHA_AFILIACION_MOVILIDAD para luego propagarla\n",
    "# ----------------------------------------------------\n",
    "s1_keys = (\n",
    "    S1[[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\", \"FECHA_AFILIACION_MOVILIDAD\"]]\n",
    "    .drop_duplicates(subset=[\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Emparejar S1 (cabezas) con maestro_ADRES (beneficiarios)\n",
    "#    - Usamos sufijos para distinguir columnas duplicadas\n",
    "# ----------------------------------------------------\n",
    "beneficiarios = (\n",
    "    maestro_ADRES\n",
    "    .merge(\n",
    "        s1_keys,\n",
    "        left_on=[\"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "        right_on=[\"TPS_IDN_ID\",    \"HST_IDN_NUMERO_IDENTIFICACION\"],\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_master\", \"_s1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Debug: ver columnas que resultaron de este merge\n",
    "print(\">> beneficiarios columns:\")\n",
    "print(beneficiarios.columns.tolist())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Construir los nuevos registros con las columnas solicitadas,\n",
    "#    propagando FECHA_AFILIACION_MOVILIDAD desde la familia cabeza\n",
    "# ----------------------------------------------------\n",
    "nuevos = pd.DataFrame({\n",
    "    # ID del beneficiario (de maestro_ADRES con sufijo _master)\n",
    "    \"TPS_IDN_ID\":                        beneficiarios[\"TPS_IDN_ID_master\"],\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\":     beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_master\"],\n",
    "    # Subgrupo según MARCASISBENIV+MARCASISBENIII\n",
    "    \"CND_AFL_SBS_SUBGRUPO_SIV\":          beneficiarios[\"MARCASISBENIV+MARCASISBENIII\"],\n",
    "    # IDs de la cabeza de familia\n",
    "    \"TPS_IDN_CF_ID\":                     beneficiarios[\"TPS_IDN_ID_CF\"],\n",
    "    \"HST_IDN_NUMERO_CF_IDENTIFICACION\":  beneficiarios[\"HST_IDN_NUMERO_IDENTIFICACION_CF\"],\n",
    "    # TPS_PRN_ID del beneficiario\n",
    "    \"TPS_PRN_ID\":                        beneficiarios[\"TPS_PRN_ID\"],\n",
    "    # Marcamos estos nuevos registros con la etiqueta fija\n",
    "    \"Where\":                             \"Beneficiarios Pila\",\n",
    "    # Propagamos la misma fecha de movilidad que tenía la cabeza de familia\n",
    "    \"FECHA_AFILIACION_MOVILIDAD\":        beneficiarios[\"FECHA_AFILIACION_MOVILIDAD\"],\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) Añadir los nuevos registros al final de S1\n",
    "# ----------------------------------------------------\n",
    "S1 = pd.concat([S1, nuevos], ignore_index=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) Mostrar resumen final\n",
    "# ----------------------------------------------------\n",
    "print(f\"Registros después del concat: {S1.shape[0]}\")\n",
    "print(f\"Columnas después del concat:  {S1.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c3e90",
   "metadata": {},
   "source": [
    "### Fechas de Beneficairios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfe2fc",
   "metadata": {},
   "source": [
    "## Sisben Maestro ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0de7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar el mapeo de los valores de maestro_ADRES[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "# a S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] solo donde S1[\"Where\"] == \"Beneficiarios Pila\"\n",
    "\n",
    "# Crear un diccionario para búsqueda rápida por ID\n",
    "maestro_map = maestro_ADRES.set_index([\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"])[\"MARCASISBENIV+MARCASISBENIII\"]\n",
    "\n",
    "# Actualizar S1 usando .apply para mantener el nombre de las columnas\n",
    "def actualizar_subgrupo(row):\n",
    "    if row.get(\"Where\") == \"Beneficiarios Pila\":\n",
    "        key = (row[\"TPS_IDN_ID\"], row[\"HST_IDN_NUMERO_IDENTIFICACION\"])\n",
    "        valor = maestro_map.get(key, row[\"CND_AFL_SBS_SUBGRUPO_SIV\"])\n",
    "        return valor\n",
    "    return row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "\n",
    "S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = S1.apply(actualizar_subgrupo, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f43ed",
   "metadata": {},
   "source": [
    "## Datos del Maestro al S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Defino cómo se llaman en maestro → cómo quiero que queden en S1\n",
    "column_mapping = {\n",
    "    \"AFL_PAIS_NACIMIENTO\":     \"PAIS_NACIMIENTO\",\n",
    "    \"AFL_MUNICIPIO_NACIMIENTO\":\"LUGAR_NACIMIENTO\",\n",
    "    \"AFL_NACIONALIDAD\":        \"NACIONALIDAD\",\n",
    "    \"AFL_SEXO_IDENTIFICACION\": \"SEXO_IDENTIFICACION\",\n",
    "    \"AFL_DISCAPACIDAD\":        \"TIPO_DISCAPACIDAD\",\n",
    "    \"DPR_ID\":                  \"DPR_ID\",\n",
    "    \"MNC_ID\":                  \"MNC_ID\",\n",
    "    \"ZNS_ID\":                  \"ZNS_ID\",\n",
    "    \"TPS_AFL_ID\":              \"TPS_AFL_ID\",\n",
    "    # (y las que ya tenías antes…)\n",
    "    \"AFL_PRIMER_APELLIDO\":    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\":   \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\":      \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\":     \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\":   \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\":             \"TPS_GNR_ID\",\n",
    "}\n",
    "\n",
    "join_keys = [\"TPS_IDN_ID\", \"HST_IDN_NUMERO_IDENTIFICACION\"]\n",
    "\n",
    "# 2) Preparo el maestro: me quedo solo con las columnas que necesito y\n",
    "#    las renombro para que coincidan con S1:\n",
    "maestro_subset = (\n",
    "    maestro_ADRES[list(column_mapping.keys()) + join_keys]\n",
    "    .rename(columns=column_mapping)\n",
    ")\n",
    "\n",
    "# 3) Hago el merge contra S1; pandas añadirá un sufijo solo a las columnas\n",
    "#    del maestro (porque ya existen en S1):\n",
    "S1_merged = S1.merge(\n",
    "    maestro_subset,\n",
    "    on=join_keys,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_maestro\")\n",
    ")\n",
    "\n",
    "# 4) Ahora, para cada columna de destino 'col', relleno S1[col] solo donde\n",
    "#    esté vacío usando la versión '_maestro', y luego elimino esa columna:\n",
    "for col in column_mapping.values():\n",
    "    maestro_col = f\"{col}_maestro\"\n",
    "    if maestro_col in S1_merged:\n",
    "        S1_merged[col] = S1_merged[col].replace(\"\", pd.NA) \\\n",
    "                                       .fillna(S1_merged[maestro_col]) \\\n",
    "                                       .fillna(\"\")  # opcional volver a cadena vacía\n",
    "        S1_merged.drop(columns=maestro_col, inplace=True)\n",
    "\n",
    "# 5) S1_merged es tu S1 final, con el mismo esquema de columnas y\n",
    "#    solo rellenando lo que estaba vacío:\n",
    "S1 = S1_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a64468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar los valores de las columnas base a las columnas \"_2\" en S1\n",
    "cols_base = [\n",
    "    \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\",\n",
    "    \"AFL_PRIMER_APELLIDO\",\n",
    "    \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\",\n",
    "    \"AFL_SEGUNDO_NOMBRE\",\n",
    "    \"AFL_FECHA_NACIMIENTO\",\n",
    "    \"TPS_GNR_ID\"\n",
    "]\n",
    "cols_2 = [\n",
    "    \"TPS_IDN_ID_2\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION_2\",\n",
    "    \"AFL_PRIMER_APELLIDO_2\",\n",
    "    \"AFL_SEGUNDO_APELLIDO_2\",\n",
    "    \"AFL_PRIMER_NOMBRE_2\",\n",
    "    \"AFL_SEGUNDO_NOMBRE_2\",\n",
    "    \"AFL_FECHA_NACIMIENTO_2\",\n",
    "    \"TPS_GNR_ID_2\"\n",
    "]\n",
    "\n",
    "for base, col2 in zip(cols_base, cols_2):\n",
    "    S1[col2] = S1[base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16936195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_nvl_ssb(subgrupo):\n",
    "    if pd.isna(subgrupo):\n",
    "        return \"\"\n",
    "    lc_match = re.match(r\"LC\\((\\d+)\\)\", subgrupo)\n",
    "    if lc_match:\n",
    "        return lc_match.group(1)\n",
    "    siv_match = re.match(r\"SIV\\([A-Z0-9]+\\)\", subgrupo)\n",
    "    if siv_match:\n",
    "        return \"5\"\n",
    "    return \"\"\n",
    "\n",
    "S1[\"TPS_GRP_PBL_ID\"] = S1[\"CND_AFL_SBS_SUBGRUPO_SIV\"].apply(extract_nvl_ssb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignación condicional según las reglas descritas\n",
    "def assign_values(row):\n",
    "    grp_pbl_id = row[\"TPS_GRP_PBL_ID\"]\n",
    "    subgrupo = row[\"CND_AFL_SBS_SUBGRUPO_SIV\"]\n",
    "    if grp_pbl_id != \"5\":\n",
    "        row[\"TPS_NVL_SSB_ID\"] = \"N\"\n",
    "        row[\"CND_AFL_SBS_METODOLOGIA\"] = \"3\"\n",
    "        row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    else:\n",
    "        # Extraer la letra y número de SIV(xxx)\n",
    "        match = re.match(r\"SIV\\(([A-Z])(\\d{2})\\)\", subgrupo)\n",
    "        if match:\n",
    "            letra = match.group(1)\n",
    "            numero = match.group(2)\n",
    "            if letra in [\"A\", \"B\"]:\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"1\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            elif letra == \"C\":\n",
    "                row[\"TPS_NVL_SSB_ID\"] = \"2\"\n",
    "                row[\"CND_AFL_SBS_METODOLOGIA\"] = \"2\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = f\"{letra}{numero}\"\n",
    "        else:\n",
    "            # Si no coincide con el patrón, dejar vacío\n",
    "            row[\"TPS_NVL_SSB_ID\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_METODOLOGIA\"] = \"\"\n",
    "            row[\"CND_AFL_SBS_SUBGRUPO_SIV\"] = \"\"\n",
    "    return row\n",
    "\n",
    "S1 = S1.apply(assign_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4c7eb",
   "metadata": {},
   "source": [
    "# Guardamos Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a45d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.ExcelWriter(S_Excel, engine=\"openpyxl\") as writer:\n",
    "    S1.to_excel(writer, sheet_name=\"S1\", index=False)\n",
    "    maestro_ADRES.to_excel(writer, sheet_name=\"maestro_ADRES\", index=False)\n",
    "\n",
    "print(\"Archivo Excel guardado en:\", S_Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "maestro_ADRES.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
