{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√≥dulo para manipulaci√≥n y an√°lisis de datos con DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# M√≥dulo para trabajar con archivos Excel (lectura y escritura)\n",
    "import openpyxl\n",
    "\n",
    "# M√≥dulo para manipular fechas y horas\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# M√≥dulo para operaciones relacionadas con el tiempo (pausas, mediciones)\n",
    "import time\n",
    "\n",
    "# M√≥dulo para trabajar con rutas del sistema operativo\n",
    "import os\n",
    "\n",
    "# M√≥dulo para expresiones regulares (b√∫squeda y manipulaci√≥n de patrones de texto)\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Red_SIE = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Reporte SIE\\Reporte_Red Asignada General_2026_02_18.csv\"\n",
    "\n",
    "R_portabilidad_NAc_EPSC25 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Portabilidad\\PORTABILIDAD_NACIONAL_REGIMEN_CONTRIBUTIVO_ENERO26.xlsx\"\n",
    "H_portabilidad_NAc_EPSC25 = \"PORT NACIONAL CONTRB\"\n",
    "R_portabilidad_NAc_EPS025 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Portabilidad\\PORTABILIDAD_NACIONAL_REGIMEN_SUBSIDIADO_ENERO26.xlsx\"\n",
    "H_portabilidad_NAc_EPS025 = \"PORT NACIONAL SUB\"\n",
    "\n",
    "R_portabilidad_Reg_EPSC25 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Portabilidad\\PORTABILIDAD_REGIONAL_REGIMEN_CONTRIBUTIVO_ENERO26.xlsx\"\n",
    "H_portabilidad_Reg_EPSC25 = \"PORT REGIONAL CONTRB\"\n",
    "R_portabilidad_Reg_EPS025 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Portabilidad\\PORTABILIDAD_REGIONAL_REGIMEN_SUBSIDIADO_ENERO26.xlsx\"\n",
    "H_portabilidad_Reg_EPS025 = \"PORT REGIONAL SUB\"\n",
    "\n",
    "R_lma = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\LMA-Compensados\\LMA ENERO 2026.xlsx\"\n",
    "H_lma = \"BD\"\n",
    "\n",
    "R_compensados = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\LMA-Compensados\\COMPENSADOS ENERO.xlsx\"\n",
    "H_compensados = \"COMPENSADOS\"\n",
    "\n",
    "R_Consolidad_EPS025 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Maestros\\EPS025MS0016022026.TXT\"\n",
    "R_Consolidad_EPSC25 = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Maestros\\EPSC25MC0016022026.TXT\"\n",
    "\n",
    "R_Consolidado_SIE = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Maestros\\Reporte_Validaci√≥n Archivos Maestro_2026_02_18.csv\"\n",
    "\n",
    "R_Municipios_SIE = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Nomalizaci√≥n SIE\\Reporte_MUNICIPIOS_2025_05_14.csv\"\n",
    "R_red_Contratada = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\\Red Asignada\\Tabla de contratos\\cto.txt\"\n",
    "\n",
    "R_Salida = r\"C:\\Users\\osmarrincon\\Documents\\capresoca-data-automation\\data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Cargue datafrmes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## df Red de servicios SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con las especificaciones indicadas\n",
    "df_red_sie = pd.read_csv(R_Red_SIE, sep=';', encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## df PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo Excel de portabilidad nacional\n",
    "df_pt_nac_EPSC25 = pd.read_excel(R_portabilidad_NAc_EPSC25, sheet_name=H_portabilidad_NAc_EPSC25)\n",
    "df_pt_nac_EPS025 = pd.read_excel(R_portabilidad_NAc_EPS025, sheet_name=H_portabilidad_NAc_EPS025)\n",
    "\n",
    "# UNIFICAR DATAFRAMES DE PORTABILIDAD\n",
    "df_pt_nac = pd.concat([df_pt_nac_EPSC25, df_pt_nac_EPS025], ignore_index=True)\n",
    "\n",
    "# MANTENER SOLO LAS 2 COLUMNAS REQUERIDAS\n",
    "df_pt_nac = df_pt_nac[['tipo_identificacion', 'numero_identificacion']]\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ELIMINAR DATAFRAMES ORIGINALES PARA LIBERAR MEMORIA\n",
    "del df_pt_nac_EPSC25, df_pt_nac_EPS025\n",
    "\n",
    "print(f\"‚úì DataFrame unificado creado: df_pt_nac\")\n",
    "print(f\"‚úì Registros totales: {len(df_pt_nac)}\")\n",
    "print(f\"‚úì Columnas: {df_pt_nac.columns.tolist()}\")\n",
    "\n",
    "# Contar registros antes de eliminar duplicados\n",
    "registros_antes = len(df_pt_nac)\n",
    "\n",
    "# Verificar duplicados usando ambas columnas como clave\n",
    "duplicados_pt = df_pt_nac.duplicated(subset=['tipo_identificacion', 'numero_identificacion'])\n",
    "\n",
    "print(f\"Duplicados encontrados: {duplicados_pt.sum()}\")\n",
    "\n",
    "# Mostrar registros duplicados (si existen)\n",
    "print(\"\\nRegistros duplicados (muestra):\")\n",
    "print(df_pt_nac[duplicados_pt].head())\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ELIMINAR DUPLICADOS: Mantener solo el primer registro de cada combinaci√≥n\n",
    "df_pt_nac = df_pt_nac.drop_duplicates(subset=['tipo_identificacion', 'numero_identificacion'], keep='first')\n",
    "\n",
    "# Contar registros despu√©s de eliminar duplicados\n",
    "registros_despues = len(df_pt_nac)\n",
    "registros_eliminados = registros_antes - registros_despues\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE ELIMINACI√ìN DE DUPLICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros ANTES: {registros_antes}\")\n",
    "print(f\"‚úì Registros DESPU√âS: {registros_despues}\")\n",
    "print(f\"‚úì Registros ELIMINADOS: {registros_eliminados}\")\n",
    "print(f\"‚úì Porcentaje eliminado: {(registros_eliminados/registros_antes)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## df PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo Excel de portabilidad nacional contributivo\n",
    "df_pt_reg_EPSC25 = pd.read_excel(R_portabilidad_Reg_EPSC25, sheet_name=H_portabilidad_Reg_EPSC25)\n",
    "df_pt_reg_EPS025 = pd.read_excel(R_portabilidad_Reg_EPS025, sheet_name=H_portabilidad_Reg_EPS025)\n",
    "\n",
    "# UNIFICAR DATAFRAMES DE PORTABILIDAD\n",
    "df_pt_reg = pd.concat([df_pt_reg_EPSC25, df_pt_reg_EPS025], ignore_index=True)\n",
    "\n",
    "print(f\"Columnas PR: {df_pt_reg_EPSC25.columns}\")\n",
    "\n",
    "# MANTENER SOLO LAS 2 COLUMNAS REQUERIDAS\n",
    "df_pt_reg = df_pt_reg[['tipo_identificacion', 'numero_identificacion', 'municipio_receptor']]\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ELIMINAR DATAFRAMES ORIGINALES PARA LIBERAR MEMORIA\n",
    "del df_pt_reg_EPSC25, df_pt_reg_EPS025\n",
    "\n",
    "print(f\"‚úì DataFrame unificado creado: df_pt_reg\")\n",
    "print(f\"‚úì Registros totales: {len(df_pt_reg)}\")\n",
    "print(f\"‚úì Columnas: {df_pt_reg.columns.tolist()}\")\n",
    "\n",
    "# Contar registros antes de eliminar duplicados\n",
    "registros_antes = len(df_pt_reg)\n",
    "\n",
    "# Verificar duplicados usando ambas columnas como clave\n",
    "duplicados_pt = df_pt_reg.duplicated(subset=['tipo_identificacion', 'numero_identificacion'])\n",
    "\n",
    "print(f\"Duplicados encontrados: {duplicados_pt.sum()}\")\n",
    "\n",
    "# Mostrar registros duplicados (si existen)\n",
    "print(\"\\nRegistros duplicados (muestra):\")\n",
    "print(df_pt_reg[duplicados_pt].head())\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ELIMINAR DUPLICADOS: Mantener solo el primer registro de cada combinaci√≥n\n",
    "df_pt_reg = df_pt_reg.drop_duplicates(subset=['tipo_identificacion', 'numero_identificacion', 'municipio_receptor'], keep='first')\n",
    "\n",
    "# Contar registros despu√©s de eliminar duplicados\n",
    "registros_despues = len(df_pt_reg)\n",
    "registros_eliminados = registros_antes - registros_despues\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE ELIMINACI√ìN DE DUPLICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros ANTES: {registros_antes}\")\n",
    "print(f\"‚úì Registros DESPU√âS: {registros_despues}\")\n",
    "print(f\"‚úì Registros ELIMINADOS: {registros_eliminados}\")\n",
    "print(f\"‚úì Porcentaje eliminado: {(registros_eliminados/registros_antes)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## df LMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì• CARGA Y LIMPIEZA DE LMA (OPTIMIZADO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# PASO 1: CARGAR SOLO LAS COLUMNAS NECESARIAS (OPTIMIZACI√ìN DE MEMORIA)\n",
    "df_lma = pd.read_excel(\n",
    "    R_lma, \n",
    "    sheet_name=H_lma,\n",
    "    usecols=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']  # ‚Üê Cargar solo lo necesario\n",
    ")\n",
    "\n",
    "print(f\"‚úì Columnas de df_lma: {df_lma.columns.tolist()}\")\n",
    "print(f\"‚úì Registros totales: {len(df_lma):,}\")\n",
    "\n",
    "# PASO 2: DETECTAR Y ELIMINAR DUPLICADOS EN UNA SOLA PASADA\n",
    "registros_antes = len(df_lma)\n",
    "\n",
    "# Crear m√°scara de duplicados (incluyendo el primero)\n",
    "duplicados_todas = df_lma.duplicated(\n",
    "    subset=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], \n",
    "    keep=False\n",
    ")\n",
    "\n",
    "# PASO 3: AN√ÅLISIS DE DUPLICADOS (solo si existen)\n",
    "if duplicados_todas.any():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã REGISTROS DUPLICADOS (muestra)\")\n",
    "    print(\"=\"*80)\n",
    "    # Mostrar solo duplicados (los que se van a eliminar)\n",
    "    duplicados_a_eliminar = df_lma.duplicated(\n",
    "        subset=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], \n",
    "        keep='first'  # ‚Üê Solo los que se van a eliminar\n",
    "    )\n",
    "    print(df_lma[duplicados_a_eliminar].head(10))\n",
    "    \n",
    "    # AN√ÅLISIS AGRUPADO (vectorizado, eficiente)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä AN√ÅLISIS DE DUPLICADOS POR CATEGOR√çA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    duplicados_agrupados = (\n",
    "        df_lma[duplicados_todas]\n",
    "        .groupby(['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'])\n",
    "        .size()\n",
    "        .reset_index(name='cantidad_duplicados')\n",
    "        .sort_values('cantidad_duplicados', ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de registros √∫nicos duplicados: {len(duplicados_agrupados):,}\")\n",
    "    print(\"\\nDetalle de duplicados:\")\n",
    "    print(duplicados_agrupados.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì No se encontraron duplicados\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# PASO 4: ELIMINAR DUPLICADOS (una sola pasada)\n",
    "df_lma = df_lma.drop_duplicates(\n",
    "    subset=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION'], \n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "registros_despues = len(df_lma)\n",
    "registros_eliminados = registros_antes - registros_despues\n",
    "porcentaje_eliminado = (registros_eliminados / registros_antes * 100) if registros_antes > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE ELIMINACI√ìN DE DUPLICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros ANTES: {registros_antes:,}\")\n",
    "print(f\"‚úì Registros DESPU√âS: {registros_despues:,}\")\n",
    "print(f\"‚úì Registros ELIMINADOS: {registros_eliminados:,}\")\n",
    "print(f\"‚úì Porcentaje eliminado: {porcentaje_eliminado:.2f}%\")\n",
    "\n",
    "# VALIDACI√ìN DE INTEGRIDAD\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Duplicados residuales: {df_lma.duplicated(subset=['TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION']).sum():,}\")\n",
    "print(f\"‚úì Valores nulos TPS_IDN_ID: {df_lma['TPS_IDN_ID'].isna().sum():,}\")\n",
    "print(f\"‚úì Valores nulos HST_IDN_NUMERO_IDENTIFICACION: {df_lma['HST_IDN_NUMERO_IDENTIFICACION'].isna().sum():,}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚è±Ô∏è TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## df Compensados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo Excel de compensados\n",
    "df_compensados = pd.read_excel(R_compensados, sheet_name=H_compensados)\n",
    "print(f\"Columnas: {df_compensados.columns}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mantener solo las 2 columnas requeridas\n",
    "df_compensados = df_compensados[['Tp_Dc', 'Num_coti']]\n",
    "\n",
    "print(f\"‚úì Columnas de df_compensados: {df_compensados.columns.tolist()}\")\n",
    "print(f\"‚úì Registros totales: {len(df_compensados)}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar registros antes de eliminar duplicados\n",
    "registros_antes = len(df_compensados)\n",
    "\n",
    "# Verificar duplicados usando ambas columnas como clave\n",
    "duplicados_compensados = df_compensados.duplicated(subset=['Tp_Dc', 'Num_coti'])\n",
    "\n",
    "print(f\"Duplicados encontrados: {duplicados_compensados.sum()}\")\n",
    "\n",
    "# Mostrar registros duplicados (si existen)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã REGISTROS DUPLICADOS (muestra)\")\n",
    "print(\"=\"*80)\n",
    "print(df_compensados[duplicados_compensados].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AN√ÅLISIS DE DUPLICADOS POR CATEGOR√çA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encontrar todos los registros duplicados (incluyendo el primero)\n",
    "duplicados_todas_ocurrencias = df_compensados.duplicated(\n",
    "    subset=['Tp_Dc', 'Num_coti'], \n",
    "    keep=False\n",
    ")\n",
    "\n",
    "# Agrupar por la combinaci√≥n de columnas y contar ocurrencias\n",
    "duplicados_agrupados = df_compensados[duplicados_todas_ocurrencias].groupby(\n",
    "    ['Tp_Dc', 'Num_coti']\n",
    ").size().reset_index(name='cantidad_duplicados')\n",
    "\n",
    "# Ordenar por cantidad de duplicados (mayor a menor)\n",
    "duplicados_agrupados = duplicados_agrupados.sort_values('cantidad_duplicados', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal de registros √∫nicos duplicados: {len(duplicados_agrupados)}\")\n",
    "print(\"\\nDetalle de duplicados:\")\n",
    "print(duplicados_agrupados.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ELIMINAR DUPLICADOS: Mantener solo el primer registro de cada combinaci√≥n\n",
    "df_compensados = df_compensados.drop_duplicates(\n",
    "    subset=['Tp_Dc', 'Num_coti'], \n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "# Contar registros despu√©s de eliminar duplicados\n",
    "registros_despues = len(df_compensados)\n",
    "registros_eliminados = registros_antes - registros_despues\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE ELIMINACI√ìN DE DUPLICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros ANTES: {registros_antes:,}\")\n",
    "print(f\"‚úì Registros DESPU√âS: {registros_despues:,}\")\n",
    "print(f\"‚úì Registros ELIMINADOS: {registros_eliminados:,}\")\n",
    "print(f\"‚úì Porcentaje eliminado: {(registros_eliminados/registros_antes)*100:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## df Consolidados ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos TXT de consolidados ADRES\n",
    "df_consolidad_EPSC25 = pd.read_csv(\n",
    "    R_Consolidad_EPSC25, \n",
    "    sep=',', \n",
    "    encoding='latin-1', \n",
    "    dtype=str,\n",
    "    header=None,\n",
    "    names=[\n",
    "        'AFL_ID', 'ENT_ID', 'TPS_IDN_ID_CF', 'HST_IDN_NUMERO_IDENTIFICACION_CF',\n",
    "        'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
    "        'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
    "        'AFL_FECHA_NACIMIENTO', 'TPS_GNR_ID', 'AFL_PAIS_NACIMIENTO',\n",
    "        'AFL_MUNICIPIO_NACIMIENTO', 'AFL_NACIONALIDAD', 'AFL_SEXO_IDENTIFICACION',\n",
    "        'AFL_DISCAPACIDAD', 'TPS_AFL_ID', 'TPS_PRN_ID', 'TPS_GRP_PBL_ID',\n",
    "        'TPS_NVL_SSB_ID', 'NUMEROFICHASISBEN', 'TPS_CND_BNF_ID', 'DPR_ID',\n",
    "        'MNC_ID', 'ZNS_ID', 'AFL_FECHA_AFILIACION_SGSSS', 'AFC_FECHA_INICIO',\n",
    "        'NUMERO_CONTRATO', 'FECHA_INICIO_CONTRATO', 'CNT_AFL_TPS_GRP_PBL_ID',\n",
    "        'CNT_AFL_TPS_PRT_ETN_ID', 'TPS_MDL_SBS_ID', 'TPS_EST_AFL_ID',\n",
    "        'CND_AFL_FECHA_INICIO', 'CND_AFL_FECHA_INICIO_2', 'GRP_FML_COTIZANTE_ID',\n",
    "        'PORTABILIDAD', 'COD_IPS_P', 'MTDLG_G_P', 'SUB_SISBEN_IV',\n",
    "        'MARCASISBENIV_MARCASISBENIII', 'CRUCE_BDEX_RNEC'\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_consolidad_EPS025 = pd.read_csv(\n",
    "    R_Consolidad_EPS025,\n",
    "    sep=',',\n",
    "    encoding='latin-1',\n",
    "    dtype=str,\n",
    "    header=None,\n",
    "    names=[\n",
    "        'AFL_ID', 'ENT_ID', 'TPS_IDN_ID_CF', 'HST_IDN_NUMERO_IDENTIFICACION_CF',\n",
    "        'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'AFL_PRIMER_APELLIDO',\n",
    "        'AFL_SEGUNDO_APELLIDO', 'AFL_PRIMER_NOMBRE', 'AFL_SEGUNDO_NOMBRE',\n",
    "        'AFL_FECHA_NACIMIENTO', 'TPS_GNR_ID', 'AFL_PAIS_NACIMIENTO',\n",
    "        'AFL_MUNICIPIO_NACIMIENTO', 'AFL_NACIONALIDAD', 'AFL_SEXO_IDENTIFICACION',\n",
    "        'AFL_DISCAPACIDAD', 'TPS_AFL_ID', 'TPS_PRN_ID', 'TPS_GRP_PBL_ID',\n",
    "        'TPS_NVL_SSB_ID', 'NUMEROFICHASISBEN', 'TPS_CND_BNF_ID', 'DPR_ID',\n",
    "        'MNC_ID', 'ZNS_ID', 'AFL_FECHA_AFILIACION_SGSSS', 'AFC_FECHA_INICIO',\n",
    "        'NUMERO_CONTRATO', 'FECHA_INICIO_CONTRATO', 'CNT_AFL_TPS_GRP_PBL_ID',\n",
    "        'CNT_AFL_TPS_PRT_ETN_ID', 'TPS_MDL_SBS_ID', 'TPS_EST_AFL_ID',\n",
    "        'CND_AFL_FECHA_INICIO', 'CND_AFL_FECHA_INICIO_2', 'GRP_FML_COTIZANTE_ID',\n",
    "        'PORTABILIDAD', 'COD_IPS_P', 'MTDLG_G_P', 'SUB_SISBEN_IV',\n",
    "        'MARCASISBENIV_MARCASISBENIII', 'CRUCE_BDEX_RNEC'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# UNIFICAR AMBOS DATAFRAMES\n",
    "df_consolidad_adres = pd.concat([df_consolidad_EPSC25, df_consolidad_EPS025], ignore_index=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä CARGA DE CONSOLIDADOS ADRES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros EPSC25 (Contributivo): {len(df_consolidad_EPSC25):,}\")\n",
    "print(f\"‚úì Registros EPS025 (Subsidiado): {len(df_consolidad_EPS025):,}\")\n",
    "print(f\"‚úì Total registros unificados: {len(df_consolidad_adres):,}\")\n",
    "print(f\"‚úì Columnas: {len(df_consolidad_adres.columns)}\")\n",
    "print(f\"\\n‚úì Primeras columnas: {df_consolidad_adres.columns[:10].tolist()}\")\n",
    "\n",
    "# ELIMINAR DATAFRAMES ORIGINALES PARA LIBERAR MEMORIA\n",
    "del df_consolidad_EPSC25, df_consolidad_EPS025\n",
    "\n",
    "print(\"\\n‚úì DataFrames originales eliminados\")\n",
    "print(\"‚úì DataFrame consolidado creado: df_consolidad_adres\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# MOSTRAR MUESTRA DE DATOS\n",
    "print(f\"\\nüìã MUESTRA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_consolidad_adres.head())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Informaci√≥n del DataFrame:\")\n",
    "df_consolidad_adres.info()\n",
    "\n",
    "# üßπ Mantener solo las columnas requeridas en df_consolidad_adres\n",
    "df_consolidad_adres = df_consolidad_adres[['AFL_ID', 'ENT_ID', 'TPS_IDN_ID', 'HST_IDN_NUMERO_IDENTIFICACION', 'DPR_ID', 'MNC_ID', 'TPS_EST_AFL_ID']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä REDUCCI√ìN DE COLUMNAS EN df_consolidad_adres\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Columnas seleccionadas: {df_consolidad_adres.columns.tolist()}\")\n",
    "print(f\"‚úì Total de registros: {len(df_consolidad_adres):,}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_consolidad_adres.columns)}\")\n",
    "\n",
    "print(\"\\nüìã MUESTRA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_consolidad_adres.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ÑπÔ∏è INFORMACI√ìN DEL DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "df_consolidad_adres.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## df_consolidado_sie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV del Consolidado SIE\n",
    "df_consolidado_sie = pd.read_csv(\n",
    "    R_Consolidado_SIE, \n",
    "    sep=';', \n",
    "    encoding='latin-1', \n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CARGA DE CONSOLIDADO SIE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros: {len(df_consolidado_sie):,}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_consolidado_sie.columns)}\")\n",
    "print(f\"\\n‚úì Columnas del archivo:\")\n",
    "print(df_consolidado_sie.columns.tolist())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ÑπÔ∏è INFORMACI√ìN DEL DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "df_consolidado_sie.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã PROCESAMIENTO DE CONSOLIDADO SIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: SELECCIONAR COLUMNAS NECESARIAS\n",
    "# ============================================================================\n",
    "columnas_seleccionar = [\n",
    "    'tipo_documento',\n",
    "    'numero_identificacion',\n",
    "    'municipio',\n",
    "    'estado',\n",
    "    'regimen',\n",
    "    'estado_traslado',\n",
    "    'resguardo_indigena'\n",
    "]\n",
    "\n",
    "# Validar que todas las columnas existan\n",
    "columnas_faltantes = [col for col in columnas_seleccionar if col not in df_consolidado_sie.columns]\n",
    "\n",
    "if columnas_faltantes:\n",
    "    print(f\"‚ö†Ô∏è  ADVERTENCIA: Columnas no encontradas: {columnas_faltantes}\")\n",
    "    print(f\"‚úì Columnas disponibles: {df_consolidado_sie.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"‚úì Todas las columnas est√°n presentes\")\n",
    "\n",
    "# Seleccionar solo las columnas necesarias\n",
    "df_consolidado_sie = df_consolidado_sie[columnas_seleccionar]\n",
    "\n",
    "print(f\"\\n‚úì Columnas ANTES de renombramiento: {df_consolidado_sie.columns.tolist()}\")\n",
    "print(f\"‚úì Total de registros: {len(df_consolidado_sie):,}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_consolidado_sie.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: RENOMBRAR COLUMNAS (MAPPING EXPL√çCITO)\n",
    "# ============================================================================\n",
    "diccionario_rename = {\n",
    "    'municipio': 'municipio_SIE',\n",
    "    'estado': 'estado_SIE',\n",
    "    'regimen': 'regimen_SIE',\n",
    "    'resguardo_indigena': 'PE'\n",
    "}\n",
    "\n",
    "df_consolidado_sie = df_consolidado_sie.rename(columns=diccionario_rename)\n",
    "\n",
    "print(f\"\\n‚úì Columnas DESPU√âS de renombramiento: {df_consolidado_sie.columns.tolist()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: VALIDACI√ìN DE INTEGRIDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validar que no hay ninguna columna vieja\n",
    "columnas_esperadas = ['tipo_documento', 'numero_identificacion', 'municipio_SIE', \n",
    "                      'estado_SIE', 'regimen_SIE', 'estado_traslado', 'PE']\n",
    "columnas_actuales = df_consolidado_sie.columns.tolist()\n",
    "\n",
    "print(f\"\\n‚úì Columnas esperadas: {columnas_esperadas}\")\n",
    "print(f\"‚úì Columnas actuales: {columnas_actuales}\")\n",
    "print(f\"‚úì ¬øCoinciden?: {columnas_esperadas == columnas_actuales}\")\n",
    "\n",
    "# Validar nulidad en columnas clave\n",
    "print(f\"\\n‚úì AN√ÅLISIS DE NULIDAD:\")\n",
    "print(\"-\" * 80)\n",
    "for col in df_consolidado_sie.columns:\n",
    "    nulos = df_consolidado_sie[col].isna().sum()\n",
    "    porcentaje = (nulos / len(df_consolidado_sie)) * 100\n",
    "    print(f\"  {col:30s}: {nulos:10,d} nulos ({porcentaje:5.1f}%)\")\n",
    "\n",
    "# Validar duplicados\n",
    "duplicados_clave = df_consolidado_sie.duplicated(\n",
    "    subset=['tipo_documento', 'numero_identificacion']\n",
    ").sum()\n",
    "\n",
    "print(f\"\\n‚úì Duplicados en clave (tipo_documento + numero_identificacion): {duplicados_clave:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: MUESTRA DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_consolidado_sie.head(10).to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: INFORMACI√ìN DEL DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ÑπÔ∏è  INFORMACI√ìN DEL DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "df_consolidado_sie.info()\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DEL PROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros totales: {len(df_consolidado_sie):,}\")\n",
    "print(f\"‚úì Columnas finales: {len(df_consolidado_sie.columns)}\")\n",
    "print(f\"‚úì Orden de columnas: {df_consolidado_sie.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n‚úì Renombramiento realizados:\")\n",
    "for columna_vieja, columna_nueva in diccionario_rename.items():\n",
    "    print(f\"  {columna_vieja} ‚Üí {columna_nueva}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## df_Municipios_SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV de Municipios SIE\n",
    "df_municipios_sie = pd.read_csv(\n",
    "    R_Municipios_SIE,\n",
    "    sep=';',\n",
    "    encoding='latin-1',\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CARGA DE MUNICIPIOS SIE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros: {len(df_municipios_sie):,}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_municipios_sie.columns)}\")\n",
    "print(f\"\\n‚úì Columnas del archivo:\")\n",
    "print(df_municipios_sie.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ÑπÔ∏è INFORMACI√ìN DEL DATAFRAME\")\n",
    "print(\"=\"*80)\n",
    "df_municipios_sie.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## df_red_Contratada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red_contratada = pd.read_csv(\n",
    "    R_red_Contratada,\n",
    "    sep=',',\n",
    "    encoding='ansi',\n",
    "    dtype=str,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CARGA DE RED CONTRATADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros: {len(df_red_contratada):,}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_red_contratada.columns)}\")\n",
    "print(f\"\\n‚úì Columnas del archivo:\")\n",
    "print(df_red_contratada.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Depuraci√≥n bases de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Reporte red Asignada General SIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Normalizaci√≥n y Pivoteo de Red de Servicios por Afiliado\n",
    "\n",
    "1. **Objetivo**\n",
    "Transformar un listado de servicios duplicados por afiliado en un formato normalizado donde cada afiliado ocupa una sola fila y sus servicios asignados se distribuyen en columnas.\n",
    "\n",
    "2. **Descripci√≥n del Proceso**\n",
    "\n",
    "**Paso 1: Crear Columna de Informaci√≥n del Prestador (SIMPLIFICADA)**\n",
    "Se crea una nueva columna `prestador_info` que concatena dos datos clave separados por gui√≥n:\n",
    "- **NIT del prestador**: Identificador √∫nico de la entidad de salud\n",
    "- **C√≥digo de servicio**: Identificador del tipo de servicio\n",
    "\n",
    "**Ejemplo:**\n",
    "844004197-328\n",
    "\n",
    "Esta informaci√≥n ser√° el valor que se almacenar√° cuando se pivoteen los servicios.\n",
    "\n",
    "**Mejora:** Se elimin√≥ la raz√≥n social para simplificar el proceso, manteniendo solo la informaci√≥n esencial (NIT + C√≥digo de servicio).\n",
    "\n",
    "---\n",
    "\n",
    "**Paso 2: Convertir Servicios en Columnas (Pivoting)**\n",
    "Se utiliza `pivot_table` para transformar la estructura de datos:\n",
    "\n",
    "| Antes (formato largo) | Despu√©s (formato ancho) |\n",
    "|---|---|\n",
    "| M√∫ltiples filas por afiliado (una por servicio) | Una sola fila por afiliado |\n",
    "| Columna `servicio` con valores como \"MEDICINA GENERAL\", \"ODONTOLOGIA\", etc. | Cada servicio se convierte en una columna |\n",
    "\n",
    "**√çndices (filas):** Se agrupan por `abreviatura + numero_identificacion + municipio` (identifican √∫nicamente a cada afiliado)\n",
    "\n",
    "**Columnas:** Los valores √∫nicos de la columna `servicio` se convierten en nuevas columnas\n",
    "\n",
    "**Valores:** La informaci√≥n concatenada del prestador (`prestador_info` = NIT-C√≥digo)\n",
    "\n",
    "**Manejo de duplicados:** Si un afiliado tiene m√∫ltiples prestadores para el mismo servicio, se toma el primer registro (`aggfunc='first'`)\n",
    "\n",
    "---\n",
    "\n",
    "**Paso 3: Limpiar Nombres de Columnas**\n",
    "Se normalizan los nombres de las columnas de servicios:\n",
    "- Se eliminan espacios en blanco al inicio y final\n",
    "- Se convierten a may√∫sculas para consistencia\n",
    "- Se asegura que no haya espacios innecesarios\n",
    "\n",
    "**Ejemplo:** \n",
    "- `medicina general` ‚Üí `MEDICINA GENERAL`\n",
    "- ` odontologia ` ‚Üí `ODONTOLOGIA`\n",
    "\n",
    "---\n",
    "\n",
    "**Paso 4: Reorganizar Columnas por Cobertura**\n",
    "Se reordena el DataFrame para que los servicios con mayor cobertura (m√°s datos disponibles) aparezcan primero, facilitando la visualizaci√≥n y an√°lisis de informaci√≥n m√°s completa.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Validaciones Aplicadas**\n",
    "\n",
    "El bloque final de validaci√≥n genera un reporte detallado que incluye:\n",
    "\n",
    "| Validaci√≥n | Descripci√≥n |\n",
    "|---|---|\n",
    "| **Conteo Antes/Despu√©s** | Muestra cu√°ntos registros hab√≠a originalmente vs. despu√©s del pivoting, y calcula el factor de compresi√≥n |\n",
    "| **Servicios Identificados** | Lista todos los tipos de servicios encontrados en la base de datos |\n",
    "| **Cobertura por Servicio** | Identifica para cada tipo de servicio: cu√°ntos afiliados tienen datos y su porcentaje de cobertura |\n",
    "| **An√°lisis de Duplicados en la Fuente** | Cuenta cu√°ntos afiliados ten√≠an m√∫ltiples servicios y su distribuci√≥n |\n",
    "\n",
    "---\n",
    "\n",
    "**4. Resultado Final**\n",
    "\n",
    "Un DataFrame limpio y consolidado donde:\n",
    "- ‚úÖ Cada fila representa un **afiliado √∫nico**\n",
    "- ‚úÖ Cada columna representa un **tipo de servicio**\n",
    "- ‚úÖ Las celdas contienen informaci√≥n del **prestador asignado** (NIT-C√≥digo)\n",
    "- ‚úÖ Las celdas vac√≠as indican que ese afiliado **no tiene asignado ese servicio**\n",
    "\n",
    "**Ejemplo de resultado:**\n",
    "\n",
    "| abreviatura | numero_identificacion | municipio | MEDICINA GENERAL | ODONTOLOGIA | MEDICAMENTOS |\n",
    "|---|---|---|---|---|---|\n",
    "| CC | 1117324769 | Belen - Boyaca | 844004197-328 | 844004197-334 | 1032360739-989 |\n",
    "\n",
    "**Ventajas del formato simplificado:**\n",
    "- ‚ú® Celdas m√°s limpias y legibles\n",
    "- ‚ö° Menor consumo de memoria\n",
    "- üîç M√°s f√°cil de buscar e indexar\n",
    "- üìä Informaci√≥n esencial preservada (NIT + Servicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ NORMALIZACI√ìN Y PIVOTEO DE RED DE SERVICIOS (OPTIMIZADO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: CREAR COLUMNA CON INFORMACI√ìN AGREGADA (SIMPLIFICADA)\n",
    "# ============================================================================\n",
    "# Concatenar SOLO NIT + c√≥digo (m√°s simple y limpio)\n",
    "df_red_sie['prestador_info'] = (\n",
    "    df_red_sie['nit'].astype(str) + '-' + \n",
    "    df_red_sie['codigo'].astype(str)\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: PIVOT\n",
    "# ============================================================================\n",
    "registros_antes = len(df_red_sie)\n",
    "\n",
    "df_pivot = df_red_sie.pivot_table(\n",
    "    index=['abreviatura', 'numero_identificacion', 'municipio'],\n",
    "    columns='servicio',\n",
    "    values='prestador_info',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "registros_despues = len(df_pivot)\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: LIMPIAR NOMBRES DE COLUMNAS\n",
    "# ============================================================================\n",
    "df_pivot.columns.name = None\n",
    "df_pivot.columns = df_pivot.columns.str.strip().str.upper()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: VALIDACIONES (TODO VECTORIZADO)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VALIDACI√ìN DEL PROCESO DE NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CONTEOS\n",
    "factor_compresion = registros_antes / registros_despues\n",
    "print(f\"\\n‚úì Registros ANTES: {registros_antes:,}\")\n",
    "print(f\"‚úì Registros DESPU√âS: {registros_despues:,}\")\n",
    "print(f\"‚úì Factor de compresi√≥n: {factor_compresion:.1f}x\")\n",
    "print(f\"‚úì Registros consolidados: {registros_antes - registros_despues:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFICAR COLUMNAS DE SERVICIOS\n",
    "# ============================================================================\n",
    "columnas_id = {'ABREVIATURA', 'NUMERO_IDENTIFICACION', 'MUNICIPIO'}\n",
    "servicios = [col for col in df_pivot.columns if col not in columnas_id]\n",
    "\n",
    "print(f\"\\n‚úì Servicios identificados ({len(servicios)}): {servicios}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACI√ìN DE NULIDAD - VECTORIZADO\n",
    "# ============================================================================\n",
    "print(f\"\\n‚úì AN√ÅLISIS DE COBERTURA POR SERVICIO:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "nulidad_por_servicio = df_pivot[servicios].isnull().sum()\n",
    "cobertura_por_servicio = registros_despues - nulidad_por_servicio\n",
    "porcentaje_cobertura = (cobertura_por_servicio / registros_despues * 100)\n",
    "\n",
    "# CREAR DATAFRAME DE AN√ÅLISIS\n",
    "df_analisis_servicios = pd.DataFrame({\n",
    "    'Servicio': servicios,\n",
    "    'Datos': cobertura_por_servicio.values,\n",
    "    'Vac√≠os': nulidad_por_servicio.values,\n",
    "    'Cobertura %': porcentaje_cobertura.values\n",
    "}).sort_values('Datos', ascending=False)\n",
    "\n",
    "# Mostrar con formato mejorado\n",
    "for _, row in df_analisis_servicios.iterrows():\n",
    "    print(f\"  {row['Servicio']:70s} | Datos: {row['Datos']:6.0f} ({row['Cobertura %']:5.1f}%) | Vac√≠os: {row['Vac√≠os']:6.0f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACI√ìN DE DUPLICADOS EN LA FUENTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AN√ÅLISIS DE DUPLICADOS EN LA FUENTE (df_red_sie)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conteos_afiliados = df_red_sie.groupby(['abreviatura', 'numero_identificacion']).size()\n",
    "\n",
    "duplicados_stats = {\n",
    "    'Total afiliados √∫nicos (despu√©s pivot)': registros_despues,\n",
    "    'Afiliados con m√∫ltiples servicios': (conteos_afiliados > 1).sum(),\n",
    "    'Servicios promedio por afiliado': registros_antes / registros_despues,\n",
    "    'M√°ximo servicios por afiliado': conteos_afiliados.max(),\n",
    "    'M√≠nimo servicios por afiliado': conteos_afiliados.min()\n",
    "}\n",
    "\n",
    "for clave, valor in duplicados_stats.items():\n",
    "    if isinstance(valor, float):\n",
    "        print(f\"  ‚úì {clave:50s}: {valor:8.2f}\")\n",
    "    else:\n",
    "        print(f\"  ‚úì {clave:50s}: {valor:8,d}\")\n",
    "\n",
    "# Mostrar distribuci√≥n detallada de servicios\n",
    "print(f\"\\n  Distribuci√≥n de servicios por afiliado:\")\n",
    "print(\"  \" + \"-\" * 76)\n",
    "distribucion = conteos_afiliados.value_counts().sort_index()\n",
    "for num_servicios, cantidad_afiliados in distribucion.items():\n",
    "    porcentaje = (cantidad_afiliados / registros_despues * 100)\n",
    "    print(f\"    {num_servicios:2.0f} servicios: {cantidad_afiliados:7,d} afiliados ({porcentaje:5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL CONSOLIDADO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà RESUMEN FINAL DE NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros originales (con duplicados): {registros_antes:,}\")\n",
    "print(f\"‚úì Registros √∫nicos (despu√©s pivot): {registros_despues:,}\")\n",
    "print(f\"‚úì Registros consolidados: {registros_antes - registros_despues:,}\")\n",
    "print(f\"‚úì Tipo de servicios identificados: {len(servicios)}\")\n",
    "print(f\"‚úì Cobertura promedio de servicios: {porcentaje_cobertura.mean():.1f}%\")\n",
    "print(f\"‚úì Servicio con mayor cobertura: {df_analisis_servicios.iloc[0]['Servicio']} ({df_analisis_servicios.iloc[0]['Cobertura %']:.1f}%)\")\n",
    "print(f\"‚úì Servicio con menor cobertura: {df_analisis_servicios.iloc[-1]['Servicio']} ({df_analisis_servicios.iloc[-1]['Cobertura %']:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACI√ìN DE INTEGRIDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Filas en df_pivot: {len(df_pivot):,}\")\n",
    "print(f\"‚úì Columnas en df_pivot: {len(df_pivot.columns)}\")\n",
    "print(f\"‚úì Nulos en ABREVIATURA: {df_pivot['ABREVIATURA'].isna().sum():,}\")\n",
    "print(f\"‚úì Nulos en NUMERO_IDENTIFICACION: {df_pivot['NUMERO_IDENTIFICACION'].isna().sum():,}\")\n",
    "print(f\"‚úì Nulos en MUNICIPIO: {df_pivot['MUNICIPIO'].isna().sum():,}\")\n",
    "print(f\"‚úì Primera columna es: {df_pivot.columns[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MUESTRA DE DATOS (NUEVO - para verificar formato simplificado)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS - FORMATO DE prestador_info SIMPLIFICADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mostrar ejemplos de los servicios con el nuevo formato\n",
    "print(f\"\\nEjemplos de prestador_info (NIT-C√≥digo):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col in servicios[:5]:  # Mostrar primeros 5 servicios\n",
    "    ejemplos = df_pivot[col].dropna().unique()[:3]\n",
    "    print(f\"\\n  {col}:\")\n",
    "    for ej in ejemplos:\n",
    "        print(f\"    ‚Ä¢ {ej}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Reorganizaci√≥n de Columnas por Cobertura de Datos\n",
    "\n",
    "**üéØ Objetivo**\n",
    "Reorganizar las columnas del DataFrame para que los servicios con **mayor cobertura** (m√°s datos disponibles) aparezcan primero, facilitando la visualizaci√≥n y an√°lisis de la informaci√≥n m√°s completa.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Proceso Paso a Paso\n",
    "\n",
    "#### **Paso 1: Calcular Cobertura por Servicio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULAR CANTIDAD DE VALORES NO VAC√çOS POR SERVICIO\n",
    "cobertura_servicio = df_pivot.isnull().sum().sort_values()\n",
    "\n",
    "# SEPARAR COLUMNAS EN GRUPOS\n",
    "columnas_identificacion = ['ABREVIATURA', 'NUMERO_IDENTIFICACION', 'MUNICIPIO']\n",
    "columnas_servicios = [col for col in df_pivot.columns if col not in columnas_identificacion]\n",
    "\n",
    "# ORDENAR SERVICIOS POR MENOS VAC√çOS (m√°s datos primero)\n",
    "columnas_servicios_ordenadas = cobertura_servicio[cobertura_servicio.index.isin(columnas_servicios)].index.tolist()\n",
    "\n",
    "# REORGANIZAR DATAFRAME: Primero identificadores, luego servicios ordenados\n",
    "df_pivot = df_pivot[columnas_identificacion + columnas_servicios_ordenadas]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã COLUMNAS REORDENADAS POR COBERTURA (Menos vac√≠os ‚Üí M√°s vac√≠os)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in columnas_servicios_ordenadas:\n",
    "    nulos = df_pivot[col].isna().sum()\n",
    "    datos = len(df_pivot) - nulos\n",
    "    porcentaje_datos = ((len(df_pivot) - nulos) / len(df_pivot)) * 100\n",
    "    print(f\"  {col:70s} | Datos: {datos:6d} ({porcentaje_datos:5.1f}%) | Vac√≠os: {nulos:6d}\")\n",
    "\n",
    "print(\"\\n‚úì Nuevo orden de columnas:\")\n",
    "print(df_pivot.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Identificaci√≥n de portabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR NUEVA COLUMNA \"Estado_Portabilidad\" VAC√çA\n",
    "df_pivot['Estado_Portabilidad'] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Nueva columna creada: 'Estado_Portabilidad'\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total de registros con la nueva columna: {len(df_pivot)}\")\n",
    "print(f\"Columnas actuales: {df_pivot.columns.tolist()}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CREAR COLUMNA TEMPORAL PARA MERGE\n",
    "df_pt_nac['clave'] = df_pt_nac['tipo_identificacion'] + '_' + df_pt_nac['numero_identificacion'].astype(str)\n",
    "df_pivot['clave'] = df_pivot['ABREVIATURA'] + '_' + df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    "\n",
    "# IDENTIFICAR REGISTROS DE PORTABILIDAD EN df_pivot\n",
    "df_pivot['Estado_Portabilidad'] = df_pivot['clave'].isin(df_pt_nac['clave']).apply(lambda x: 'PN' if x else None)\n",
    "\n",
    "# CALCULAR ESTAD√çSTICAS\n",
    "total_df_pivot = len(df_pivot)\n",
    "total_df_pt_nac = len(df_pt_nac)\n",
    "identificados = df_pivot['Estado_Portabilidad'].notna().sum()\n",
    "no_identificados = total_df_pivot - identificados\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä IDENTIFICACI√ìN DE PORTABILIDAD NACIONAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros en df_pivot: {total_df_pivot:,}\")\n",
    "print(f\"‚úì Total de registros en df_pt_nac: {total_df_pt_nac:,}\")\n",
    "print(f\"‚úì Registros identificados (PN): {identificados:,} ({(identificados/total_df_pivot)*100:.2f}%)\")\n",
    "print(f\"‚úì Registros NO identificados: {no_identificados:,} ({(no_identificados/total_df_pivot)*100:.2f}%)\")\n",
    "\n",
    "# AUDITOR√çA: IDENTIFICAR REGISTROS DE PORTABILIDAD QUE NO APARECEN EN df_pivot\n",
    "df_pt_sin_servicios = df_pt_nac[~df_pt_nac['clave'].isin(df_pivot['clave'])]\n",
    "\n",
    "# CREAR DATAFRAME DE AUDITOR√çA\n",
    "df_auditoria = pd.DataFrame({\n",
    "    'Tp_D': df_pt_sin_servicios['tipo_identificacion'],\n",
    "    'No_D': df_pt_sin_servicios['numero_identificacion'],\n",
    "    'Descripci√≥n': 'PN sin servicios'\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AUDITOR√çA DE PORTABILIDAD\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros de portabilidad SIN servicios asignados: {len(df_auditoria):,}\")\n",
    "print(f\"\\nMuestra de registros en auditor√≠a:\")\n",
    "print(df_auditoria.head(10))\n",
    "\n",
    "# LIMPIAR COLUMNAS TEMPORALES\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_pt_nac = df_pt_nac.drop('clave', axis=1)\n",
    "\n",
    "print(\"\\n‚úì Columna 'Estado_Portabilidad' actualizada con identificaciones\")\n",
    "print(\"‚úì DataFrame de auditor√≠a creado: df_auditoria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR COLUMNA TEMPORAL PARA MERGE CON df_pt_reg\n",
    "df_pt_reg['clave'] = df_pt_reg['tipo_identificacion'] + '_' + df_pt_reg['numero_identificacion'].astype(str)\n",
    "df_pivot['clave'] = df_pivot['ABREVIATURA'] + '_' + df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    "\n",
    "# IDENTIFICAR REGISTROS DE PORTABILIDAD REGIONAL EN df_pivot\n",
    "# Si ya tiene \"PN\", agregar \"PR\" para obtener \"PN y PR\"\n",
    "df_pivot['Estado_Portabilidad'] = df_pivot.apply(\n",
    "    lambda row: (row['Estado_Portabilidad'] + ' y PR' if row['Estado_Portabilidad'] else 'PR') \n",
    "    if row['clave'] in df_pt_reg['clave'].values else row['Estado_Portabilidad'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# OBTENER EL MUNICIPIO RECEPTOR Y CREAR NUEVA COLUMNA\n",
    "df_pivot['municipio_receptor'] = None\n",
    "\n",
    "# Crear un diccionario de clave -> municipio_receptor para b√∫squeda r√°pida\n",
    "dict_municipio = dict(zip(df_pt_reg['clave'], df_pt_reg['municipio_receptor']))\n",
    "\n",
    "# Asignar los municipios receptores a los registros identificados en PR\n",
    "df_pivot.loc[df_pivot['clave'].isin(df_pt_reg['clave']), 'municipio_receptor'] = (\n",
    "    df_pivot.loc[df_pivot['clave'].isin(df_pt_reg['clave']), 'clave'].map(dict_municipio)\n",
    ")\n",
    "\n",
    "# CALCULAR ESTAD√çSTICAS ANTES Y DESPU√âS\n",
    "total_antes = len(df_pivot)\n",
    "identificados_pr = (df_pivot['Estado_Portabilidad'].notna()) & (df_pivot['Estado_Portabilidad'].str.contains('PR', na=False))\n",
    "total_pr = identificados_pr.sum()\n",
    "total_pn = df_pivot['Estado_Portabilidad'].eq('PN').sum()\n",
    "total_pn_y_pr = df_pivot['Estado_Portabilidad'].eq('PN y PR').sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä IDENTIFICACI√ìN DE PORTABILIDAD REGIONAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros en df_pivot: {total_antes:,}\")\n",
    "print(f\"‚úì Total de registros en df_pt_reg: {len(df_pt_reg):,}\")\n",
    "print(f\"‚úì Registros identificados SOLO PR: {(total_pr - total_pn_y_pr):,}\")\n",
    "print(f\"‚úì Registros identificados PN y PR: {total_pn_y_pr:,}\")\n",
    "print(f\"‚úì Total registros con PR: {total_pr:,} ({(total_pr/total_antes)*100:.2f}%)\")\n",
    "\n",
    "# AUDITOR√çA: IDENTIFICAR REGISTROS DE PORTABILIDAD REGIONAL QUE NO APARECEN EN df_pivot\n",
    "df_pt_reg_sin_servicios = df_pt_reg[~df_pt_reg['clave'].isin(df_pivot['clave'])]\n",
    "\n",
    "# AGREGAR A df_auditoria LOS REGISTROS DE PR SIN SERVICIOS\n",
    "df_auditoria_pr = pd.DataFrame({\n",
    "    'Tp_D': df_pt_reg_sin_servicios['tipo_identificacion'],\n",
    "    'No_D': df_pt_reg_sin_servicios['numero_identificacion'],\n",
    "    'Descripci√≥n': 'PR sin servicios'\n",
    "})\n",
    "\n",
    "# CONCATENAR CON df_auditoria EXISTENTE\n",
    "df_auditoria = pd.concat([df_auditoria, df_auditoria_pr], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AUDITOR√çA DE PORTABILIDAD REGIONAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros de portabilidad regional SIN servicios asignados: {len(df_auditoria_pr):,}\")\n",
    "print(f\"‚úì Total de registros en auditor√≠a: {len(df_auditoria):,}\")\n",
    "print(f\"\\nMuestra de registros en auditor√≠a:\")\n",
    "print(df_auditoria.head(15))\n",
    "\n",
    "# LIMPIAR COLUMNAS TEMPORALES\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_pt_reg = df_pt_reg.drop('clave', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Columna 'Estado_Portabilidad' actualizada con identificaciones PR\")\n",
    "print(\"‚úì Columna 'municipio_receptor' creada con datos de df_pt_reg\")\n",
    "print(\"‚úì DataFrame de auditor√≠a actualizado con registros PR sin servicios\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Eliminar df Portabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_pt_nac, df_pt_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Identificar Usaurios Certificados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### LMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR NUEVA COLUMNA \"Certificados\" VAC√çA\n",
    "df_pivot['Certificados'] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Nueva columna creada: 'Certificados'\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total de registros con la nueva columna: {len(df_pivot)}\")\n",
    "print(f\"Columnas actuales: {df_pivot.columns.tolist()}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CREAR COLUMNA TEMPORAL PARA MERGE\n",
    "df_lma['clave'] = df_lma['TPS_IDN_ID'] + '_' + df_lma['HST_IDN_NUMERO_IDENTIFICACION'].astype(str)\n",
    "df_pivot['clave'] = df_pivot['ABREVIATURA'] + '_' + df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    "\n",
    "# IDENTIFICAR REGISTROS CERTIFICADOS EN df_pivot\n",
    "df_pivot['Certificados'] = df_pivot['clave'].isin(df_lma['clave']).apply(lambda x: 'LMA' if x else None)\n",
    "\n",
    "# CALCULAR ESTAD√çSTICAS\n",
    "total_df_pivot = len(df_pivot)\n",
    "total_df_lma = len(df_lma)\n",
    "identificados = df_pivot['Certificados'].notna().sum()\n",
    "no_identificados = total_df_pivot - identificados\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä IDENTIFICACI√ìN DE USUARIOS CERTIFICADOS (LMA)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros en df_pivot: {total_df_pivot:,}\")\n",
    "print(f\"‚úì Total de registros en df_lma: {total_df_lma:,}\")\n",
    "print(f\"‚úì Registros identificados (LMA): {identificados:,} ({(identificados/total_df_pivot)*100:.2f}%)\")\n",
    "print(f\"‚úì Registros NO identificados: {no_identificados:,} ({(no_identificados/total_df_pivot)*100:.2f}%)\")\n",
    "\n",
    "# AUDITOR√çA: IDENTIFICAR REGISTROS DE LMA QUE NO APARECEN EN df_pivot\n",
    "df_lma_sin_servicios = df_lma[~df_lma['clave'].isin(df_pivot['clave'])]\n",
    "\n",
    "# CREAR DATAFRAME AUXILIAR CON ESTRUCTURA COMPLETA DE df_pivot PARA LMA SIN SERVICIOS\n",
    "# Obtener nombres de columnas de df_pivot (excepto 'clave')\n",
    "columnas_pivot = [col for col in df_pivot.columns if col != 'clave']\n",
    "\n",
    "# Crear DataFrame vac√≠o con la estructura de df_pivot\n",
    "df_lma_sin_servicios_completo = pd.DataFrame(columns=columnas_pivot)\n",
    "\n",
    "# Llenar solo las columnas necesarias\n",
    "for idx, row in df_lma_sin_servicios.iterrows():\n",
    "    nuevo_registro = {col: None for col in columnas_pivot}\n",
    "    nuevo_registro['ABREVIATURA'] = row['TPS_IDN_ID']\n",
    "    nuevo_registro['NUMERO_IDENTIFICACION'] = row['HST_IDN_NUMERO_IDENTIFICACION']\n",
    "    nuevo_registro['Certificados'] = 'LMA'\n",
    "    df_lma_sin_servicios_completo = pd.concat(\n",
    "        [df_lma_sin_servicios_completo, pd.DataFrame([nuevo_registro])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AUDITOR√çA DE USUARIOS CERTIFICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros de LMA SIN servicios asignados: {len(df_lma_sin_servicios_completo):,}\")\n",
    "\n",
    "# CONCATENAR CON df_auditoria EXISTENTE (si existe)\n",
    "if len(df_auditoria) > 0:\n",
    "    print(f\"‚úì Registros en auditor√≠a antes: {len(df_auditoria):,}\")\n",
    "    \n",
    "    # Agregar los registros de LMA en formato df_pivot a df_auditoria\n",
    "    for idx, row in df_lma_sin_servicios_completo.iterrows():\n",
    "        registro_auditoria = {\n",
    "            'Tp_D': row['ABREVIATURA'],\n",
    "            'No_D': row['NUMERO_IDENTIFICACION'],\n",
    "            'Descripci√≥n': 'LMA sin servicios'\n",
    "        }\n",
    "        df_auditoria = pd.concat(\n",
    "            [df_auditoria, pd.DataFrame([registro_auditoria])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úì Registros en auditor√≠a despu√©s: {len(df_auditoria):,}\")\n",
    "else:\n",
    "    # Si df_auditoria no existe, crear con registros de LMA\n",
    "    df_auditoria = pd.DataFrame({\n",
    "        'Tp_D': df_lma_sin_servicios['TPS_IDN_ID'],\n",
    "        'No_D': df_lma_sin_servicios['HST_IDN_NUMERO_IDENTIFICACION'],\n",
    "        'Descripci√≥n': 'LMA sin servicios'\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Total de registros en auditor√≠a: {len(df_auditoria):,}\")\n",
    "\n",
    "# AGREGAR REGISTROS LMA SIN SERVICIOS A df_pivot CON ESTRUCTURA COMPLETA\n",
    "df_pivot = pd.concat([df_pivot, df_lma_sin_servicios_completo], ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úì Registros en df_pivot despu√©s de agregar LMA sin servicios: {len(df_pivot):,}\")\n",
    "\n",
    "# LIMPIAR COLUMNAS TEMPORALES\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_lma = df_lma.drop('clave', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Columna 'Certificados' actualizada con identificaciones LMA\")\n",
    "print(\"‚úì Registros de LMA sin servicios agregados a df_pivot\")\n",
    "print(\"‚úì DataFrame de auditor√≠a actualizado con registros LMA sin servicios\")\n",
    "print(f\"‚úì Total de registros en df_pivot nuevos registros: {len(df_pivot),}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Compensados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAR COLUMNA TEMPORAL PARA MERGE CON df_compensados\n",
    "df_compensados['clave'] = df_compensados['Tp_Dc'] + '_' + df_compensados['Num_coti'].astype(str)\n",
    "df_pivot['clave'] = df_pivot['ABREVIATURA'] + '_' + df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    "\n",
    "# IDENTIFICAR REGISTROS COMPENSADOS EN df_pivot\n",
    "# Si ya tiene \"LMA\", agregar \"y Compensado\" para obtener \"LMA y Compensado\"\n",
    "# IMPORTANTE: Solo actualizar si Certificados es None o contiene 'Compensado'\n",
    "df_pivot['Certificados'] = df_pivot.apply(\n",
    "    lambda row: (row['Certificados'] + ' y Compensado' if pd.notna(row['Certificados']) and row['Certificados'] != '' else 'Compensado') \n",
    "    if row['clave'] in df_compensados['clave'].values else row['Certificados'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# CALCULAR ESTAD√çSTICAS\n",
    "total_antes = len(df_pivot)\n",
    "identificados_comp = (df_pivot['Certificados'].notna()) & (df_pivot['Certificados'].str.contains('Compensado', na=False))\n",
    "total_comp = identificados_comp.sum()\n",
    "total_lma = df_pivot['Certificados'].eq('LMA').sum()\n",
    "total_lma_y_comp = df_pivot['Certificados'].eq('LMA y Compensado').sum()\n",
    "total_solo_comp = (df_pivot['Certificados'].eq('Compensado')).sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä IDENTIFICACI√ìN DE USUARIOS COMPENSADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros en df_pivot: {total_antes:,}\")\n",
    "print(f\"‚úì Total de registros en df_compensados: {len(df_compensados):,}\")\n",
    "print(f\"‚úì Registros identificados SOLO Compensado: {total_solo_comp:,}\")\n",
    "print(f\"‚úì Registros identificados LMA y Compensado: {total_lma_y_comp:,}\")\n",
    "print(f\"‚úì Registros SOLO LMA (sin cambios): {total_lma:,}\")\n",
    "print(f\"‚úì Total registros con Compensado: {total_comp:,} ({(total_comp/total_antes)*100:.2f}%)\")\n",
    "\n",
    "# VALIDACI√ìN: Contar registros que mantienen \"LMA\"\n",
    "registros_lma_mantenidos = df_pivot['Certificados'].eq('LMA').sum()\n",
    "print(f\"‚úì Registros LMA preservados: {registros_lma_mantenidos:,}\")\n",
    "\n",
    "# AUDITOR√çA: IDENTIFICAR REGISTROS DE COMPENSADOS QUE NO APARECEN EN df_pivot\n",
    "df_compensados_sin_servicios = df_compensados[~df_compensados['clave'].isin(df_pivot['clave'])]\n",
    "\n",
    "# CREAR DATAFRAME AUXILIAR CON ESTRUCTURA COMPLETA DE df_pivot PARA COMPENSADOS SIN SERVICIOS\n",
    "# Obtener nombres de columnas de df_pivot (excepto 'clave')\n",
    "columnas_pivot = [col for col in df_pivot.columns if col != 'clave']\n",
    "\n",
    "# Crear DataFrame vac√≠o con la estructura de df_pivot\n",
    "df_compensados_sin_servicios_completo = pd.DataFrame(columns=columnas_pivot)\n",
    "\n",
    "# Llenar solo las columnas necesarias\n",
    "for idx, row in df_compensados_sin_servicios.iterrows():\n",
    "    nuevo_registro = {col: None for col in columnas_pivot}\n",
    "    nuevo_registro['ABREVIATURA'] = row['Tp_Dc']\n",
    "    nuevo_registro['NUMERO_IDENTIFICACION'] = row['Num_coti']\n",
    "    nuevo_registro['Certificados'] = 'Compensado'\n",
    "    df_compensados_sin_servicios_completo = pd.concat(\n",
    "        [df_compensados_sin_servicios_completo, pd.DataFrame([nuevo_registro])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AUDITOR√çA DE USUARIOS COMPENSADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros de Compensados SIN servicios asignados: {len(df_compensados_sin_servicios_completo):,}\")\n",
    "\n",
    "# CONCATENAR CON df_auditoria EXISTENTE (si existe)\n",
    "if len(df_auditoria) > 0:\n",
    "    print(f\"‚úì Registros en auditor√≠a antes: {len(df_auditoria):,}\")\n",
    "    \n",
    "    # Agregar los registros de Compensados en formato df_pivot a df_auditoria\n",
    "    for idx, row in df_compensados_sin_servicios_completo.iterrows():\n",
    "        registro_auditoria = {\n",
    "            'Tp_D': row['ABREVIATURA'],\n",
    "            'No_D': row['NUMERO_IDENTIFICACION'],\n",
    "            'Descripci√≥n': 'Compensado sin servicios'\n",
    "        }\n",
    "        df_auditoria = pd.concat(\n",
    "            [df_auditoria, pd.DataFrame([registro_auditoria])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úì Registros en auditor√≠a despu√©s: {len(df_auditoria):,}\")\n",
    "else:\n",
    "    # Si df_auditoria no existe, crear con registros de Compensados\n",
    "    df_auditoria = pd.DataFrame({\n",
    "        'Tp_D': df_compensados_sin_servicios['Tp_Dc'],\n",
    "        'No_D': df_compensados_sin_servicios['Num_coti'],\n",
    "        'Descripci√≥n': 'Compensado sin servicios'\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Total de registros en auditor√≠a: {len(df_auditoria):,}\")\n",
    "\n",
    "# AGREGAR REGISTROS COMPENSADOS SIN SERVICIOS A df_pivot CON ESTRUCTURA COMPLETA\n",
    "df_pivot = pd.concat([df_pivot, df_compensados_sin_servicios_completo], ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úì Registros en df_pivot despu√©s de agregar Compensados sin servicios: {len(df_pivot):,}\")\n",
    "\n",
    "# LIMPIAR COLUMNAS TEMPORALES\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_compensados = df_compensados.drop('clave', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Columna 'Certificados' actualizada con identificaciones de Compensados\")\n",
    "print(\"‚úì Registros LMA preservados sin modificaci√≥n\")\n",
    "print(\"‚úì Registros de Compensados sin servicios agregados a df_pivot\")\n",
    "print(\"‚úì DataFrame de auditor√≠a actualizado con registros Compensados sin servicios\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTAR REGISTROS POR CATEGOR√çA EN LA COLUMNA 'Certificados'\n",
    "conteo_certificados = df_pivot['Certificados'].value_counts(dropna=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DISTRIBUCI√ìN DE REGISTROS POR CATEGOR√çA DE CERTIFICADOS\")\n",
    "print(\"=\"*80)\n",
    "print(conteo_certificados)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà RESUMEN DETALLADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for categoria, cantidad in conteo_certificados.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"Sin certificado\"\n",
    "    print(f\"‚úì {label:30s}: {cantidad:7,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Total de registros: {len(df_pivot):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Eliminar certificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_lma, df_compensados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Identicar Usuarios Maestro ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚è±Ô∏è INICIO: IDENTIFICACI√ìN DE USUARIOS EN MAESTRO ADRES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# PASO 1: CREAR CLAVES VECTORIZADAS (SIN CONVERSIONES AUTOM√ÅTICAS)\n",
    "# Convertir expl√≠citamente a string antes de concatenar\n",
    "df_consolidad_adres['clave'] = (\n",
    "    df_consolidad_adres['TPS_IDN_ID'].astype(str) + '_' + \n",
    "    df_consolidad_adres['HST_IDN_NUMERO_IDENTIFICACION'].astype(str)\n",
    ")\n",
    "\n",
    "df_pivot['clave'] = (\n",
    "    df_pivot['ABREVIATURA'].astype(str) + '_' + \n",
    "    df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    ")\n",
    "\n",
    "# PASO 2: USAR MERGE EN LUGAR DE DICCIONARIO + BUCLE\n",
    "# Seleccionar SOLO las columnas necesarias para evitar duplicados\n",
    "df_adres_merge = df_consolidad_adres[[\n",
    "    'clave', 'AFL_ID', 'ENT_ID', 'DPR_ID', 'MNC_ID', 'TPS_EST_AFL_ID'\n",
    "]].copy()\n",
    "\n",
    "# Renombrar columnas EXPL√çCITAMENTE\n",
    "df_adres_merge = df_adres_merge.rename(columns={\n",
    "    'ENT_ID': 'Regimen_Adres',\n",
    "    'DPR_ID': 'DPR_ADRES',\n",
    "    'MNC_ID': 'MNC_ADRES',\n",
    "    'TPS_EST_AFL_ID': 'Estado_ADRES'\n",
    "})\n",
    "\n",
    "# VALIDACI√ìN: Verificar tipos de datos antes del merge\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE TIPOS DE DATOS ANTES DE MERGE\")\n",
    "print(\"=\"*80)\n",
    "print(\"df_adres_merge dtypes:\")\n",
    "print(df_adres_merge.dtypes)\n",
    "print(\"\\ndf_pivot clave dtype:\")\n",
    "print(f\"  {df_pivot['clave'].dtype}\")\n",
    "\n",
    "# MERGE LEFT (busca coincidencias, mantiene estructura de df_pivot)\n",
    "# indicator=True permite validar qu√© registros fueron encontrados\n",
    "df_pivot = df_pivot.merge(\n",
    "    df_adres_merge,\n",
    "    on='clave',\n",
    "    how='left',\n",
    "    indicator=False  # No usar indicator para evitar columna extra\n",
    ")\n",
    "\n",
    "# PASO 3: IDENTIFICAR REGISTROS ADRES SIN SERVICIOS (SIN BUCLES)\n",
    "# Usar isin() que es vectorizado\n",
    "df_adres_sin_servicios = df_consolidad_adres[\n",
    "    ~df_consolidad_adres['clave'].isin(df_pivot['clave'].dropna().unique())\n",
    "].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä IDENTIFICACI√ìN DE USUARIOS EN MAESTRO ADRES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_antes_pivot = len(df_pivot) - len(df_adres_sin_servicios)\n",
    "total_adres = len(df_consolidad_adres)\n",
    "identificados_adres = df_pivot['AFL_ID'].notna().sum()\n",
    "no_identificados = total_antes_pivot - identificados_adres\n",
    "\n",
    "print(f\"‚úì Total de registros en df_pivot (antes): {total_antes_pivot:,}\")\n",
    "print(f\"‚úì Total de registros en df_consolidad_adres: {total_adres:,}\")\n",
    "print(f\"‚úì Registros identificados (ADRES): {identificados_adres:,} ({(identificados_adres/total_antes_pivot)*100:.2f}%)\")\n",
    "print(f\"‚úì Registros NO identificados: {no_identificados:,} ({(no_identificados/total_antes_pivot)*100:.2f}%)\")\n",
    "\n",
    "# PASO 4: CREAR REGISTROS SIN SERVICIOS (SIN ITERROWS - VECTORIZADO)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AUDITOR√çA DE USUARIOS ADRES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros de ADRES SIN servicios: {len(df_adres_sin_servicios):,}\")\n",
    "\n",
    "# Obtener lista de columnas de df_pivot\n",
    "columnas_pivot = [col for col in df_pivot.columns if col != 'clave']\n",
    "\n",
    "# CREAR DATAFRAME DE NUEVOS REGISTROS CON VALORES EXPL√çCITOS\n",
    "# Sin loops, todo vectorizado\n",
    "df_adres_sin_servicios_completo = pd.DataFrame()\n",
    "\n",
    "# Crear solo con las columnas que tienen valores de ADRES\n",
    "df_adres_sin_servicios_completo['ABREVIATURA'] = df_adres_sin_servicios['TPS_IDN_ID'].values\n",
    "df_adres_sin_servicios_completo['NUMERO_IDENTIFICACION'] = df_adres_sin_servicios['HST_IDN_NUMERO_IDENTIFICACION'].values\n",
    "df_adres_sin_servicios_completo['AFL_ID'] = df_adres_sin_servicios['AFL_ID'].values\n",
    "df_adres_sin_servicios_completo['Regimen_Adres'] = df_adres_sin_servicios['ENT_ID'].values\n",
    "df_adres_sin_servicios_completo['DPR_ADRES'] = df_adres_sin_servicios['DPR_ID'].values\n",
    "df_adres_sin_servicios_completo['MNC_ADRES'] = df_adres_sin_servicios['MNC_ID'].values\n",
    "df_adres_sin_servicios_completo['Estado_ADRES'] = df_adres_sin_servicios['TPS_EST_AFL_ID'].values\n",
    "\n",
    "# Agregar columnas restantes con None expl√≠citamente\n",
    "for col in columnas_pivot:\n",
    "    if col not in df_adres_sin_servicios_completo.columns:\n",
    "        df_adres_sin_servicios_completo[col] = None\n",
    "\n",
    "# Reordenar columnas para que coincidan exactamente con df_pivot\n",
    "df_adres_sin_servicios_completo = df_adres_sin_servicios_completo[columnas_pivot]\n",
    "\n",
    "# VALIDACI√ìN: Verificar estructura\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE ESTRUCTURA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Columnas df_pivot: {len(df_pivot.columns)}\")\n",
    "print(f\"‚úì Columnas df_adres_sin_servicios_completo: {len(df_adres_sin_servicios_completo.columns)}\")\n",
    "print(f\"‚úì ¬øColumnas coinciden?: {list(df_pivot.columns) == list(df_adres_sin_servicios_completo.columns)}\")\n",
    "\n",
    "# Verificar tipos de datos coinciden\n",
    "print(\"\\nüîç COINCIDENCIA DE TIPOS DE DATOS\")\n",
    "tipos_coinciden = True\n",
    "for col in df_pivot.columns:\n",
    "    if col in df_adres_sin_servicios_completo.columns:\n",
    "        tipo_pivot = df_pivot[col].dtype\n",
    "        tipo_adres = df_adres_sin_servicios_completo[col].dtype\n",
    "        if tipo_pivot != tipo_adres:\n",
    "            # Para columnas None, esto es normal\n",
    "            if df_adres_sin_servicios_completo[col].dtype == 'object' and tipo_pivot == 'object':\n",
    "                continue\n",
    "            print(f\"  ‚ö†Ô∏è  {col}: pivot={tipo_pivot}, adres={tipo_adres}\")\n",
    "            tipos_coinciden = False\n",
    "\n",
    "if tipos_coinciden:\n",
    "    print(\"  ‚úÖ Todos los tipos de datos coinciden\")\n",
    "\n",
    "# PASO 5: AUDITOR√çA SIN BUCLES (VECTORIZADO)\n",
    "df_auditoria_adres = pd.DataFrame({\n",
    "    'Tp_D': df_adres_sin_servicios['TPS_IDN_ID'].values,\n",
    "    'No_D': df_adres_sin_servicios['HST_IDN_NUMERO_IDENTIFICACION'].values,\n",
    "    'Descripci√≥n': ('Adres sin servicios ' + df_adres_sin_servicios['TPS_EST_AFL_ID'].astype(str)).values\n",
    "})\n",
    "\n",
    "# Validaci√≥n de auditor√≠a\n",
    "print(\"\\nüîç VALIDACI√ìN DE AUDITOR√çA\")\n",
    "print(f\"  ‚úì Registros en auditor√≠a: {len(df_auditoria_adres):,}\")\n",
    "print(f\"  ‚úì Registros sin Tp_D (nulos): {df_auditoria_adres['Tp_D'].isna().sum():,}\")\n",
    "print(f\"  ‚úì Registros sin No_D (nulos): {df_auditoria_adres['No_D'].isna().sum():,}\")\n",
    "\n",
    "# Concatenar con auditor√≠a existente\n",
    "registros_auditoria_antes = len(df_auditoria) if len(df_auditoria) > 0 else 0\n",
    "\n",
    "if len(df_auditoria) > 0:\n",
    "    df_auditoria = pd.concat([df_auditoria, df_auditoria_adres], ignore_index=True)\n",
    "else:\n",
    "    df_auditoria = df_auditoria_adres.copy()\n",
    "\n",
    "registros_auditoria_despues = len(df_auditoria)\n",
    "\n",
    "print(f\"  ‚úì Registros en auditor√≠a antes: {registros_auditoria_antes:,}\")\n",
    "print(f\"  ‚úì Registros en auditor√≠a despu√©s: {registros_auditoria_despues:,}\")\n",
    "print(f\"  ‚úì Registros AGREGADOS: {registros_auditoria_despues - registros_auditoria_antes:,}\")\n",
    "\n",
    "# PASO 6: AGREGAR A df_pivot\n",
    "total_pivot_antes = len(df_pivot)\n",
    "\n",
    "# Validar que no haya duplicados\n",
    "duplicados_antes = df_pivot.duplicated(subset=['ABREVIATURA', 'NUMERO_IDENTIFICACION']).sum()\n",
    "if duplicados_antes > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Hay {duplicados_antes:,} duplicados potenciales en df_pivot\")\n",
    "\n",
    "df_pivot = pd.concat([df_pivot, df_adres_sin_servicios_completo], ignore_index=True)\n",
    "total_pivot_despues = len(df_pivot)\n",
    "\n",
    "# VALIDACI√ìN POST-CONCAT\n",
    "print(\"\\nüîç VALIDACI√ìN POST-CONCATENACI√ìN\")\n",
    "print(f\"  ‚úì Registros df_pivot antes: {total_pivot_antes:,}\")\n",
    "print(f\"  ‚úì Registros df_pivot despu√©s: {total_pivot_despues:,}\")\n",
    "print(f\"  ‚úì Registros AGREGADOS: {total_pivot_despues - total_pivot_antes:,}\")\n",
    "print(f\"  ‚úì ¬øSuma correcta?: {(total_pivot_antes + len(df_adres_sin_servicios_completo)) == total_pivot_despues}\")\n",
    "\n",
    "# Verificar integridad de datos\n",
    "print(f\"  ‚úì Total de filas (sin clave): {len(df_pivot):,}\")\n",
    "print(f\"  ‚úì Valores nulos en AFL_ID: {df_pivot['AFL_ID'].isna().sum():,}\")\n",
    "print(f\"  ‚úì Valores nulos en Estado_ADRES: {df_pivot['Estado_ADRES'].isna().sum():,}\")\n",
    "\n",
    "# LIMPIAR COLUMNAS TEMPORALES\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_consolidad_adres = df_consolidad_adres.drop('clave', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PROCESO COMPLETADO CON VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì Columnas ADRES agregadas a df_pivot:\")\n",
    "print(\"  - AFL_ID\")\n",
    "print(\"  - Regimen_Adres\")\n",
    "print(\"  - DPR_ADRES\")\n",
    "print(\"  - MNC_ADRES\")\n",
    "print(\"  - Estado_ADRES\")\n",
    "print(\"‚úì Registros ADRES sin servicios agregados\")\n",
    "print(\"‚úì DataFrame de auditor√≠a actualizado\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# RESUMEN FINAL CON VALIDACIONES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà RESUMEN DEL MOVIMIENTO DE DATOS (CON VALIDACIONES)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Registros totales en df_pivot: {len(df_pivot):,}\")\n",
    "print(f\"‚úì Registros totales en df_auditoria: {len(df_auditoria):,}\")\n",
    "print(f\"‚úì Registros con datos ADRES completos: {df_pivot['AFL_ID'].notna().sum():,}\")\n",
    "print(f\"‚úì Registros ADRES sin servicios: {len(df_adres_sin_servicios_completo):,}\")\n",
    "print(f\"‚úì Integridad: PN + ADRES sin servicios = {identificados_adres + len(df_adres_sin_servicios_completo):,}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "print(f\"\\n‚è±Ô∏è TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.2f} segundos ({tiempo_transcurrido/60:.2f} minutos)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener lista de todas las columnas\n",
    "columnas = df_pivot.columns.tolist()\n",
    "\n",
    "# Remover 'AFL_ID' de su posici√≥n actual\n",
    "columnas.remove('AFL_ID')\n",
    "\n",
    "# Insertar 'AFL_ID' al inicio\n",
    "columnas.insert(0, 'AFL_ID')\n",
    "\n",
    "# Reordenar el DataFrame\n",
    "df_pivot = df_pivot[columnas]\n",
    "\n",
    "# Verificar el nuevo orden\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ORDEN DE COLUMNAS ACTUALIZADO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Primera columna: {df_pivot.columns[0]}\")\n",
    "print(f\"‚úì Total de columnas: {len(df_pivot.columns)}\")\n",
    "print(f\"\\n‚úì Primeras 10 columnas:\")\n",
    "print(df_pivot.columns[:10].tolist())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### eliminacion Consolidados ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_consolidad_adres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Identicar Usuarios Maestro SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîó INTEGRACI√ìN DE CONSOLIDADO SIE CON df_pivot\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: CREAR CLAVES PARA MERGE (VECTORIZADO)\n",
    "# ============================================================================\n",
    "# Crear clave en df_consolidado_sie (convertir a string una sola vez)\n",
    "df_consolidado_sie['clave'] = (\n",
    "    df_consolidado_sie['tipo_documento'].astype(str) + '_' +\n",
    "    df_consolidado_sie['numero_identificacion'].astype(str)\n",
    ")\n",
    "\n",
    "# Crear clave en df_pivot (igual estructura)\n",
    "df_pivot['clave'] = (\n",
    "    df_pivot['ABREVIATURA'].astype(str) + '_' +\n",
    "    df_pivot['NUMERO_IDENTIFICACION'].astype(str)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Claves creadas en ambos DataFrames\")\n",
    "print(f\"  - df_consolidado_sie: {len(df_consolidado_sie):,} registros\")\n",
    "print(f\"  - df_pivot: {len(df_pivot):,} registros\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: IDENTIFICAR REGISTROS SIE SIN SERVICIOS (VECTORIZADO)\n",
    "# ============================================================================\n",
    "# Sin usar loops, todo vectorizado con isin()\n",
    "df_sie_sin_servicios = df_consolidado_sie[\n",
    "    ~df_consolidado_sie['clave'].isin(df_pivot['clave'].dropna().unique())\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n‚úì Registros SIE SIN servicios identificados: {len(df_sie_sin_servicios):,}\")\n",
    "print(f\"  ({(len(df_sie_sin_servicios) / len(df_consolidado_sie) * 100):.2f}% del total)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: PREPARAR DATAFRAME PARA MERGE (SELECCIONAR)\n",
    "# ============================================================================\n",
    "# Seleccionar columnas que queremos agregar a df_pivot\n",
    "columnas_sie_para_merge = ['clave', 'municipio_SIE', 'estado_SIE', \n",
    "                           'regimen_SIE', 'estado_traslado', 'PE']\n",
    "\n",
    "df_sie_merge = df_consolidado_sie[columnas_sie_para_merge].copy()\n",
    "\n",
    "print(f\"\\n‚úì Columnas SIE seleccionadas para merge: {columnas_sie_para_merge}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: MERGE LEFT (df_pivot es la tabla principal)\n",
    "# ============================================================================\n",
    "total_pivot_antes = len(df_pivot)\n",
    "\n",
    "df_pivot = df_pivot.merge(\n",
    "    df_sie_merge,\n",
    "    on='clave',\n",
    "    how='left',\n",
    "    indicator=False\n",
    ")\n",
    "\n",
    "total_pivot_despues = len(df_pivot)\n",
    "\n",
    "print(f\"\\n‚úì MERGE completado:\")\n",
    "print(f\"  - Registros df_pivot antes: {total_pivot_antes:,}\")\n",
    "print(f\"  - Registros df_pivot despu√©s: {total_pivot_despues:,}\")\n",
    "print(f\"  - Integridad: ¬øSe mantienen registros?: {total_pivot_antes == total_pivot_despues}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: CREAR AUDITOR√çA CON REGISTROS SIE SIN SERVICIOS (VECTORIZADO)\n",
    "# ============================================================================\n",
    "# Crear descripci√≥n VECTORIZADA (sin bucles)\n",
    "descripcion_sie = (\n",
    "    'SIE sin servicios ' + df_sie_sin_servicios['estado_SIE'].astype(str)\n",
    ")\n",
    "\n",
    "# Crear DataFrame de auditor√≠a directamente (sin concatenaciones)\n",
    "df_auditoria_sie = pd.DataFrame({\n",
    "    'Tp_D': df_sie_sin_servicios['tipo_documento'].values,\n",
    "    'No_D': df_sie_sin_servicios['numero_identificacion'].values,\n",
    "    'Descripci√≥n': descripcion_sie.values\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úì DataFrame de auditor√≠a SIE creado:\")\n",
    "print(f\"  - Registros: {len(df_auditoria_sie):,}\")\n",
    "print(f\"  - Columnas: {df_auditoria_sie.columns.tolist()}\")\n",
    "\n",
    "# VALIDACI√ìN: Verificar que no hay nulos en descripci√≥n\n",
    "nulos_en_descripcion = df_auditoria_sie['Descripci√≥n'].isna().sum()\n",
    "print(f\"  - Valores nulos en Descripci√≥n: {nulos_en_descripcion:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 6: AGREGAR REGISTROS SIE A AUDITOR√çA (EFICIENTE)\n",
    "# ============================================================================\n",
    "registros_auditoria_antes = len(df_auditoria) if 'df_auditoria' in dir() else 0\n",
    "\n",
    "if 'df_auditoria' in dir() and len(df_auditoria) > 0:\n",
    "    df_auditoria = pd.concat([df_auditoria, df_auditoria_sie], ignore_index=True)\n",
    "    registros_auditoria_despues = len(df_auditoria)\n",
    "    \n",
    "    print(f\"\\n‚úì Auditor√≠a actualizada:\")\n",
    "    print(f\"  - Registros antes: {registros_auditoria_antes:,}\")\n",
    "    print(f\"  - Registros agregados: {len(df_auditoria_sie):,}\")\n",
    "    print(f\"  - Registros despu√©s: {registros_auditoria_despues:,}\")\n",
    "else:\n",
    "    df_auditoria = df_auditoria_sie.copy()\n",
    "    print(f\"\\n‚úì DataFrame df_auditoria creado con registros SIE: {len(df_auditoria):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 7: AGREGAR REGISTROS SIE SIN SERVICIOS A df_pivot (VECTORIZADO CORRECTO)\n",
    "# ============================================================================\n",
    "# Obtener lista de columnas de df_pivot\n",
    "columnas_pivot = [col for col in df_pivot.columns if col != 'clave']\n",
    "\n",
    "# ‚úÖ M√âTODO CORRECTO: Crear DataFrame con los datos de df_sie_sin_servicios\n",
    "# El truco es usar .reindex() para agregar columnas faltantes con None\n",
    "df_sie_sin_servicios_completo = df_sie_sin_servicios[['tipo_documento', 'numero_identificacion', \n",
    "                                                       'municipio_SIE', 'estado_SIE', \n",
    "                                                       'regimen_SIE', 'estado_traslado', 'PE', 'clave']].copy()\n",
    "\n",
    "# Renombrar columnas para que coincidan con df_pivot\n",
    "df_sie_sin_servicios_completo = df_sie_sin_servicios_completo.rename(columns={\n",
    "    'tipo_documento': 'ABREVIATURA',\n",
    "    'numero_identificacion': 'NUMERO_IDENTIFICACION'\n",
    "})\n",
    "\n",
    "# ‚úÖ REINDEXAR: Agregar todas las columnas que falten con NaN/None\n",
    "df_sie_sin_servicios_completo = df_sie_sin_servicios_completo.reindex(columns=columnas_pivot)\n",
    "\n",
    "print(f\"\\n‚úì Registros SIE sin servicios preparados para agregar: {len(df_sie_sin_servicios_completo):,}\")\n",
    "\n",
    "# Validar estructura\n",
    "columnas_coinciden = list(df_pivot.columns) == list(df_sie_sin_servicios_completo.columns)\n",
    "print(f\"  - ¬øColumnas coinciden?: {columnas_coinciden}\")\n",
    "\n",
    "if not columnas_coinciden:\n",
    "    print(f\"  ‚ö†Ô∏è  ADVERTENCIA: Las columnas no coinciden\")\n",
    "    print(f\"  Columnas df_pivot: {list(df_pivot.columns)}\")\n",
    "    print(f\"  Columnas df_sie: {list(df_sie_sin_servicios_completo.columns)}\")\n",
    "\n",
    "# Concatenar\n",
    "total_pivot_antes_concat = len(df_pivot)\n",
    "df_pivot = pd.concat([df_pivot, df_sie_sin_servicios_completo], ignore_index=True)\n",
    "total_pivot_despues_concat = len(df_pivot)\n",
    "\n",
    "print(f\"\\n‚úì Registros SIE sin servicios agregados a df_pivot:\")\n",
    "print(f\"  - Registros df_pivot antes: {total_pivot_antes_concat:,}\")\n",
    "print(f\"  - Registros agregados: {len(df_sie_sin_servicios_completo):,}\")\n",
    "print(f\"  - Registros df_pivot despu√©s: {total_pivot_despues_concat:,}\")\n",
    "print(f\"  - Integridad suma: {(total_pivot_antes_concat + len(df_sie_sin_servicios_completo)) == total_pivot_despues_concat}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 8: ELIMINAR COLUMNA CLAVE (LIMPIEZA)\n",
    "# ============================================================================\n",
    "df_pivot = df_pivot.drop('clave', axis=1)\n",
    "df_consolidado_sie = df_consolidado_sie.drop('clave', axis=1)\n",
    "\n",
    "print(f\"\\n‚úì Columna 'clave' eliminada de ambos DataFrames\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 9: VALIDACI√ìN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN FINAL DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Conteo de datos SIE\n",
    "tiene_datos_sie = df_pivot[['municipio_SIE', 'estado_SIE', 'regimen_SIE']].notna().any(axis=1).sum()\n",
    "sin_datos_sie = len(df_pivot) - tiene_datos_sie\n",
    "\n",
    "print(f\"\\n‚úì Distribuci√≥n de datos SIE en df_pivot:\")\n",
    "print(f\"  - Registros CON datos SIE: {tiene_datos_sie:,} ({(tiene_datos_sie/len(df_pivot)*100):.2f}%)\")\n",
    "print(f\"  - Registros SIN datos SIE: {sin_datos_sie:,} ({(sin_datos_sie/len(df_pivot)*100):.2f}%)\")\n",
    "\n",
    "# An√°lisis de nulidad por columna SIE\n",
    "print(f\"\\n‚úì An√°lisis de nulidad en columnas SIE:\")\n",
    "print(\"-\" * 80)\n",
    "columnas_sie = ['municipio_SIE', 'estado_SIE', 'regimen_SIE', 'estado_traslado', 'PE']\n",
    "for col in columnas_sie:\n",
    "    nulos = df_pivot[col].isna().sum()\n",
    "    porcentaje = (nulos / len(df_pivot)) * 100\n",
    "    print(f\"  {col:25s}: {nulos:10,d} nulos ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# Validar integridad de auditor√≠a\n",
    "print(f\"\\n‚úì Validaci√≥n de auditor√≠a:\")\n",
    "print(f\"  - Total de registros en df_auditoria: {len(df_auditoria):,}\")\n",
    "print(f\"  - Registros 'SIE sin servicios': {df_auditoria['Descripci√≥n'].str.contains('SIE sin servicios').sum():,}\")\n",
    "print(f\"  - Valores nulos en Descripci√≥n: {df_auditoria['Descripci√≥n'].isna().sum():,}\")\n",
    "\n",
    "# Muestra de descripciones SIE\n",
    "print(f\"\\n‚úì Ejemplos de descripciones SIE en auditor√≠a:\")\n",
    "print(\"-\" * 80)\n",
    "descripciones_sie = df_auditoria[df_auditoria['Descripci√≥n'].str.contains('SIE sin servicios')]\n",
    "if len(descripciones_sie) > 0:\n",
    "    for descripcion in descripciones_sie['Descripci√≥n'].unique()[:10]:\n",
    "        cantidad = (descripciones_sie['Descripci√≥n'] == descripcion).sum()\n",
    "        print(f\"  '{descripcion}': {cantidad:,} registros\")\n",
    "\n",
    "# ============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà RESUMEN DE INTEGRACI√ìN SIE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Total de registros en df_consolidado_sie: {len(df_consolidado_sie) + len(df_sie_sin_servicios):,}\")\n",
    "print(f\"  - Con servicios (en df_pivot): {tiene_datos_sie:,}\")\n",
    "print(f\"  - Sin servicios (en auditor√≠a): {len(df_auditoria_sie):,}\")\n",
    "print(f\"\\n‚úì Total de registros en df_pivot: {len(df_pivot):,}\")\n",
    "print(f\"‚úì Total de registros en df_auditoria: {len(df_auditoria):,}\")\n",
    "print(f\"\\n‚úì Columnas SIE agregadas a df_pivot:\")\n",
    "for col in columnas_sie:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 10: ELIMINACI√ìN DE CONSOLIDADO SIE (LIBERACI√ìN DE MEMORIA)\n",
    "# ============================================================================\n",
    "del df_consolidado_sie\n",
    "\n",
    "print(\"\\n‚úì DataFrame df_consolidado_sie eliminado (memoria liberada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Organizar Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ REORGANIZANDO ORDEN DE COLUMNAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: DEFINIR NUEVO ORDEN DE COLUMNAS (EXPL√çCITO)\n",
    "# ============================================================================\n",
    "nuevo_orden_columnas = [\n",
    "    # Identificadores principales\n",
    "    'AFL_ID',\n",
    "    'ABREVIATURA',\n",
    "    'NUMERO_IDENTIFICACION',\n",
    "    \n",
    "    # Datos ADRES\n",
    "    'Regimen_Adres',\n",
    "    'DPR_ADRES',\n",
    "    'MNC_ADRES',\n",
    "    'Estado_ADRES',\n",
    "    \n",
    "    # Datos SIE\n",
    "    'municipio_SIE',\n",
    "    'estado_SIE',\n",
    "    'regimen_SIE',\n",
    "    'estado_traslado',\n",
    "    'PE',\n",
    "    \n",
    "    # Estado de certificaci√≥n y portabilidad\n",
    "    'Certificados',\n",
    "    'Estado_Portabilidad',\n",
    "    'municipio_receptor',\n",
    "    \n",
    "    # Ubicaci√≥n del afiliado\n",
    "    'MUNICIPIO',\n",
    "    \n",
    "    # SERVICIOS (Orden original)\n",
    "    'MEDICINA GENERAL',\n",
    "    'LABORATORIO CLINICO',\n",
    "    'ODONTOLOGIA GENERAL',\n",
    "    'MEDICAMENTOS',\n",
    "    'OPTOMETRIA',\n",
    "    'PROMOCION Y PREVENCION GENERAL',\n",
    "    'OFTALMOLOGIA',\n",
    "    'SERVICIO DE DEMANDA INDUCIDA A LOS PROGRAMAS DE DETECCION TEMPRANA Y PROTECCION ESPECIFICA',\n",
    "    'PROCEDIMIENTOS MENORES',\n",
    "    'RUTA MATERNO PERINATAL',\n",
    "    'RUTA PROMOCION Y MANTENIMIENTO',\n",
    "    'HOSPITALIZACION GENERAL',\n",
    "    'SERVICIO DE URGENCIAS',\n",
    "    'TRANSPORTE ASISTENCIAL BASICO',\n",
    "    'RAYOS X',\n",
    "    'FONOAUDIOLOGIA Y/O TERAPIA DEL LENGUAJE',\n",
    "    'FISIOTERAPIA',\n",
    "    'TERAPIA OCUPACIONAL',\n",
    "    'TRANSPORTE ASISTENCIAL MEDICALIZADO',\n",
    "    'TERAPIA RESPIRATORIA',\n",
    "    'IMAGENES DIAGNOSTICAS - IONIZANTES',\n",
    "    'ENFERMERIA',\n",
    "    'ANESTESIA',\n",
    "    'INFECTOLOGIA',\n",
    "    'MEDICINA FAMILIAR',\n",
    "    'CIRUGIA ORTOPEDICA',\n",
    "    'PROTECCION ESPECIFICA - VACUNACION',\n",
    "    'SERVICIO FARMACEUTICO',\n",
    "    'NUTRICION Y DIETETICA',\n",
    "    'TOMA DE MUESTRAS DE LABORATORIO CLINICO',\n",
    "    'MEDICAMENTOS ESPECIALIZADOS',\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: VALIDAR QUE TODAS LAS COLUMNAS EXISTAN\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando columnas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "columnas_actuales = set(df_pivot.columns)\n",
    "columnas_esperadas = set(nuevo_orden_columnas)\n",
    "\n",
    "# Columnas que faltan en el nuevo orden\n",
    "columnas_faltantes = columnas_actuales - columnas_esperadas\n",
    "\n",
    "if columnas_faltantes:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Columnas en df_pivot que NO est√°n en nuevo_orden_columnas:\")\n",
    "    print(f\"   Columnas: {sorted(columnas_faltantes)}\")\n",
    "    print(f\"\\n   Estas columnas se agregar√° al final del nuevo orden\")\n",
    "    nuevo_orden_columnas.extend(sorted(columnas_faltantes))\n",
    "\n",
    "# Columnas que faltan en el DataFrame\n",
    "columnas_inexistentes = columnas_esperadas - columnas_actuales\n",
    "\n",
    "if columnas_inexistentes:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Columnas en nuevo_orden_columnas que NO existen en df_pivot:\")\n",
    "    print(f\"   Columnas: {sorted(columnas_inexistentes)}\")\n",
    "    print(f\"\\n   Estas columnas se ELIMINAR√ÅN del nuevo orden\")\n",
    "    nuevo_orden_columnas = [col for col in nuevo_orden_columnas if col not in columnas_inexistentes]\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: REORDENAR DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Aplicando nuevo orden de columnas...\")\n",
    "\n",
    "df_pivot = df_pivot[nuevo_orden_columnas]\n",
    "\n",
    "print(f\"\\n‚úì DataFrame reorganizado exitosamente\")\n",
    "print(f\"  - Total de columnas: {len(df_pivot.columns)}\")\n",
    "print(f\"  - Total de filas: {len(df_pivot):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: VALIDACI√ìN DEL NUEVO ORDEN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VALIDACI√ìN DEL NUEVO ORDEN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì SECCIONES DEL NUEVO ORDEN:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Identificadores\n",
    "print(f\"\\n1Ô∏è‚É£  IDENTIFICADORES PRINCIPALES:\")\n",
    "for i, col in enumerate(['AFL_ID', 'ABREVIATURA', 'NUMERO_IDENTIFICACION'], 1):\n",
    "    if col in df_pivot.columns:\n",
    "        print(f\"    {i}. {col}\")\n",
    "\n",
    "# ADRES\n",
    "print(f\"\\n2Ô∏è‚É£  DATOS ADRES:\")\n",
    "for i, col in enumerate(['Regimen_Adres', 'DPR_ADRES', 'MNC_ADRES', 'Estado_ADRES'], 1):\n",
    "    if col in df_pivot.columns:\n",
    "        print(f\"    {i}. {col}\")\n",
    "\n",
    "# SIE\n",
    "print(f\"\\n3Ô∏è‚É£  DATOS SIE:\")\n",
    "for i, col in enumerate(['municipio_SIE', 'estado_SIE', 'regimen_SIE', 'estado_traslado', 'PE'], 1):\n",
    "    if col in df_pivot.columns:\n",
    "        print(f\"    {i}. {col}\")\n",
    "\n",
    "# Estados y Portabilidad\n",
    "print(f\"\\n4Ô∏è‚É£  CERTIFICACI√ìN Y PORTABILIDAD:\")\n",
    "for i, col in enumerate(['Certificados', 'Estado_Portabilidad', 'municipio_receptor'], 1):\n",
    "    if col in df_pivot.columns:\n",
    "        print(f\"    {i}. {col}\")\n",
    "\n",
    "# Locaci√≥n\n",
    "print(f\"\\n5Ô∏è‚É£  UBICACI√ìN:\")\n",
    "if 'MUNICIPIO' in df_pivot.columns:\n",
    "    print(f\"    1. MUNICIPIO\")\n",
    "\n",
    "# Servicios\n",
    "print(f\"\\n6Ô∏è‚É£  SERVICIOS ASIGNADOS ({len([col for col in df_pivot.columns if col.isupper() and col not in ['AFL_ID', 'ABREVIATURA', 'NUMERO_IDENTIFICACION', 'MUNICIPIO', 'Regimen_Adres', 'DPR_ADRES', 'MNC_ADRES', 'Estado_ADRES', 'municipio_SIE', 'estado_SIE', 'regimen_SIE', 'estado_traslado', 'PE', 'Certificados', 'Estado_Portabilidad', 'municipio_receptor']])} servicios):\")\n",
    "servicios = [col for col in df_pivot.columns if col.isupper() and col not in ['AFL_ID', 'ABREVIATURA', 'NUMERO_IDENTIFICACION', 'MUNICIPIO', 'Regimen_Adres', 'DPR_ADRES', 'MNC_ADRES', 'Estado_ADRES', 'municipio_SIE', 'estado_SIE', 'regimen_SIE', 'estado_traslado', 'PE', 'Certificados', 'Estado_Portabilidad', 'municipio_receptor']]\n",
    "for i, servicio in enumerate(servicios, 1):\n",
    "    print(f\"    {i:2d}. {servicio}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: MUESTRA DE PRIMERAS FILAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE PRIMERAS FILAS CON NUEVO ORDEN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mostrar solo primeras 5 columnas + algunos servicios\n",
    "columnas_muestra = list(df_pivot.columns[:15]) + ['MEDICINA GENERAL', 'SERVICIOS', 'MEDICAMENTOS'][:3]\n",
    "columnas_muestra_existentes = [col for col in columnas_muestra if col in df_pivot.columns]\n",
    "\n",
    "print(f\"\\nPrimeras 5 registros (mostrando primeras 15 columnas):\")\n",
    "print(\"-\" * 80)\n",
    "print(df_pivot[columnas_muestra_existentes].head().to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 6: INFORMACI√ìN DE TIPOS DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç INFORMACI√ìN DE TIPOS DE DATOS (PRIMERAS 20 COLUMNAS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(\"-\" * 80)\n",
    "for col in df_pivot.columns[:20]:\n",
    "    dtype = df_pivot[col].dtype\n",
    "    nulos = df_pivot[col].isna().sum()\n",
    "    porcentaje_nulos = (nulos / len(df_pivot)) * 100\n",
    "    print(f\"  {col:60s} | {str(dtype):15s} | Nulos: {nulos:10,d} ({porcentaje_nulos:5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 7: RESUMEN FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE REORGANIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar secciones\n",
    "identificadores = 3\n",
    "adres = 4\n",
    "sie = 5\n",
    "portabilidad = 3\n",
    "locacion = 1\n",
    "servicios_count = len(servicios)\n",
    "\n",
    "print(f\"\\n‚úì DESGLOSE POR SECCI√ìN:\")\n",
    "print(f\"  - Identificadores: {identificadores} columnas\")\n",
    "print(f\"  - Datos ADRES: {adres} columnas\")\n",
    "print(f\"  - Datos SIE: {sie} columnas\")\n",
    "print(f\"  - Certificaci√≥n/Portabilidad: {portabilidad} columnas\")\n",
    "print(f\"  - Ubicaci√≥n: {locacion} columna\")\n",
    "print(f\"  - Servicios: {servicios_count} columnas\")\n",
    "print(f\"  {'‚îÄ' * 40}\")\n",
    "print(f\"  - TOTAL: {len(df_pivot.columns)} columnas\")\n",
    "\n",
    "print(f\"\\n‚úì ESTRUCTURA FINAL:\")\n",
    "print(f\"  - Registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Columnas: {len(df_pivot.columns)}\")\n",
    "print(f\"  - Tama√±o en memoria: {df_pivot.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n‚úì PRIMERAS COLUMNAS DEL NUEVO ORDEN:\")\n",
    "for i, col in enumerate(df_pivot.columns[:10], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 8: MOSTRAR ORDEN COMPLETO (OPCIONAL)\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì ORDEN COMPLETO DE COLUMNAS:\")\n",
    "print(\"-\" * 80)\n",
    "for i, col in enumerate(df_pivot.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ REORGANIZACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Municipio ADRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîó UNIFICANDO COLUMNAS DPR_ADRES Y MNC_ADRES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: VALIDAR QUE LAS COLUMNAS EXISTAN\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando columnas existentes...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "columnas_requeridas = ['DPR_ADRES', 'MNC_ADRES']\n",
    "columnas_presentes = [col for col in columnas_requeridas if col in df_pivot.columns]\n",
    "columnas_faltantes = [col for col in columnas_requeridas if col not in df_pivot.columns]\n",
    "\n",
    "print(f\"‚úì Columnas a unificar: {columnas_presentes}\")\n",
    "\n",
    "if columnas_faltantes:\n",
    "    print(f\"\\n‚ùå ERROR: Las siguientes columnas NO existen en df_pivot:\")\n",
    "    for col in columnas_faltantes:\n",
    "        print(f\"   - {col}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Operaci√≥n cancelada. Verifica los nombres de las columnas.\")\n",
    "    \n",
    "else:\n",
    "    # ========================================================================\n",
    "    # PASO 2: ANALIZAR DATOS ACTUALES\n",
    "    # ========================================================================\n",
    "    print(f\"\\n‚úì Analizando datos actuales...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìä COLUMNA 'DPR_ADRES' (C√≥digo Departamento - 2 d√≠gitos):\")\n",
    "    print(f\"   - Tipo de dato: {df_pivot['DPR_ADRES'].dtype}\")\n",
    "    print(f\"   - Total de registros: {len(df_pivot):,}\")\n",
    "    print(f\"   - Valores no nulos: {df_pivot['DPR_ADRES'].notna().sum():,}\")\n",
    "    print(f\"   - Valores nulos/vac√≠os: {df_pivot['DPR_ADRES'].isna().sum():,}\")\n",
    "    print(f\"   - Valores √∫nicos: {df_pivot['DPR_ADRES'].nunique()}\")\n",
    "    print(f\"   - Ejemplos:\")\n",
    "    ejemplos_dpr = df_pivot['DPR_ADRES'].dropna().unique()[:5]\n",
    "    for ej in ejemplos_dpr:\n",
    "        print(f\"     ‚Ä¢ {ej} (tipo: {type(ej).__name__}, longitud: {len(str(ej))})\")\n",
    "    \n",
    "    print(f\"\\nüìä COLUMNA 'MNC_ADRES' (C√≥digo Municipio - 3 d√≠gitos):\")\n",
    "    print(f\"   - Tipo de dato: {df_pivot['MNC_ADRES'].dtype}\")\n",
    "    print(f\"   - Total de registros: {len(df_pivot):,}\")\n",
    "    print(f\"   - Valores no nulos: {df_pivot['MNC_ADRES'].notna().sum():,}\")\n",
    "    print(f\"   - Valores nulos/vac√≠os: {df_pivot['MNC_ADRES'].isna().sum():,}\")\n",
    "    print(f\"   - Valores √∫nicos: {df_pivot['MNC_ADRES'].nunique()}\")\n",
    "    print(f\"   - Ejemplos:\")\n",
    "    ejemplos_mnc = df_pivot['MNC_ADRES'].dropna().unique()[:5]\n",
    "    for ej in ejemplos_mnc:\n",
    "        print(f\"     ‚Ä¢ {ej} (tipo: {type(ej).__name__}, longitud: {len(str(ej))})\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 3: CREAR FUNCI√ìN DE UNIFICACI√ìN\n",
    "    # ========================================================================\n",
    "    print(f\"\\n‚úì Creando funci√≥n de unificaci√≥n...\")\n",
    "    \n",
    "    def unificar_id_municipal(fila):\n",
    "        \"\"\"\n",
    "        Unifica DPR_ADRES (2 d√≠gitos) + MNC_ADRES (3 d√≠gitos)\n",
    "        Resultado: XXXXX (5 d√≠gitos) √≥ vac√≠o si ambos son nulos/vac√≠os\n",
    "        \n",
    "        Ejemplos:\n",
    "        - 85 + 315 = 85315\n",
    "        - 09 + 001 = 09001\n",
    "        - NaN + NaN = NaN (vac√≠o)\n",
    "        - 85 + NaN = NaN (ambos deben existir)\n",
    "        \"\"\"\n",
    "        dpr = fila['DPR_ADRES']\n",
    "        mnc = fila['MNC_ADRES']\n",
    "        \n",
    "        # Si ambos son nulos o vac√≠os\n",
    "        if pd.isna(dpr) or pd.isna(mnc):\n",
    "            return None\n",
    "        \n",
    "        # Convertir a string y limpiar espacios\n",
    "        dpr_str = str(dpr).strip()\n",
    "        mnc_str = str(mnc).strip()\n",
    "        \n",
    "        # Si despu√©s de limpiar quedan vac√≠os\n",
    "        if not dpr_str or not mnc_str:\n",
    "            return None\n",
    "        \n",
    "        # Rellenar con ceros si es necesario\n",
    "        dpr_str = dpr_str.zfill(2)  # Asegurar 2 d√≠gitos\n",
    "        mnc_str = mnc_str.zfill(3)  # Asegurar 3 d√≠gitos\n",
    "        \n",
    "        # Concatenar\n",
    "        return dpr_str + mnc_str\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 4: CREAR NUEVA COLUMNA\n",
    "    # ========================================================================\n",
    "    print(f\"\\n‚úì Creando nueva columna 'ID_MNC_ADRES'...\")\n",
    "    \n",
    "    # Encontrar √≠ndice de MNC_ADRES para insertar la nueva columna en su lugar\n",
    "    indice_mnc = df_pivot.columns.get_loc('MNC_ADRES')\n",
    "    \n",
    "    # Crear la nueva columna\n",
    "    df_pivot['ID_MNC_ADRES'] = df_pivot.apply(unificar_id_municipal, axis=1)\n",
    "    \n",
    "    print(f\"‚úì Nueva columna creada exitosamente\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 5: VALIDAR RESULTADOS ANTES DE ELIMINAR\n",
    "    # ========================================================================\n",
    "    print(f\"\\n‚úì Validando resultados de unificaci√≥n...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Estad√≠sticas de la nueva columna\n",
    "    print(f\"\\nüìä COLUMNA 'ID_MNC_ADRES' (Nueva):\")\n",
    "    print(f\"   - Tipo de dato: {df_pivot['ID_MNC_ADRES'].dtype}\")\n",
    "    print(f\"   - Valores no nulos: {df_pivot['ID_MNC_ADRES'].notna().sum():,}\")\n",
    "    print(f\"   - Valores nulos/vac√≠os: {df_pivot['ID_MNC_ADRES'].isna().sum():,}\")\n",
    "    print(f\"   - Valores √∫nicos: {df_pivot['ID_MNC_ADRES'].nunique()}\")\n",
    "    print(f\"   - Longitud m√≠nima: {df_pivot['ID_MNC_ADRES'].str.len().min() if df_pivot['ID_MNC_ADRES'].notna().any() else 'N/A'}\")\n",
    "    print(f\"   - Longitud m√°xima: {df_pivot['ID_MNC_ADRES'].str.len().max() if df_pivot['ID_MNC_ADRES'].notna().any() else 'N/A'}\")\n",
    "    \n",
    "    # Ejemplos de unificaci√≥n\n",
    "    print(f\"\\nüìã EJEMPLOS DE UNIFICACI√ìN:\")\n",
    "    print(f\"   DPR_ADRES ‚Üí MNC_ADRES ‚Üí ID_MNC_ADRES\")\n",
    "    print(f\"   ‚îÄ\" * 30)\n",
    "    \n",
    "    # Seleccionar ejemplos variados\n",
    "    df_ejemplos = df_pivot[['DPR_ADRES', 'MNC_ADRES', 'ID_MNC_ADRES']].drop_duplicates()\n",
    "    df_ejemplos_muestra = pd.concat([\n",
    "        df_ejemplos[df_ejemplos['ID_MNC_ADRES'].notna()].head(5),\n",
    "        df_ejemplos[df_ejemplos['ID_MNC_ADRES'].isna()].head(3)\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    for idx, row in df_ejemplos_muestra.iterrows():\n",
    "        dpr_val = str(row['DPR_ADRES']) if pd.notna(row['DPR_ADRES']) else 'NaN'\n",
    "        mnc_val = str(row['MNC_ADRES']) if pd.notna(row['MNC_ADRES']) else 'NaN'\n",
    "        id_val = str(row['ID_MNC_ADRES']) if pd.notna(row['ID_MNC_ADRES']) else 'NaN'\n",
    "        \n",
    "        print(f\"   {dpr_val:6s} ‚Üí {mnc_val:6s} ‚Üí {id_val}\")\n",
    "    \n",
    "    # Validar que no hay valores duplicados incorrectamente\n",
    "    print(f\"\\n‚úì Validaciones adicionales:\")\n",
    "    \n",
    "    # Contar coincidencias correctas\n",
    "    df_valida = df_pivot[df_pivot['ID_MNC_ADRES'].notna()].copy()\n",
    "    \n",
    "    if len(df_valida) > 0:\n",
    "        # Verificar que los primeros 2 caracteres coincidan con DPR_ADRES\n",
    "        dpr_check = df_valida['ID_MNC_ADRES'].str[:2] == df_valida['DPR_ADRES'].astype(str).str.zfill(2)\n",
    "        print(f\"   ‚Ä¢ Coincidencia DPR_ADRES con ID_MNC_ADRES: {dpr_check.sum():,} / {len(dpr_check):,} registros\")\n",
    "        \n",
    "        # Verificar que los √∫ltimos 3 caracteres coincidan con MNC_ADRES\n",
    "        mnc_check = df_valida['ID_MNC_ADRES'].str[-3:] == df_valida['MNC_ADRES'].astype(str).str.zfill(3)\n",
    "        print(f\"   ‚Ä¢ Coincidencia MNC_ADRES con ID_MNC_ADRES: {mnc_check.sum():,} / {len(mnc_check):,} registros\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 6: REORDENAR Y ELIMINAR COLUMNAS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n‚úì Reorganizando dataframe...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Obtener la lista de columnas actual\n",
    "    columnas_actuales = list(df_pivot.columns)\n",
    "    \n",
    "    # Obtener √≠ndices de las columnas a manipular\n",
    "    idx_dpr = columnas_actuales.index('DPR_ADRES')\n",
    "    idx_mnc = columnas_actuales.index('MNC_ADRES')\n",
    "    idx_id_mnc = columnas_actuales.index('ID_MNC_ADRES')\n",
    "    \n",
    "    print(f\"\\n   Posiciones actuales:\")\n",
    "    print(f\"   ‚Ä¢ DPR_ADRES en posici√≥n: {idx_dpr}\")\n",
    "    print(f\"   ‚Ä¢ MNC_ADRES en posici√≥n: {idx_mnc}\")\n",
    "    print(f\"   ‚Ä¢ ID_MNC_ADRES en posici√≥n: {idx_id_mnc}\")\n",
    "    \n",
    "    # Crear nueva lista de columnas sin DPR_ADRES, MNC_ADRES e ID_MNC_ADRES\n",
    "    nuevas_columnas = [col for col in columnas_actuales \n",
    "                       if col not in ['DPR_ADRES', 'MNC_ADRES', 'ID_MNC_ADRES']]\n",
    "    \n",
    "    # Insertar ID_MNC_ADRES en la posici√≥n de MNC_ADRES (que es la menor de las dos)\n",
    "    posicion_insercion = min(idx_dpr, idx_mnc)\n",
    "    nuevas_columnas.insert(posicion_insercion, 'ID_MNC_ADRES')\n",
    "    \n",
    "    # Aplicar nuevo orden\n",
    "    df_pivot = df_pivot[nuevas_columnas]\n",
    "    \n",
    "    print(f\"\\n   Nuevo orden aplicado:\")\n",
    "    print(f\"   ‚Ä¢ ID_MNC_ADRES en posici√≥n: {nuevas_columnas.index('ID_MNC_ADRES')}\")\n",
    "    print(f\"   ‚Ä¢ DPR_ADRES: ELIMINADA ‚úì\")\n",
    "    print(f\"   ‚Ä¢ MNC_ADRES: ELIMINADA ‚úì\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 7: RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ UNIFICACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä RESUMEN DE CAMBIOS:\")\n",
    "    print(f\"-\" * 80)\n",
    "    print(f\"‚úì Columnas eliminadas: 2\")\n",
    "    print(f\"  ‚Ä¢ DPR_ADRES (C√≥digo Departamento)\")\n",
    "    print(f\"  ‚Ä¢ MNC_ADRES (C√≥digo Municipio)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Columnas a√±adidas: 1\")\n",
    "    print(f\"  ‚Ä¢ ID_MNC_ADRES (C√≥digo Departamento + Municipio unificado)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Estad√≠sticas de la nueva columna:\")\n",
    "    print(f\"  ‚Ä¢ Registros totales: {len(df_pivot):,}\")\n",
    "    print(f\"  ‚Ä¢ Valores rellenados (no nulos): {df_pivot['ID_MNC_ADRES'].notna().sum():,}\")\n",
    "    print(f\"  ‚Ä¢ Valores vac√≠os (nulos): {df_pivot['ID_MNC_ADRES'].isna().sum():,}\")\n",
    "    print(f\"  ‚Ä¢ Porcentaje rellenado: {(df_pivot['ID_MNC_ADRES'].notna().sum() / len(df_pivot) * 100):.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Valores √∫nicos: {df_pivot['ID_MNC_ADRES'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\n‚úì Cambio en estructura:\")\n",
    "    print(f\"  ‚Ä¢ Columnas anteriores: {len(columnas_actuales)}\")\n",
    "    print(f\"  ‚Ä¢ Columnas actuales: {len(df_pivot.columns)}\")\n",
    "    print(f\"  ‚Ä¢ Diferencia: {len(columnas_actuales) - len(df_pivot.columns)} columna eliminada ‚úì\")\n",
    "    \n",
    "    print(f\"\\n‚úì Columnas cercanas a ID_MNC_ADRES:\")\n",
    "    idx_id_mnc_nueva = df_pivot.columns.get_loc('ID_MNC_ADRES')\n",
    "    print(f\"  ‚Ä¢ Posici√≥n: {idx_id_mnc_nueva}\")\n",
    "    \n",
    "    start_idx = max(0, idx_id_mnc_nueva - 2)\n",
    "    end_idx = min(len(df_pivot.columns), idx_id_mnc_nueva + 3)\n",
    "    \n",
    "    for i, col in enumerate(df_pivot.columns[start_idx:end_idx], start_idx + 1):\n",
    "        marcador = \" ‚Üê NEW\" if col == 'ID_MNC_ADRES' else \"\"\n",
    "        print(f\"  ‚Ä¢ [{i:2d}] {col}{marcador}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 8: MUESTRA DE DATOS\n",
    "    # ========================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã MUESTRA DE DATOS REORGANIZADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Seleccionar columnas para mostrar\n",
    "    columnas_muestra = [\n",
    "        'AFL_ID',\n",
    "        'NUMERO_IDENTIFICACION',\n",
    "        'Regimen_Adres',\n",
    "        'ID_MNC_ADRES',\n",
    "        'Estado_ADRES',\n",
    "        'municipio_SIE'\n",
    "    ]\n",
    "    \n",
    "    columnas_muestra_existentes = [col for col in columnas_muestra if col in df_pivot.columns]\n",
    "    \n",
    "    print(f\"\\nPrimeros 10 registros:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(df_pivot[columnas_muestra_existentes].head(10).to_string())\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PASO 9: INFORMACI√ìN Final\n",
    "    # ========================================================================\n",
    "    tiempo_final = time.time()\n",
    "    tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n‚úÖ El dataframe est√° listo para la siguiente operaci√≥n\")\n",
    "    print(f\"\\nüí° PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
    "    print(f\"  1. Guardar el dataframe modificado\")\n",
    "    print(f\"  2. Verificar integridad de datos si es necesario\")\n",
    "    print(f\"  3. Exportar a Excel con el nuevo orden\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Estado SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DISTRIBUCI√ìN DE CATEGOR√çAS EN COLUMNA 'estado_SIE'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conteo_estado_sie = df_pivot['estado_SIE'].value_counts(dropna=False)\n",
    "\n",
    "for categoria, cantidad in conteo_estado_sie.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"Sin dato (NaN)\"\n",
    "    print(f\"  {label:30s}: {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"‚úì Categor√≠as √∫nicas: {df_pivot['estado_SIE'].nunique()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ NORMALIZANDO COLUMNA 'estado_SIE'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: DEFINIR DICCIONARIO DE MAPEO\n",
    "# ============================================================================\n",
    "diccionario_estado_sie = {\n",
    "    'Activo': 'AC',\n",
    "    'Retirado': 'RE',\n",
    "    'Fallecido': 'AF',\n",
    "    'Suspendido': 'SM'\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Diccionario de mapeo definido:\")\n",
    "print(\"-\" * 80)\n",
    "for clave, valor in diccionario_estado_sie.items():\n",
    "    print(f\"  {clave:20s} ‚Üí {valor}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: ANALIZAR DATOS ACTUALES\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Analizando datos actuales de 'estado_SIE'...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS ANTES DEL MAPEO:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos: {df_pivot['estado_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['estado_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['estado_SIE'].nunique()}\")\n",
    "\n",
    "# Mostrar distribuci√≥n actual\n",
    "print(f\"\\nüìã DISTRIBUCI√ìN ACTUAL DE 'estado_SIE':\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "conteo_actual = df_pivot['estado_SIE'].value_counts(dropna=False)\n",
    "\n",
    "for categoria, cantidad in conteo_actual.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"NaN (vac√≠o)\"\n",
    "    print(f\"  {label:30s}: {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: VALIDAR QUE TODAS LAS CATEGOR√çAS EXISTAN EN EL DICCIONARIO\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando mapeo...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Obtener categor√≠as √∫nicas (sin NaN)\n",
    "categorias_actuales = set(df_pivot['estado_SIE'].dropna().unique())\n",
    "categorias_diccionario = set(diccionario_estado_sie.keys())\n",
    "\n",
    "# Categor√≠as que no est√°n en el diccionario\n",
    "categorias_sin_mapeo = categorias_actuales - categorias_diccionario\n",
    "\n",
    "if categorias_sin_mapeo:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Algunas categor√≠as NO est√°n en el diccionario:\")\n",
    "    for categoria in sorted(categorias_sin_mapeo):\n",
    "        cantidad = (df_pivot['estado_SIE'] == categoria).sum()\n",
    "        porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "        print(f\"  - '{categoria}': {cantidad:,} registros ({porcentaje:.2f}%)\")\n",
    "    print(f\"\\n  Estas categor√≠as se MANTENDR√ÅN sin cambios (si no est√°n en diccionario)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Todas las categor√≠as est√°n en el diccionario\")\n",
    "\n",
    "# Categor√≠as en el diccionario que no existen en los datos\n",
    "categorias_no_usadas = categorias_diccionario - categorias_actuales\n",
    "\n",
    "if categorias_no_usadas:\n",
    "    print(f\"\\nüí° INFORMACI√ìN: Algunas categor√≠as del diccionario NO aparecen en los datos:\")\n",
    "    for categoria in sorted(categorias_no_usadas):\n",
    "        print(f\"  - '{categoria}' ‚Üí '{diccionario_estado_sie[categoria]}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: CREAR FUNCI√ìN DE MAPEO (MANTIENE NULOS Y SIN MAPEO)\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Creando funci√≥n de mapeo...\")\n",
    "\n",
    "def mapear_estado_sie(valor):\n",
    "    \"\"\"\n",
    "    Mapea valores de estado_SIE seg√∫n el diccionario.\n",
    "    - Si el valor est√° en el diccionario, retorna el valor mapeado\n",
    "    - Si es NaN/None, retorna NaN/None (mantiene vac√≠os)\n",
    "    - Si no est√° en el diccionario, retorna el valor original\n",
    "    \n",
    "    Ejemplos:\n",
    "    - 'Activo' ‚Üí 'AC'\n",
    "    - 'Fallecido' ‚Üí 'AF'\n",
    "    - NaN ‚Üí NaN (vac√≠o)\n",
    "    - 'OtroValor' ‚Üí 'OtroValor' (sin cambios si no est√° en diccionario)\n",
    "    \"\"\"\n",
    "    # Si es nulo, retornar nulo\n",
    "    if pd.isna(valor):\n",
    "        return None\n",
    "    \n",
    "    # Si est√° en el diccionario, retornar el valor mapeado\n",
    "    if valor in diccionario_estado_sie:\n",
    "        return diccionario_estado_sie[valor]\n",
    "    \n",
    "    # Si no est√° en el diccionario, retornar el valor original\n",
    "    return valor\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: APLICAR MAPEO (VECTORIZADO CUANDO ES POSIBLE)\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Aplicando mapeo a la columna...\")\n",
    "\n",
    "# M√âTODO VECTORIZADO: map() + fillna() es m√°s eficiente que apply()\n",
    "# Para Series, .map() es mejor que apply() porque puede usar diccionarios directamente\n",
    "\n",
    "# Crear m√°scara de valores no nulos\n",
    "mascara_no_nulos = df_pivot['estado_SIE'].notna()\n",
    "\n",
    "# Aplicar mapeo SOLO a valores no nulos\n",
    "df_pivot['estado_SIE'] = df_pivot['estado_SIE'].map(diccionario_estado_sie)\n",
    "\n",
    "print(f\"‚úì Mapeo aplicado exitosamente\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 6: VALIDAR RESULTADOS\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando resultados...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DESPU√âS DEL MAPEO:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos: {df_pivot['estado_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['estado_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['estado_SIE'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 7: IMPRIMIR CATEGOR√çAS NORMALIZADAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DISTRIBUCI√ìN DE CATEGOR√çAS NORMALIZADAS EN 'estado_SIE'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar categor√≠as\n",
    "conteo_normalizado = df_pivot['estado_SIE'].value_counts(dropna=False).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n‚úì CATEGOR√çAS ENCONTRADAS ({len(conteo_normalizado)}):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for categoria, cantidad in conteo_normalizado.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"NaN (vac√≠o/sin dato)\"\n",
    "    \n",
    "    # Buscar la categor√≠a original en el diccionario\n",
    "    categoria_original = None\n",
    "    for original, nueva in diccionario_estado_sie.items():\n",
    "        if nueva == categoria:\n",
    "            categoria_original = original\n",
    "            break\n",
    "    \n",
    "    if categoria_original:\n",
    "        etiqueta_completa = f\"{label} ({categoria_original})\"\n",
    "    else:\n",
    "        etiqueta_completa = label\n",
    "    \n",
    "    print(f\"  {etiqueta_completa:50s}: {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# Crear tabla resumen\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üìã TABLA RESUMEN DE NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_resumen_normalizacion = pd.DataFrame({\n",
    "    'Categor√≠a_Normalizada': conteo_normalizado.index,\n",
    "    'Cantidad': conteo_normalizado.values,\n",
    "    'Porcentaje': (conteo_normalizado.values / len(df_pivot) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Agregar categor√≠a original\n",
    "def obtener_categoria_original(codigo):\n",
    "    if pd.isna(codigo):\n",
    "        return \"Sin dato\"\n",
    "    for original, nueva in diccionario_estado_sie.items():\n",
    "        if nueva == codigo:\n",
    "            return original\n",
    "    return \"N/A\"\n",
    "\n",
    "df_resumen_normalizacion['Categor√≠a_Original'] = df_resumen_normalizacion['Categor√≠a_Normalizada'].apply(obtener_categoria_original)\n",
    "\n",
    "# Reordenar columnas\n",
    "df_resumen_normalizacion = df_resumen_normalizacion[['Categor√≠a_Original', 'Categor√≠a_Normalizada', 'Cantidad', 'Porcentaje']]\n",
    "\n",
    "print(\"\\n\" + df_resumen_normalizacion.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 8: VALIDACI√ìN DE INTEGRIDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar que el total de registros se mantiene\n",
    "print(f\"\\n‚úì Integridad de registros:\")\n",
    "print(f\"  - Total registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Suma de distribuci√≥n: {conteo_normalizado.sum():,}\")\n",
    "print(f\"  - ¬øCoinciden?: {len(df_pivot) == conteo_normalizado.sum()}\")\n",
    "\n",
    "# Verificar c√≥digos v√°lidos\n",
    "codigos_validos = set(diccionario_estado_sie.values())\n",
    "codigos_en_datos = set(df_pivot['estado_SIE'].dropna().unique())\n",
    "codigos_inesperados = codigos_en_datos - codigos_validos\n",
    "\n",
    "if codigos_inesperados:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: C√≥digos encontrados que NO est√°n en el diccionario:\")\n",
    "    for codigo in sorted(codigos_inesperados):\n",
    "        print(f\"  - {codigo}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Todos los c√≥digos son v√°lidos\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 9: MUESTRA DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS NORMALIZADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Seleccionar columnas para mostrar\n",
    "columnas_muestra = [\n",
    "    'AFL_ID',\n",
    "    'NUMERO_IDENTIFICACION',\n",
    "    'Estado_ADRES',\n",
    "    'estado_SIE',\n",
    "    'regimen_SIE'\n",
    "]\n",
    "\n",
    "columnas_muestra_existentes = [col for col in columnas_muestra if col in df_pivot.columns]\n",
    "\n",
    "print(f\"\\nPrimeros 15 registros:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_pivot[columnas_muestra_existentes].head(15).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 10: RESUMEN FINAL\n",
    "# ============================================================================\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NORMALIZACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Columna normalizada: 'estado_SIE'\")\n",
    "print(f\"  - Categor√≠as despu√©s: {df_pivot['estado_SIE'].nunique()}\")\n",
    "print(f\"  - Valores nulos preservados: {df_pivot['estado_SIE'].isna().sum():,}\")\n",
    "\n",
    "print(f\"\\nüîÑ MAPEO APLICADO:\")\n",
    "for original, nuevo in diccionario_estado_sie.items():\n",
    "    cantidad = (df_resumen_normalizacion['Categor√≠a_Original'] == original).sum()\n",
    "    if cantidad > 0:\n",
    "        cantidad_real = df_resumen_normalizacion[df_resumen_normalizacion['Categor√≠a_Original'] == original]['Cantidad'].values[0]\n",
    "        print(f\"  '{original}' ‚Üí '{nuevo}': {cantidad_real:,} registros\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Regimen SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ NORMALIZANDO COLUMNA 'regimen_SIE'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: DEFINIR DICCIONARIO DE MAPEO\n",
    "# ============================================================================\n",
    "diccionario_regimen_sie = {\n",
    "    'Subsidiado': 'EPS025',\n",
    "    'Contributivo': 'EPSC25'\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Diccionario de mapeo definido:\")\n",
    "print(\"-\" * 80)\n",
    "for clave, valor in diccionario_regimen_sie.items():\n",
    "    print(f\"  {clave:20s} ‚Üí {valor}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: ANALIZAR DATOS ACTUALES\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Analizando datos actuales de 'regimen_SIE'...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS ANTES DEL MAPEO:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos: {df_pivot['regimen_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['regimen_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['regimen_SIE'].nunique()}\")\n",
    "\n",
    "# Mostrar distribuci√≥n actual\n",
    "print(f\"\\nüìã DISTRIBUCI√ìN ACTUAL DE 'regimen_SIE':\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "conteo_actual = df_pivot['regimen_SIE'].value_counts(dropna=False)\n",
    "\n",
    "for categoria, cantidad in conteo_actual.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"NaN (vac√≠o)\"\n",
    "    print(f\"  {label:30s}: {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: VALIDAR QUE TODAS LAS CATEGOR√çAS EXISTAN EN EL DICCIONARIO\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando mapeo...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Obtener categor√≠as √∫nicas (sin NaN)\n",
    "categorias_actuales = set(df_pivot['regimen_SIE'].dropna().unique())\n",
    "categorias_diccionario = set(diccionario_regimen_sie.keys())\n",
    "\n",
    "# Categor√≠as que no est√°n en el diccionario\n",
    "categorias_sin_mapeo = categorias_actuales - categorias_diccionario\n",
    "\n",
    "if categorias_sin_mapeo:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Algunas categor√≠as NO est√°n en el diccionario:\")\n",
    "    for categoria in sorted(categorias_sin_mapeo):\n",
    "        cantidad = (df_pivot['regimen_SIE'] == categoria).sum()\n",
    "        porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "        print(f\"  - '{categoria}': {cantidad:,} registros ({porcentaje:.2f}%)\")\n",
    "    print(f\"\\n  Estas categor√≠as se MANTENDR√ÅN sin cambios (si no est√°n en diccionario)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Todas las categor√≠as est√°n en el diccionario\")\n",
    "\n",
    "# Categor√≠as en el diccionario que no existen en los datos\n",
    "categorias_no_usadas = categorias_diccionario - categorias_actuales\n",
    "\n",
    "if categorias_no_usadas:\n",
    "    print(f\"\\nüí° INFORMACI√ìN: Algunas categor√≠as del diccionario NO aparecen en los datos:\")\n",
    "    for categoria in sorted(categorias_no_usadas):\n",
    "        print(f\"  - '{categoria}' ‚Üí '{diccionario_regimen_sie[categoria]}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: APLICAR MAPEO (VECTORIZADO)\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Aplicando mapeo a la columna...\")\n",
    "\n",
    "# M√âTODO VECTORIZADO: map() es m√°s eficiente que apply()\n",
    "df_pivot['regimen_SIE'] = df_pivot['regimen_SIE'].map(diccionario_regimen_sie)\n",
    "\n",
    "print(f\"‚úì Mapeo aplicado exitosamente\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: VALIDAR RESULTADOS\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando resultados...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DESPU√âS DEL MAPEO:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos: {df_pivot['regimen_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['regimen_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['regimen_SIE'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 6: IMPRIMIR CATEGOR√çAS NORMALIZADAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä DISTRIBUCI√ìN DE CATEGOR√çAS NORMALIZADAS EN 'regimen_SIE'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar categor√≠as\n",
    "conteo_normalizado = df_pivot['regimen_SIE'].value_counts(dropna=False).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n‚úì CATEGOR√çAS ENCONTRADAS ({len(conteo_normalizado)}):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for categoria, cantidad in conteo_normalizado.items():\n",
    "    porcentaje = (cantidad / len(df_pivot)) * 100\n",
    "    label = categoria if pd.notna(categoria) else \"NaN (vac√≠o/sin dato)\"\n",
    "    \n",
    "    # Buscar la categor√≠a original en el diccionario\n",
    "    categoria_original = None\n",
    "    for original, nueva in diccionario_regimen_sie.items():\n",
    "        if nueva == categoria:\n",
    "            categoria_original = original\n",
    "            break\n",
    "    \n",
    "    if categoria_original:\n",
    "        etiqueta_completa = f\"{label} ({categoria_original})\"\n",
    "    else:\n",
    "        etiqueta_completa = label\n",
    "    \n",
    "    print(f\"  {etiqueta_completa:50s}: {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# Crear tabla resumen\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"üìã TABLA RESUMEN DE NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_resumen_normalizacion = pd.DataFrame({\n",
    "    'Categor√≠a_Normalizada': conteo_normalizado.index,\n",
    "    'Cantidad': conteo_normalizado.values,\n",
    "    'Porcentaje': (conteo_normalizado.values / len(df_pivot) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Agregar categor√≠a original\n",
    "def obtener_categoria_original(codigo):\n",
    "    if pd.isna(codigo):\n",
    "        return \"Sin dato\"\n",
    "    for original, nueva in diccionario_regimen_sie.items():\n",
    "        if nueva == codigo:\n",
    "            return original\n",
    "    return \"N/A\"\n",
    "\n",
    "df_resumen_normalizacion['Categor√≠a_Original'] = df_resumen_normalizacion['Categor√≠a_Normalizada'].apply(obtener_categoria_original)\n",
    "\n",
    "# Reordenar columnas\n",
    "df_resumen_normalizacion = df_resumen_normalizacion[['Categor√≠a_Original', 'Categor√≠a_Normalizada', 'Cantidad', 'Porcentaje']]\n",
    "\n",
    "print(\"\\n\" + df_resumen_normalizacion.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 7: VALIDACI√ìN DE INTEGRIDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar que el total de registros se mantiene\n",
    "print(f\"\\n‚úì Integridad de registros:\")\n",
    "print(f\"  - Total registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Suma de distribuci√≥n: {conteo_normalizado.sum():,}\")\n",
    "print(f\"  - ¬øCoinciden?: {len(df_pivot) == conteo_normalizado.sum()}\")\n",
    "\n",
    "# Verificar c√≥digos v√°lidos\n",
    "codigos_validos = set(diccionario_regimen_sie.values())\n",
    "codigos_en_datos = set(df_pivot['regimen_SIE'].dropna().unique())\n",
    "codigos_inesperados = codigos_en_datos - codigos_validos\n",
    "\n",
    "if codigos_inesperados:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: C√≥digos encontrados que NO est√°n en el diccionario:\")\n",
    "    for codigo in sorted(codigos_inesperados):\n",
    "        print(f\"  - {codigo}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Todos los c√≥digos son v√°lidos\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 8: MUESTRA DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS NORMALIZADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Seleccionar columnas para mostrar\n",
    "columnas_muestra = [\n",
    "    'AFL_ID',\n",
    "    'NUMERO_IDENTIFICACION',\n",
    "    'Regimen_Adres',\n",
    "    'regimen_SIE',\n",
    "    'estado_SIE'\n",
    "]\n",
    "\n",
    "columnas_muestra_existentes = [col for col in columnas_muestra if col in df_pivot.columns]\n",
    "\n",
    "print(f\"\\nPrimeros 15 registros:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_pivot[columnas_muestra_existentes].head(15).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 9: COMPARACI√ìN ENTRE COLUMNAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN: regimen_SIE vs Regimen_Adres\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear tabla de comparaci√≥n\n",
    "df_comparacion = pd.DataFrame({\n",
    "    'regimen_SIE': df_pivot['regimen_SIE'].value_counts(dropna=False),\n",
    "    'Regimen_Adres': df_pivot['Regimen_Adres'].value_counts(dropna=False)\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "print(\"\\nDistribuci√≥n comparativa:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_comparacion.to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 10: RESUMEN FINAL\n",
    "# ============================================================================\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NORMALIZACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Columna normalizada: 'regimen_SIE'\")\n",
    "print(f\"  - Categor√≠as despu√©s: {df_pivot['regimen_SIE'].nunique()}\")\n",
    "print(f\"  - Valores nulos preservados: {df_pivot['regimen_SIE'].isna().sum():,}\")\n",
    "\n",
    "print(f\"\\nüîÑ MAPEO APLICADO:\")\n",
    "for original, nuevo in diccionario_regimen_sie.items():\n",
    "    cantidad = (df_resumen_normalizacion['Categor√≠a_Original'] == original).sum()\n",
    "    if cantidad > 0:\n",
    "        cantidad_real = df_resumen_normalizacion[df_resumen_normalizacion['Categor√≠a_Original'] == original]['Cantidad'].values[0]\n",
    "        print(f\"  '{original}' ‚Üí '{nuevo}': {cantidad_real:,} registros\")\n",
    "\n",
    "print(f\"\\nüí° NOTAS:\")\n",
    "print(f\"  - Solo hay 2 categor√≠as (Subsidiado y Contributivo)\")\n",
    "print(f\"  - Se normaliz√≥ a c√≥digos de EPS: EPS025 (Subsidiado) y EPSC25 (Contributivo)\")\n",
    "print(f\"  - Los valores nulos se mantienen como vac√≠os\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Municipio SIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üó∫Ô∏è  NORMALIZANDO COLUMNA 'municipio_SIE' A C√ìDIGO DANE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: AN√ÅLISIS PREVIO DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Analizando datos antes de la normalizaci√≥n...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE 'municipio_SIE' ANTES:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos: {df_pivot['municipio_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['municipio_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['municipio_SIE'].nunique()}\")\n",
    "\n",
    "valores_vacios = (df_pivot['municipio_SIE'].isna()) | (df_pivot['municipio_SIE'].str.strip() == '')\n",
    "print(f\"  - Valores vac√≠os/NaN total: {valores_vacios.sum():,}\")\n",
    "\n",
    "print(f\"\\nüìä INFORMACI√ìN DE 'df_municipios_sie':\")\n",
    "print(f\"  - Total de municipios en cat√°logo: {len(df_municipios_sie):,}\")\n",
    "print(f\"  - Columna 'municipio' (c√≥digo): {df_municipios_sie['municipio'].nunique()} c√≥digos √∫nicos\")\n",
    "print(f\"  - Columna 'descripcion' (nombre): {df_municipios_sie['descripcion'].nunique()} descripciones √∫nicas\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: REVISAR DUPLICADOS EN EL CAT√ÅLOGO DE MUNICIPIOS\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Verificando integridad del cat√°logo de municipios...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "descripciones_duplicadas = df_municipios_sie[df_municipios_sie.duplicated(subset=['descripcion'], keep=False)]\n",
    "\n",
    "if len(descripciones_duplicadas) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Se encontraron descripciones duplicadas:\")\n",
    "    print(f\"  Cantidad de registros con descripciones duplicadas: {len(descripciones_duplicadas)}\")\n",
    "    print(f\"\\n  Primeras duplicadas encontradas:\")\n",
    "    print(descripciones_duplicadas.groupby('descripcion').size().head(10).to_string())\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No hay descripciones duplicadas en el cat√°logo\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: CREAR DICCIONARIO DE B√öSQUEDA Y GUARDAR NOMBRES ANTES DE ELIMINAR\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Creando diccionario de b√∫squeda y guardando informaci√≥n...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ‚úÖ GUARDAR DICCIONARIO DE NOMBRES ANTES DE ELIMINAR df_municipios_sie\n",
    "diccionario_municipios = dict(zip(\n",
    "    df_municipios_sie['descripcion'].str.strip().str.upper(),\n",
    "    df_municipios_sie['municipio'].astype(str)\n",
    "))\n",
    "\n",
    "# ‚úÖ GUARDAR DICCIONARIO INVERSO (c√≥digo ‚Üí nombre) PARA PASO 14\n",
    "diccionario_nombres_municipios = dict(zip(\n",
    "    df_municipios_sie['municipio'].astype(str),\n",
    "    df_municipios_sie['descripcion'].str.strip()\n",
    "))\n",
    "\n",
    "print(f\"  - Diccionario creado con {len(diccionario_municipios)} pares descripci√≥n-c√≥digo\")\n",
    "print(f\"  - Diccionario inverso creado con {len(diccionario_nombres_municipios)} pares c√≥digo-nombre\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: PREPARAR COLUMNA PARA B√öSQUEDA\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Preparando datos para b√∫squeda...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "municipio_sIE_original = df_pivot['municipio_SIE'].copy()\n",
    "municipio_sIE_normalizado = df_pivot['municipio_SIE'].str.strip().str.upper()\n",
    "\n",
    "print(f\"  - Datos preparados y listos para b√∫squeda\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 5: REALIZAR B√öSQUEDA Y MAPEO\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Realizando b√∫squeda y mapeo...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_pivot['municipio_SIE_codigo'] = municipio_sIE_normalizado.map(diccionario_municipios)\n",
    "\n",
    "print(f\"  - B√∫squeda completada\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 6: IDENTIFICAR REGISTROS NO NORMALIZADOS\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Identificando registros no normalizados...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "registros_vacios = valores_vacios.sum()\n",
    "\n",
    "registros_sin_mapeo = (\n",
    "    (municipio_sIE_original.notna()) & \n",
    "    (municipio_sIE_original.str.strip() != '') & \n",
    "    (df_pivot['municipio_SIE_codigo'].isna())\n",
    ").sum()\n",
    "\n",
    "registros_normalizados = (df_pivot['municipio_SIE_codigo'].notna()).sum() - registros_vacios\n",
    "\n",
    "print(f\"\\nüìä RESULTADO DEL MAPEO:\")\n",
    "print(f\"  - Registros normalizados: {registros_normalizados:,}\")\n",
    "print(f\"  - Registros sin mapeo (error): {registros_sin_mapeo:,}\")\n",
    "print(f\"  - Registros vac√≠os (sin valor): {registros_vacios:,}\")\n",
    "print(f\"  - Total procesado: {registros_normalizados + registros_sin_mapeo + registros_vacios:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 7: MOSTRAR MUNICIPIOS NO ENCONTRADOS\n",
    "# ============================================================================\n",
    "if registros_sin_mapeo > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ö†Ô∏è  MUNICIPIOS NO ENCONTRADOS EN CAT√ÅLOGO DANE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    municipios_no_encontrados = df_pivot[\n",
    "        (municipio_sIE_original.notna()) & \n",
    "        (municipio_sIE_original.str.strip() != '') & \n",
    "        (df_pivot['municipio_SIE_codigo'].isna())\n",
    "    ]['municipio_SIE'].unique()\n",
    "    \n",
    "    print(f\"\\n‚úó {len(municipios_no_encontrados)} municipios √∫nicos no encontrados:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, municipio in enumerate(sorted(municipios_no_encontrados), 1):\n",
    "        cantidad = (municipio_sIE_original == municipio).sum()\n",
    "        print(f\"  {idx:3d}. '{municipio}' ‚Üí {cantidad:,} registros\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 8: COPIAR REGISTROS NO NORMALIZADOS A AUDITOR√çA\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Copiando registros no normalizados a auditor√≠a...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "registros_auditoria = df_pivot[\n",
    "    (municipio_sIE_original.notna()) & \n",
    "    (municipio_sIE_original.str.strip() != '') & \n",
    "    (df_pivot['municipio_SIE_codigo'].isna())\n",
    "].copy()\n",
    "\n",
    "if registros_sin_mapeo > 0:\n",
    "    registros_auditoria['Descripci√≥n'] = 'No se identifica codigo DANE SIE'\n",
    "    registros_auditoria['municipio_SIE_original'] = registros_auditoria['municipio_SIE']\n",
    "    \n",
    "    df_auditoria = pd.concat([df_auditoria, registros_auditoria], ignore_index=True)\n",
    "    \n",
    "    print(f\"  - {registros_sin_mapeo:,} registros copiados a auditor√≠a\")\n",
    "    print(f\"  - Descripci√≥n asignada: 'No se identifica codigo DANE SIE'\")\n",
    "    print(f\"  - Total registros en auditor√≠a: {len(df_auditoria):,}\")\n",
    "else:\n",
    "    print(f\"  - No hay registros para copiar a auditor√≠a\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 9: VALIDAR C√ìDIGOS DANE (5 D√çGITOS)\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Validando formato de c√≥digos DANE...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n  Analizando c√≥digos DANE obtenidos...\")\n",
    "\n",
    "codigos_validos = 0\n",
    "codigos_invalidos = 0\n",
    "codigos_invalidos_list = []\n",
    "\n",
    "for codigo in df_pivot['municipio_SIE_codigo'].dropna().unique():\n",
    "    try:\n",
    "        codigo_str = str(codigo)\n",
    "        if len(codigo_str) == 5 and codigo_str.isdigit():\n",
    "            codigos_validos += 1\n",
    "        else:\n",
    "            codigos_invalidos += 1\n",
    "            if codigo not in codigos_invalidos_list:\n",
    "                codigos_invalidos_list.append(codigo)\n",
    "    except:\n",
    "        codigos_invalidos += 1\n",
    "\n",
    "print(f\"  - C√≥digos DANE v√°lidos (5 d√≠gitos): {codigos_validos:,}\")\n",
    "if codigos_invalidos > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  C√≥digos DANE inv√°lidos: {codigos_invalidos:,}\")\n",
    "    print(f\"     C√≥digos encontrados: {codigos_invalidos_list[:10]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 10: MOSTRAR EJEMPLOS DE NORMALIZACI√ìN EXITOSA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EJEMPLOS DE NORMALIZACI√ìN EXITOSA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_normalizados_ej = df_pivot[\n",
    "    (municipio_sIE_original.notna()) & \n",
    "    (df_pivot['municipio_SIE_codigo'].notna())\n",
    "][['municipio_SIE', 'municipio_SIE_codigo']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nPrimeros 20 municipios normalizados:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, (nombre, codigo) in enumerate(df_normalizados_ej.head(20).values, 1):\n",
    "    print(f\"  {idx:2d}. '{nombre}' ‚Üí '{codigo}'\")\n",
    "\n",
    "if len(df_normalizados_ej) > 20:\n",
    "    print(f\"  ... y {len(df_normalizados_ej) - 20} municipios m√°s\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 11: REEMPLAZAR COLUMNA ORIGINAL\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Reemplazando columna original con c√≥digos DANE...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_pivot['municipio_SIE'] = df_pivot['municipio_SIE_codigo']\n",
    "\n",
    "df_pivot.drop(columns=['municipio_SIE_codigo'], inplace=True)\n",
    "\n",
    "print(f\"  - Columna 'municipio_SIE' actualizada con c√≥digos DANE\")\n",
    "print(f\"  - Columna temporal eliminada\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 12: AN√ÅLISIS FINAL DE DATOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ESTAD√çSTICAS DE 'municipio_SIE' DESPU√âS DE NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì ESTAD√çSTICAS FINALES:\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - Valores no nulos (c√≥digos DANE): {df_pivot['municipio_SIE'].notna().sum():,}\")\n",
    "print(f\"  - Valores nulos/vac√≠os: {df_pivot['municipio_SIE'].isna().sum():,}\")\n",
    "print(f\"  - Valores √∫nicos: {df_pivot['municipio_SIE'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 13: TABLA RESUMEN DE NORMALIZACI√ìN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã RESUMEN DE LA NORMALIZACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resumen_normalizacion = pd.DataFrame({\n",
    "    'Concepto': [\n",
    "        'Registros normalizados exitosamente',\n",
    "        'Registros sin mapeo (enviados a auditor√≠a)',\n",
    "        'Registros vac√≠os/sin valor original',\n",
    "        'Total de registros procesados'\n",
    "    ],\n",
    "    'Cantidad': [\n",
    "        registros_normalizados,\n",
    "        registros_sin_mapeo,\n",
    "        registros_vacios,\n",
    "        len(df_pivot)\n",
    "    ],\n",
    "    'Porcentaje': [\n",
    "        f\"{(registros_normalizados/len(df_pivot)*100):.2f}%\",\n",
    "        f\"{(registros_sin_mapeo/len(df_pivot)*100):.2f}%\",\n",
    "        f\"{(registros_vacios/len(df_pivot)*100):.2f}%\",\n",
    "        \"100.00%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + resumen_normalizacion.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 14: DISTRIBUCI√ìN DE MUNICIPIOS NORMALIZADOS (USA DICCIONARIO GUARDADO)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üó∫Ô∏è  DISTRIBUCI√ìN DE MUNICIPIOS NORMALIZADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "distribucion_municipios = df_pivot['municipio_SIE'].value_counts(dropna=True).head(20)\n",
    "\n",
    "print(f\"\\nTop 20 municipios normalizados:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, (codigo, cantidad) in enumerate(distribucion_municipios.items(), 1):\n",
    "    # ‚úÖ CONVERTIR C√ìDIGO A STRING Y LIMPIAR\n",
    "    codigo_str = str(int(float(codigo))) if pd.notna(codigo) else \"N/A\"\n",
    "    \n",
    "    # ‚úÖ USAR DICCIONARIO GUARDADO (no df_municipios_sie)\n",
    "    nombre = diccionario_nombres_municipios.get(codigo_str, \"N/A\")\n",
    "    porcentaje = (cantidad / df_pivot['municipio_SIE'].notna().sum()) * 100\n",
    "    \n",
    "    # ‚úÖ ASEGURAR QUE nombre ES STRING\n",
    "    nombre_str = str(nombre) if nombre != \"N/A\" else \"N/A\"\n",
    "    \n",
    "    print(f\"  {idx:2d}. C√≥digo {codigo_str} ({nombre_str:40s}): {cantidad:10,d} ({porcentaje:6.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 15: LIMPIAR MEMORIA - ELIMINAR df_municipios_sie\n",
    "# ============================================================================\n",
    "print(\"\\n‚úì Limpiando memoria...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "tama√±o_antes = len(df_municipios_sie)\n",
    "\n",
    "del df_municipios_sie\n",
    "\n",
    "print(f\"  - DataFrame 'df_municipios_sie' eliminado ({tama√±o_antes:,} registros liberados)\")\n",
    "print(f\"  - Memoria liberada exitosamente\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 16: MUESTRA DE DATOS FINALES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS FINALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "columnas_muestra = [\n",
    "    'AFL_ID',\n",
    "    'NUMERO_IDENTIFICACION',\n",
    "    'municipio_SIE',\n",
    "    'MUNICIPIO',\n",
    "    'estado_SIE'\n",
    "]\n",
    "\n",
    "columnas_muestra_existentes = [col for col in columnas_muestra if col in df_pivot.columns]\n",
    "\n",
    "print(f\"\\nPrimeros 15 registros:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_pivot[columnas_muestra_existentes].head(15).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 17: ESTAD√çSTICAS DE AUDITOR√çA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ESTAD√çSTICAS DE AUDITOR√çA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì INFORMACI√ìN DE df_auditoria:\")\n",
    "print(f\"  - Total de registros: {len(df_auditoria):,}\")\n",
    "print(f\"  - Registros por municipio_SIE no encontrado: {registros_sin_mapeo:,}\")\n",
    "\n",
    "if 'Descripci√≥n' in df_auditoria.columns:\n",
    "    conteo_descripciones = df_auditoria['Descripci√≥n'].value_counts()\n",
    "    print(f\"\\n  Descripci√≥n de errores registrados:\")\n",
    "    for descripcion, cantidad in conteo_descripciones.items():\n",
    "        print(f\"    - '{descripcion}': {cantidad:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 18: VERIFICACI√ìN DE INTEGRIDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VERIFICACI√ìN DE INTEGRIDAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_verificacion = registros_normalizados + registros_sin_mapeo + registros_vacios\n",
    "\n",
    "print(f\"\\n‚úì Verificaci√≥n de conteos:\")\n",
    "print(f\"  - Suma de categor√≠as: {total_verificacion:,}\")\n",
    "print(f\"  - Total de registros: {len(df_pivot):,}\")\n",
    "print(f\"  - ¬øCoinciden?: {total_verificacion == len(df_pivot)}\")\n",
    "\n",
    "if total_verificacion == len(df_pivot):\n",
    "    print(f\"\\n‚úÖ Integridad verificada correctamente\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Hay discrepancia en los conteos\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 19: RESUMEN FINAL\n",
    "# ============================================================================\n",
    "tiempo_final = time.time()\n",
    "tiempo_transcurrido = tiempo_final - tiempo_inicio\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ NORMALIZACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMEN FINAL:\")\n",
    "print(f\"  ‚úì Columna normalizada: 'municipio_SIE'\")\n",
    "print(f\"  ‚úì Tipo de normalizaci√≥n: Descripci√≥n ‚Üí C√≥digo DANE (5 d√≠gitos)\")\n",
    "print(f\"  ‚úì Registros procesados exitosamente: {registros_normalizados:,}\")\n",
    "print(f\"  ‚úó Registros no encontrados y auditados: {registros_sin_mapeo:,}\")\n",
    "print(f\"  ‚àÖ Registros vac√≠os preservados: {registros_vacios:,}\")\n",
    "print(f\"  ‚úì Municipios √∫nicos normalizados: {df_pivot['municipio_SIE'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüîß OPERACIONES REALIZADAS:\")\n",
    "print(f\"  ‚úì B√∫squeda de nombres en cat√°logo DANE\")\n",
    "print(f\"  ‚úì Mapeo a c√≥digos de 5 d√≠gitos\")\n",
    "print(f\"  ‚úì Validaci√≥n de c√≥digos DANE\")\n",
    "print(f\"  ‚úì Registro de errores en auditor√≠a\")\n",
    "print(f\"  ‚úì Limpieza de memoria (df_municipios_sie eliminado)\")\n",
    "\n",
    "print(f\"\\nüíæ ESTADO DE DataFrames:\")\n",
    "print(f\"  - df_pivot: {len(df_pivot):,} registros\")\n",
    "print(f\"  - df_auditoria: {len(df_auditoria):,} registros\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  TIEMPO TRANSCURRIDO: {tiempo_transcurrido:.4f} segundos\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Comparar red de servicios SIE vs Contratada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VALIDANDO COLUMNAS DISPONIBLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar qu√© columnas existen en df_pivot\n",
    "print(\"\\n‚úì Primeras columnas de df_pivot:\")\n",
    "print(df_pivot.columns[:20].tolist())\n",
    "\n",
    "# Identificar la columna correcta para EPS\n",
    "columna_eps = None\n",
    "if 'ABREVIATURA' in df_pivot.columns:\n",
    "    columna_eps = 'ABREVIATURA'\n",
    "elif 'abreviatura' in df_pivot.columns:\n",
    "    columna_eps = 'abreviatura'\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: No se encontr√≥ columna de EPS/abreviatura\")\n",
    "    print(f\"Columnas disponibles: {df_pivot.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n‚úì Columna EPS identificada: {columna_eps}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PREPARAR LOS DATOS\n",
    "# ============================================================================\n",
    "mapeo_servicios = {\n",
    "    'MEDICINA GENERAL': 'MEDICINA GENERAL',\n",
    "    'LABORATORIO CLINICO': 'LABORATORIO',\n",
    "    'ODONTOLOGIA GENERAL': 'ODONTOLOGIA GENERAL',\n",
    "    'MEDICAMENTOS': 'MEDICAMENTOS',\n",
    "    'OPTOMETRIA': 'OPTOMETRIA',\n",
    "    'IMAGENES DIAGNOSTICAS - IONIZANTES': 'IMAGENES DIAGNOSTICAS - IONIZANTES',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# 2. NORMALIZAR NOMBRES DE SERVICIOS EN df_pivot\n",
    "# ============================================================================\n",
    "servicios_pivot = ['MEDICINA GENERAL', 'LABORATORIO CLINICO', 'ODONTOLOGIA GENERAL', \n",
    "                    'MEDICAMENTOS', 'OPTOMETRIA', 'IMAGENES DIAGNOSTICAS - IONIZANTES']\n",
    "\n",
    "# Filtrar solo servicios que existan en df_pivot\n",
    "servicios_validos = [s for s in servicios_pivot if s in df_pivot.columns]\n",
    "\n",
    "print(f\"\\n‚úì Servicios disponibles para validar: {len(servicios_validos)}\")\n",
    "print(f\"  Servicios: {servicios_validos}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREAR UN LOOKUP DE RED CONTRATADA POR (MUNICIPIO, SERVICIO)\n",
    "# ============================================================================\n",
    "red_lookup = {}\n",
    "for idx, row in df_red_contratada.iterrows():\n",
    "    municipio = str(row['MUNICIPIO']).strip()\n",
    "    servicio = str(row['NOMBRE SERVICIO']).strip()\n",
    "    nit = str(row['NIT']).strip()\n",
    "    \n",
    "    key = (municipio, servicio)\n",
    "    if key not in red_lookup:\n",
    "        red_lookup[key] = {\n",
    "            'NIT': nit,\n",
    "            'CODIGO_SERVICIO': row['CODIGO SERVICIO'],\n",
    "            'ID': row['ID'],\n",
    "            'NOMBRE_IPS': row['NOMBRE IPS']\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úì Red contratada cargada: {len(red_lookup)} pares municipio-servicio\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VALIDAR Y COMPARAR\n",
    "# ============================================================================\n",
    "def validar_red_asignada(row, municipio_col, eps_col, servicios_a_validar):\n",
    "    \"\"\"\n",
    "    Valida si la red asignada en SIE coincide con la contratada.\n",
    "    Retorna: 'OK', 'VACIO', 'INCORRECTO', o 'SIN_CONTRATO'\n",
    "    \"\"\"\n",
    "    municipio = str(row[municipio_col]).strip()\n",
    "    \n",
    "    validaciones = {}\n",
    "    \n",
    "    for servicio in servicios_a_validar:\n",
    "        nit_asignado = str(row[servicio]).strip() if pd.notna(row[servicio]) else None\n",
    "        \n",
    "        key = (municipio, servicio)\n",
    "        \n",
    "        if key in red_lookup:\n",
    "            nit_contratado = red_lookup[key]['NIT']\n",
    "            \n",
    "            if nit_asignado is None or nit_asignado == '':\n",
    "                validaciones[servicio] = {\n",
    "                    'estado': 'VACIO',\n",
    "                    'asignado': None,\n",
    "                    'contratado': nit_contratado,\n",
    "                    'deberia_ser': nit_contratado\n",
    "                }\n",
    "            elif nit_asignado == nit_contratado:\n",
    "                validaciones[servicio] = {\n",
    "                    'estado': 'OK',\n",
    "                    'asignado': nit_asignado,\n",
    "                    'contratado': nit_contratado\n",
    "                }\n",
    "            else:\n",
    "                validaciones[servicio] = {\n",
    "                    'estado': 'INCORRECTO',\n",
    "                    'asignado': nit_asignado,\n",
    "                    'contratado': nit_contratado,\n",
    "                    'deberia_ser': nit_contratado\n",
    "                }\n",
    "        else:\n",
    "            validaciones[servicio] = {\n",
    "                'estado': 'SIN_CONTRATO',\n",
    "                'asignado': nit_asignado,\n",
    "                'contratado': None\n",
    "            }\n",
    "    \n",
    "    return validaciones\n",
    "\n",
    "# ============================================================================\n",
    "# 5. APLICAR VALIDACI√ìN\n",
    "# ============================================================================\n",
    "print(f\"\\n‚úì Aplicando validaci√≥n a {len(df_pivot):,} registros...\")\n",
    "\n",
    "df_pivot['validacion_red'] = df_pivot.apply(\n",
    "    lambda row: validar_red_asignada(row, 'municipio_SIE', columna_eps, servicios_validos), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"‚úì Validaci√≥n completada\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EXPANDIR RESULTADOS PARA AN√ÅLISIS\n",
    "# ============================================================================\n",
    "def expandir_validacion(row, eps_col):\n",
    "    \"\"\"Convierte el dict de validaci√≥n en filas expandidas\"\"\"\n",
    "    municipio = row['municipio_SIE']\n",
    "    eps = row[eps_col]\n",
    "    numero_id = row['NUMERO_IDENTIFICACION']\n",
    "    \n",
    "    validaciones = row['validacion_red']\n",
    "    \n",
    "    resultados = []\n",
    "    for servicio, val in validaciones.items():\n",
    "        resultados.append({\n",
    "            'municipio': municipio,\n",
    "            'eps': eps,\n",
    "            'numero_id': numero_id,\n",
    "            'servicio': servicio,\n",
    "            'estado': val['estado'],\n",
    "            'nit_asignado': val.get('asignado'),\n",
    "            'nit_contratado': val.get('contratado'),\n",
    "            'deberia_ser': val.get('deberia_ser')\n",
    "        })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CREAR DATAFRAME EXPANDIDO\n",
    "# ============================================================================\n",
    "print(f\"\\n‚úì Expandiendo resultados...\")\n",
    "\n",
    "resultados_expandidos = []\n",
    "for idx, row in df_pivot.iterrows():\n",
    "    resultados_expandidos.extend(expandir_validacion(row, columna_eps))\n",
    "\n",
    "df_validacion = pd.DataFrame(resultados_expandidos)\n",
    "\n",
    "print(f\"‚úì DataFrame de validaci√≥n creado: {len(df_validacion):,} registros\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. REPORTE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE VALIDACI√ìN DE RED\")\n",
    "print(\"=\"*80)\n",
    "print(df_validacion['estado'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"SERVICIOS CON PROBLEMAS POR MUNICIPIO:\")\n",
    "problemas = df_validacion[df_validacion['estado'].isin(['VACIO', 'INCORRECTO'])]\n",
    "resumen_problemas = problemas.groupby(['municipio', 'servicio', 'estado']).size().reset_index(name='cantidad')\n",
    "print(resumen_problemas.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"EJEMPLO DE REGISTROS CON RED INCORRECTA:\")\n",
    "incorrectos = df_validacion[df_validacion['estado'] == 'INCORRECTO'].head(10)\n",
    "print(incorrectos[['municipio', 'servicio', 'nit_asignado', 'nit_contratado']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VALIDACI√ìN COMPLETADA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pivot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_red_contratada['NOMBRE SERVICIO'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red_contratada.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "# Guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from openpyxl.styles import PatternFill, Font, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ GUARDANDO M√öLTIPLES DATAFRAMES EN EXCEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: PREPARAR NOMBRE DE ARCHIVO Y RUTA\n",
    "# ============================================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "nombre_archivo = f\"Red_Asignada_Consolidada_{timestamp}.xlsx\"\n",
    "ruta_completa = os.path.join(R_Salida, nombre_archivo)\n",
    "\n",
    "print(f\"\\n‚úì Archivo a crear: {nombre_archivo}\")\n",
    "print(f\"‚úì Ruta completa: {ruta_completa}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: CREAR ARCHIVO EXCEL CON M√öLTIPLES HOJAS\n",
    "# ============================================================================\n",
    "try:\n",
    "    # Crear ExcelWriter (gestor de contexto para m√∫ltiples hojas)\n",
    "    with pd.ExcelWriter(ruta_completa, engine='openpyxl') as writer:\n",
    "        \n",
    "        # ====================================================================\n",
    "        # HOJA 1: Red consolidada (df_pivot)\n",
    "        # ====================================================================\n",
    "        print(\"\\n‚úì Escribiendo hoja 1: 'Red_Consolidada'\")\n",
    "        df_pivot.to_excel(\n",
    "            writer, \n",
    "            sheet_name='Red_Consolidada',\n",
    "            index=False,\n",
    "            startrow=0,\n",
    "            startcol=0\n",
    "        )\n",
    "        \n",
    "        registros_pivot = len(df_pivot)\n",
    "        columnas_pivot = len(df_pivot.columns)\n",
    "        print(f\"  - Registros: {registros_pivot:,}\")\n",
    "        print(f\"  - Columnas: {columnas_pivot}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # HOJA 2: Auditor√≠a (df_auditoria)\n",
    "        # ====================================================================\n",
    "        print(\"\\n‚úì Escribiendo hoja 2: 'Auditoria'\")\n",
    "        df_auditoria.to_excel(\n",
    "            writer,\n",
    "            sheet_name='Auditoria',\n",
    "            index=False,\n",
    "            startrow=0,\n",
    "            startcol=0\n",
    "        )\n",
    "        \n",
    "        registros_auditoria = len(df_auditoria)\n",
    "        columnas_auditoria = len(df_auditoria.columns)\n",
    "        print(f\"  - Registros: {registros_auditoria:,}\")\n",
    "        print(f\"  - Columnas: {columnas_auditoria}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # HOJA 3: Resumen de auditor√≠a SIE (opcional pero recomendado)\n",
    "        # ====================================================================\n",
    "        # Crear resumen de tipos de auditor√≠a\n",
    "        df_resumen_auditoria = pd.DataFrame({\n",
    "            'Tipo_Auditoria': df_auditoria['Descripci√≥n'].value_counts().index,\n",
    "            'Cantidad_Registros': df_auditoria['Descripci√≥n'].value_counts().values,\n",
    "            'Porcentaje': (df_auditoria['Descripci√≥n'].value_counts().values / len(df_auditoria) * 100).round(2)\n",
    "        }).reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\n‚úì Escribiendo hoja 3: 'Resumen_Auditoria'\")\n",
    "        df_resumen_auditoria.to_excel(\n",
    "            writer,\n",
    "            sheet_name='Resumen_Auditoria',\n",
    "            index=False,\n",
    "            startrow=0,\n",
    "            startcol=0\n",
    "        )\n",
    "        \n",
    "        print(f\"  - Tipos de auditor√≠a identificados: {len(df_resumen_auditoria)}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # HOJA 4: Estad√≠sticas Generales (opcional pero muy √∫til)\n",
    "        # ====================================================================\n",
    "        # Crear hoja de estad√≠sticas\n",
    "        df_estadisticas = pd.DataFrame({\n",
    "            'Concepto': [\n",
    "                'Total de registros en Red Consolidada',\n",
    "                'Total de registros en Auditor√≠a',\n",
    "                'Afiliados con servicios asignados',\n",
    "                'Afiliados sin servicios',\n",
    "                'Porcentaje con servicios',\n",
    "                'Porcentaje sin servicios',\n",
    "                'Total de columnas de servicios',\n",
    "                'Total de columnas en Red Consolidada',\n",
    "                'Fecha de generaci√≥n',\n",
    "                'Nombre del archivo'\n",
    "            ],\n",
    "            'Valor': [\n",
    "                registros_pivot,\n",
    "                registros_auditoria,\n",
    "                registros_pivot - registros_auditoria,\n",
    "                registros_auditoria,\n",
    "                f\"{((registros_pivot - registros_auditoria) / registros_pivot * 100):.2f}%\",\n",
    "                f\"{(registros_auditoria / registros_pivot * 100):.2f}%\",\n",
    "                columnas_pivot - 3,  # Menos las columnas de identificaci√≥n\n",
    "                columnas_pivot,\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                nombre_archivo\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(\"\\n‚úì Escribiendo hoja 4: 'Estadisticas'\")\n",
    "        df_estadisticas.to_excel(\n",
    "            writer,\n",
    "            sheet_name='Estadisticas',\n",
    "            index=False,\n",
    "            startrow=0,\n",
    "            startcol=0\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # OBTENER WORKBOOK PARA FORMATO (OPCIONAL)\n",
    "        # ====================================================================\n",
    "        workbook = writer.book\n",
    "        \n",
    "        # Formato para hoja de Estad√≠sticas\n",
    "        ws_stats = writer.sheets['Estadisticas']\n",
    "        fill_header = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "        font_header = Font(bold=True, color=\"FFFFFF\", size=11)\n",
    "        \n",
    "        for cell in ws_stats[1]:\n",
    "            if cell.value:\n",
    "                cell.fill = fill_header\n",
    "                cell.font = font_header\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        \n",
    "        # Ajustar ancho de columnas en Estad√≠sticas\n",
    "        ws_stats.column_dimensions['A'].width = 45\n",
    "        ws_stats.column_dimensions['B'].width = 35\n",
    "        \n",
    "        # Formato para hoja de Resumen de Auditor√≠a\n",
    "        ws_resumen = writer.sheets['Resumen_Auditoria']\n",
    "        for cell in ws_resumen[1]:\n",
    "            if cell.value:\n",
    "                cell.fill = fill_header\n",
    "                cell.font = font_header\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        \n",
    "        ws_resumen.column_dimensions['A'].width = 40\n",
    "        ws_resumen.column_dimensions['B'].width = 25\n",
    "        ws_resumen.column_dimensions['C'].width = 20\n",
    "        \n",
    "        # Formato para hoja de Auditor√≠a\n",
    "        ws_auditoria = writer.sheets['Auditoria']\n",
    "        for cell in ws_auditoria[1]:\n",
    "            if cell.value:\n",
    "                cell.fill = fill_header\n",
    "                cell.font = font_header\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        \n",
    "        ws_auditoria.column_dimensions['A'].width = 15\n",
    "        ws_auditoria.column_dimensions['B'].width = 25\n",
    "        ws_auditoria.column_dimensions['C'].width = 60\n",
    "        \n",
    "        # Formato para hoja de Red Consolidada\n",
    "        ws_pivot = writer.sheets['Red_Consolidada']\n",
    "        for cell in ws_pivot[1]:\n",
    "            if cell.value:\n",
    "                cell.fill = fill_header\n",
    "                cell.font = font_header\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        \n",
    "        # Ajustar ancho de primeras columnas de identificaci√≥n\n",
    "        ws_pivot.column_dimensions['A'].width = 12\n",
    "        ws_pivot.column_dimensions['B'].width = 25\n",
    "        ws_pivot.column_dimensions['C'].width = 30\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ ARCHIVO GUARDADO EXITOSAMENTE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Ruta: {ruta_completa}\")\n",
    "    print(f\"üíæ Tama√±o del archivo: {os.path.getsize(ruta_completa) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    print(\"\\nüìã HOJAS CREADAS:\")\n",
    "    print(\"  1Ô∏è‚É£  'Red_Consolidada'    ‚Üí Datos principales con servicios asignados\")\n",
    "    print(\"  2Ô∏è‚É£  'Auditoria'          ‚Üí Registros sin servicios y sus razones\")\n",
    "    print(\"  3Ô∏è‚É£  'Resumen_Auditoria'  ‚Üí Resumen estad√≠stico de auditor√≠a\")\n",
    "    print(\"  4Ô∏è‚É£  'Estadisticas'       ‚Üí Estad√≠sticas generales del proceso\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä RESUMEN DE CONTENIDO:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úì Red Consolidada:\")\n",
    "    print(f\"  - Registros: {registros_pivot:,}\")\n",
    "    print(f\"  - Columnas: {columnas_pivot}\")\n",
    "    print(f\"\\n‚úì Auditor√≠a:\")\n",
    "    print(f\"  - Registros totales: {registros_auditoria:,}\")\n",
    "    print(f\"  - Tipos de auditor√≠a: {len(df_resumen_auditoria)}\")\n",
    "    \n",
    "    print(\"\\nüìà Desglose de auditor√≠a:\")\n",
    "    for _, row in df_resumen_auditoria.iterrows():\n",
    "        print(f\"  - {row['Tipo_Auditoria']:40s}: {row['Cantidad_Registros']:7,d} ({row['Porcentaje']:5.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå ERROR: No se puede acceder a la ruta\")\n",
    "    print(f\"Ruta: {R_Salida}\")\n",
    "    print(f\"Por favor, verifica que la carpeta exista\")\n",
    "    \n",
    "except PermissionError:\n",
    "    print(f\"\\n‚ùå ERROR: Permiso denegado\")\n",
    "    print(f\"El archivo podr√≠a estar abierto en Excel\")\n",
    "    print(f\"Por favor, cierra el archivo e intenta nuevamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR AL GUARDAR EL ARCHIVO\")\n",
    "    print(f\"Tipo de error: {type(e).__name__}\")\n",
    "    print(f\"Mensaje: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nDetalles completos:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
