{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9ea159",
   "metadata": {},
   "source": [
    "# 1. MODULO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64515fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa el módulo 'os' para interactuar con el sistema de archivos y rutas del sistema operativo\n",
    "import os\n",
    "\n",
    "# Importa 'pandas' como 'pd', una biblioteca potente para manipulación y análisis de datos mediante DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Importa el módulo 'datetime' para trabajar con fechas y horas de manera eficiente\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85aa25",
   "metadata": {},
   "source": [
    "# 2. Unificación anual maximo y minimos maestros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43056b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir las rutas de entrada y salida\n",
    "R_Maestros = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\"\n",
    "R_SALIDA_ANUAL = os.path.join(R_Maestros, \"All\")\n",
    "\n",
    "# Asegurar que las carpetas de salida existen\n",
    "os.makedirs(os.path.join(R_SALIDA_ANUAL, \"Max\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(R_SALIDA_ANUAL, \"Min\"), exist_ok=True)\n",
    "\n",
    "# Lista de encabezados proporcionados (43 columnas)\n",
    "encabezados = [\n",
    "    \"AFL_ID\", \"ENT_ID\", \"TPS_IDN_ID_CF\", \"HST_IDN_NUMERO_IDENTIFICACION_CF\", \"TPS_IDN_ID\",\n",
    "    \"HST_IDN_NUMERO_IDENTIFICACION\", \"AFL_PRIMER_APELLIDO\", \"AFL_SEGUNDO_APELLIDO\",\n",
    "    \"AFL_PRIMER_NOMBRE\", \"AFL_SEGUNDO_NOMBRE\", \"AFL_FECHA_NACIMIENTO\", \"TPS_GNR_ID\",\n",
    "    \"AFL_PAIS_NACIMIENTO\", \"AFL_MUNICIPIO_NACIMIENTO\", \"AFL_NACIONALIDAD\",\n",
    "    \"AFL_SEXO_IDENTIFICACION\", \"AFL_DISCAPACIDAD\", \"TPS_AFL_ID\", \"TPS_PRN_ID\",\n",
    "    \"TPS_GRP_PBL_ID\", \"TPS_NVL_SSB_ID\", \"NUMEROFICHASISBEN\", \"TPS_CND_BNF_ID\", \"DPR_ID\",\n",
    "    \"MNC_ID\", \"ZNS_ID\", \"AFL_FECHA_AFILIACION_SGSSS\", \"AFC_FECHA_INICIO\", \"NUMERO CONTRATO\",\n",
    "    \"FECHADE INICIO DEL CONTRATO\", \"CNT_AFL_TPS_GRP_PBL_ID\", \"CNT_AFL_TPS_PRT_ETN_ID\",\n",
    "    \"TPS_MDL_SBS_ID\", \"TPS_EST_AFL_ID\", \"CND_AFL_FECHA_INICIO\", \"CND_AFL_FECHA_INICIO_2\",\n",
    "    \"GRP_FML_COTIZANTE_ID\", \"PORTABILIDAD\", \"COD_IPS_P\", \"MTDLG_G_P\", \"SUB_SISBEN_IV\",\n",
    "    \"MARCASISBENIV+MARCASISBENIII\", \"CRUCE_BDEX_RNEC\",\n",
    "]\n",
    "\n",
    "def get_maestro_date(file_name):\n",
    "    \"\"\"Extrae la fecha del nombre del archivo y la devuelve como objeto datetime.\"\"\"\n",
    "    try:\n",
    "        idx = file_name.upper().find('EPS025MS00')\n",
    "        if idx != -1:\n",
    "            fecha_str = file_name[idx + 10:idx + 18]\n",
    "            return datetime.strptime(fecha_str, '%d%m%Y')\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def pad_dataframe(df, target_cols):\n",
    "    \"\"\"Ajusta las columnas del DataFrame a la estructura de 43 columnas.\"\"\"\n",
    "    num_cols = df.shape[1]\n",
    "    if num_cols == 31:\n",
    "        for i in range(5): df.insert(12, f'col_vacia_{12+i}', '')\n",
    "        for i in range(7): df[f'col_vacia_end_{i}'] = ''\n",
    "    elif num_cols == 37:\n",
    "        for i in range(5): df.insert(12, f'col_vacia_{12+i}', '')\n",
    "        df['col_vacia_end_0'] = ''\n",
    "    elif num_cols == 38:\n",
    "        for i in range(5): df.insert(12, f'col_vacia_{12+i}', '')\n",
    "    \n",
    "    if df.shape[1] == len(target_cols):\n",
    "        df.columns = target_cols\n",
    "    else:\n",
    "        print(f\"Error: El DataFrame resultante no tiene 43 columnas. Tiene: {df.shape[1]}\")\n",
    "    return df\n",
    "\n",
    "# Recorrer cada subdirectorio de año\n",
    "for dir_name in os.listdir(R_Maestros):\n",
    "    year_path = os.path.join(R_Maestros, dir_name)\n",
    "    if os.path.isdir(year_path) and dir_name != \"All\":\n",
    "        print(f\"--- Procesando carpeta: {dir_name} ---\")\n",
    "        \n",
    "        monthly_files = {}\n",
    "        for root, _, files in os.walk(year_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.txt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_date = get_maestro_date(file)\n",
    "                    \n",
    "                    if file_date:\n",
    "                        month_year = file_date.strftime('%Y-%m')\n",
    "                        if month_year not in monthly_files:\n",
    "                            monthly_files[month_year] = []\n",
    "                        monthly_files[month_year].append({'path': file_path, 'date': file_date, 'name': file})\n",
    "\n",
    "        if not monthly_files:\n",
    "            print(\"No se encontraron archivos válidos. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        output_min_path = os.path.join(R_SALIDA_ANUAL, \"Min\", f\"{dir_name}_Min.txt\")\n",
    "        output_max_path = os.path.join(R_SALIDA_ANUAL, \"Max\", f\"{dir_name}_Max.txt\")\n",
    "\n",
    "        # Proceso optimizado: escribir directamente a los archivos de salida\n",
    "        first_min_file = True\n",
    "        first_max_file = True\n",
    "\n",
    "        for month_year in sorted(monthly_files.keys()):\n",
    "            files_list = sorted(monthly_files[month_year], key=lambda x: x['date'])\n",
    "            \n",
    "            # Procesar archivo mínimo\n",
    "            min_file_info = files_list[0]\n",
    "            df_min = pd.read_csv(min_file_info['path'], sep=\",\", header=None, encoding='latin1', dtype=str)\n",
    "            df_min = pad_dataframe(df_min, encabezados)\n",
    "            df_min['nombre_archivo'] = min_file_info['name']\n",
    "            df_min['fecha_maestro'] = min_file_info['date'].strftime('%d/%m/%Y')\n",
    "            \n",
    "            # Escribir al archivo de salida\n",
    "            df_min.to_csv(output_min_path, sep=\",\", index=False, encoding='latin1', header=first_min_file, mode='a')\n",
    "            if first_min_file: first_min_file = False\n",
    "\n",
    "            # Procesar archivo máximo\n",
    "            max_file_info = files_list[-1]\n",
    "            df_max = pd.read_csv(max_file_info['path'], sep=\",\", header=None, encoding='latin1', dtype=str)\n",
    "            df_max = pad_dataframe(df_max, encabezados)\n",
    "            df_max['nombre_archivo'] = max_file_info['name']\n",
    "            df_max['fecha_maestro'] = max_file_info['date'].strftime('%d/%m/%Y')\n",
    "            \n",
    "            # Escribir al archivo de salida\n",
    "            df_max.to_csv(output_max_path, sep=\",\", index=False, encoding='latin1', header=first_max_file, mode='a')\n",
    "            if first_max_file: first_max_file = False\n",
    "            \n",
    "        print(f\"Archivos consolidados para {dir_name} guardados en 'All/Min' y 'All/Max'.\")\n",
    "\n",
    "print(\"\\nProceso de consolidación anual finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1067fa9",
   "metadata": {},
   "source": [
    "# 3. validar maximos y minimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3370ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir las rutas de entrada y salida\n",
    "R_Maestros = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\"\n",
    "ruta_maximos_anual = os.path.join(R_Maestros, \"All\", \"Max\")\n",
    "ruta_minimos_anual = os.path.join(R_Maestros, \"All\", \"Min\")\n",
    "ruta_salida = os.path.join(R_Maestros, \"All\")\n",
    "\n",
    "def validar_y_corregir_optimo(ruta_directorio, es_maximo):\n",
    "    \"\"\"\n",
    "    Valida y corrige los archivos anuales de manera eficiente en dos pases.\n",
    "    \"\"\"\n",
    "    grupo_nombre = \"Máximos\" if es_maximo else \"Mínimos\"\n",
    "    print(f\"--- Iniciando validación y corrección optimizada de {grupo_nombre} ---\")\n",
    "\n",
    "    # --- Primer Pase: Consolidar solo metadatos ---\n",
    "    print(\"Paso 1: Consolidando metadatos (fechas y nombres de archivo)...\")\n",
    "    dfs_metadata = []\n",
    "    if not os.path.exists(ruta_directorio):\n",
    "        print(f\"Advertencia: La ruta no existe: {ruta_directorio}\")\n",
    "        return\n",
    "        \n",
    "    archivos = [f for f in os.listdir(ruta_directorio) if f.endswith('.txt')]\n",
    "    for file_name in archivos:\n",
    "        file_path = os.path.join(ruta_directorio, file_name)\n",
    "        try:\n",
    "            # Leer solo las columnas de interés\n",
    "            df_temp = pd.read_csv(file_path, sep=\",\", encoding='latin1', usecols=['fecha_maestro', 'nombre_archivo'], dtype=str)\n",
    "            df_temp['año_consolidado'] = file_name.split('_')[0]\n",
    "            dfs_metadata.append(df_temp)\n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia: Error al leer metadatos de {file_name}: {e}\")\n",
    "    \n",
    "    if not dfs_metadata:\n",
    "        print(f\"No se encontraron metadatos para el grupo {grupo_nombre}.\")\n",
    "        return\n",
    "    \n",
    "    metadata_df = pd.concat(dfs_metadata, ignore_index=True)\n",
    "    \n",
    "    # --- CORRECCIÓN DEL ERROR ---\n",
    "    # Eliminar las filas donde el valor de la fecha es el encabezado literal\n",
    "    metadata_df = metadata_df[metadata_df['fecha_maestro'] != 'fecha_maestro']\n",
    "    \n",
    "    # Ahora sí, convertir a formato de fecha\n",
    "    metadata_df['fecha_maestro'] = pd.to_datetime(metadata_df['fecha_maestro'], format='%d/%m/%Y')\n",
    "    metadata_df['mes_año'] = metadata_df['fecha_maestro'].dt.to_period('M')\n",
    "\n",
    "    # --- Validación: Identificar los maestros correctos ---\n",
    "    print(\"Paso 2: Identificando el maestro correcto para cada mes...\")\n",
    "    if es_maximo:\n",
    "        idx_to_keep = metadata_df.loc[metadata_df.groupby('mes_año')['fecha_maestro'].idxmax()].index\n",
    "    else:\n",
    "        idx_to_keep = metadata_df.loc[metadata_df.groupby('mes_año')['fecha_maestro'].idxmin()].index\n",
    "\n",
    "    maestros_correctos = set(metadata_df.loc[idx_to_keep, 'nombre_archivo'])\n",
    "\n",
    "    # --- Segundo Pase: Corregir y sobrescribir archivos uno por uno ---\n",
    "    print(\"Paso 3: Sobrescribiendo archivos con los registros correctos...\")\n",
    "    \n",
    "    df_validado = metadata_df.loc[idx_to_keep].copy()\n",
    "    \n",
    "    for file_name in archivos:\n",
    "        file_path = os.path.join(ruta_directorio, file_name)\n",
    "        try:\n",
    "            df_completo = pd.read_csv(file_path, sep=\",\", encoding='latin1', dtype=str)\n",
    "            df_filtrado = df_completo[df_completo['nombre_archivo'].isin(maestros_correctos)].copy()\n",
    "            df_filtrado.to_csv(file_path, sep=\",\", index=False, encoding='latin1')\n",
    "            print(f\"  - Archivo corregido: {file_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Advertencia: Error al corregir {file_name}: {e}\")\n",
    "\n",
    "    # --- Verificación de la completitud de los meses ---\n",
    "    print(\"\\n--- Verificando la completitud de los meses ---\")\n",
    "    años_consolidados = sorted(df_validado['año_consolidado'].unique())\n",
    "    año_actual = datetime.now().year\n",
    "    mes_actual = datetime.now().month\n",
    "\n",
    "    for año_consolidado in años_consolidados:\n",
    "        try:\n",
    "            año_val = int(año_consolidado)\n",
    "        except ValueError:\n",
    "            año_val = int(año_consolidado.split('-')[0])\n",
    "        \n",
    "        meses_disponibles = sorted(df_validado[df_validado['año_consolidado'] == año_consolidado]['mes_año'].dt.month.unique())\n",
    "        \n",
    "        if año_val == año_actual:\n",
    "            meses_esperados = list(range(1, mes_actual + 1))\n",
    "        else:\n",
    "            meses_esperados = list(range(1, 13))\n",
    "\n",
    "        meses_faltantes = [m for m in meses_esperados if m not in meses_disponibles]\n",
    "        \n",
    "        if meses_faltantes:\n",
    "            print(f\"Advertencia para {año_consolidado}: Faltan maestros para los meses: {meses_faltantes}\")\n",
    "        else:\n",
    "            print(f\"Validación exitosa para {año_consolidado}: Se encontraron maestros para todos los meses esperados.\")\n",
    "\n",
    "    print(f\"\\nProceso de validación y corrección de {grupo_nombre} finalizado.\")\n",
    "\n",
    "# Ejecutar el proceso para máximos y mínimos\n",
    "validar_y_corregir_optimo(ruta_maximos_anual, es_maximo=True)\n",
    "validar_y_corregir_optimo(ruta_minimos_anual, es_maximo=False)\n",
    "\n",
    "print(\"\\nTodos los maestros anuales han sido validados, corregidos y están listos para la unificación total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0a6a0",
   "metadata": {},
   "source": [
    "# 4. Unicos Por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea74b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Archivo unificado guardado: 2019_Mínim.txt\n",
      "  - Consolidando archivos para el año 2020...\n",
      "  - Archivo unificado guardado: 2020_Mínim.txt\n",
      "  - Consolidando archivos para el año 2021...\n",
      "  - Archivo unificado guardado: 2021_Mínim.txt\n",
      "  - Consolidando archivos para el año 2022...\n",
      "  - Archivo unificado guardado: 2022_Mínim.txt\n",
      "  - Consolidando archivos para el año 2023...\n",
      "  - Archivo unificado guardado: 2023_Mínim.txt\n",
      "  - Consolidando archivos para el año 2024...\n",
      "  - Archivo unificado guardado: 2024_Mínim.txt\n",
      "  - Consolidando archivos para el año 2025...\n",
      "  - Archivo unificado guardado: 2025_Mínim.txt\n",
      "\n",
      "  - Eliminando archivos originales...\n",
      "--- Proceso de unificación anual de Mínimos finalizado. ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la ruta de la carpeta All, que contiene los subdirectorios Max y Min\n",
    "ruta_base_all = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\\All\"\n",
    "\n",
    "def consolidar_por_año(ruta_directorio):\n",
    "    \"\"\"\n",
    "    Agrupa los archivos por su año base y los consolida en un único archivo por año.\n",
    "    \"\"\"\n",
    "    grupo_nombre = \"Máximos\" if \"Max\" in ruta_directorio else \"Mínimos\"\n",
    "    print(f\"--- Iniciando unificación anual para el grupo de {grupo_nombre} ---\")\n",
    "\n",
    "    if not os.path.exists(ruta_directorio):\n",
    "        print(f\"Advertencia: La ruta no existe: {ruta_directorio}\")\n",
    "        return\n",
    "\n",
    "    # Diccionario para agrupar archivos por su año base\n",
    "    archivos_por_año = {}\n",
    "    archivos_a_eliminar = []\n",
    "\n",
    "    for file_name in os.listdir(ruta_directorio):\n",
    "        if file_name.endswith('.txt'):\n",
    "            year_prefix = file_name.split('_')[0]\n",
    "            # Extraer el año base (ej. '2022' de '2022-01')\n",
    "            base_year = year_prefix.split('-')[0]\n",
    "            \n",
    "            if base_year not in archivos_por_año:\n",
    "                archivos_por_año[base_year] = []\n",
    "            \n",
    "            file_path = os.path.join(ruta_directorio, file_name)\n",
    "            archivos_por_año[base_year].append(file_path)\n",
    "            archivos_a_eliminar.append(file_path)\n",
    "\n",
    "    for base_year, file_paths in archivos_por_año.items():\n",
    "        print(f\"  - Consolidando archivos para el año {base_year}...\")\n",
    "        dfs = []\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=\",\", encoding='latin1', dtype=str)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"    - Advertencia: No se pudo leer {os.path.basename(file_path)}: {e}\")\n",
    "        \n",
    "        if dfs:\n",
    "            df_unificado = pd.concat(dfs, ignore_index=True)\n",
    "            output_file_name = f\"{base_year}_{grupo_nombre.replace('os','')}.txt\"\n",
    "            output_path = os.path.join(ruta_directorio, output_file_name)\n",
    "            \n",
    "            df_unificado.to_csv(output_path, sep=\",\", index=False, encoding='latin1')\n",
    "            print(f\"  - Archivo unificado guardado: {output_file_name}\")\n",
    "\n",
    "    # Eliminar los archivos originales después de la unificación\n",
    "    print(\"\\n  - Eliminando archivos originales...\")\n",
    "    for file_path in archivos_a_eliminar:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"    - Advertencia: No se pudo eliminar {os.path.basename(file_path)}: {e}\")\n",
    "    \n",
    "    print(f\"--- Proceso de unificación anual de {grupo_nombre} finalizado. ---\\n\")\n",
    "\n",
    "# Ejecutar el proceso para ambos grupos\n",
    "consolidar_por_año(os.path.join(ruta_base_all, \"Max\"))\n",
    "consolidar_por_año(os.path.join(ruta_base_all, \"Min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee2be0",
   "metadata": {},
   "source": [
    "# 5. Consolidado Maximo y minimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126c565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Iniciando la unificación de maestros máximos de todos los años...\n",
      "\n",
      "No se encontraron archivos de maestros máximos para unificar.\n",
      "---\n",
      "Iniciando la unificación de maestros mínimos de todos los años...\n",
      "\n",
      "No se encontraron archivos de maestros mínimos para unificar.\n",
      "---\n",
      "Proceso de unificación finalizado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Definir las rutas de entrada y salida\n",
    "R_Maestros = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\Procesos BDUA\\Subsidiados\\Maestro\\MS\"\n",
    "ruta_maximos_anual = os.path.join(R_Maestros, \"All\", \"Max\")\n",
    "ruta_minimos_anual = os.path.join(R_Maestros, \"All\", \"Min\")\n",
    "ruta_salida = R_Maestros\n",
    "\n",
    "# --- Unificación de archivos máximos ---\n",
    "print(\"---\")\n",
    "print(\"Iniciando la unificación de maestros máximos de todos los años...\")\n",
    "\n",
    "archivos_maximos = [f for f in sorted(os.listdir(ruta_maximos_anual)) if f.endswith('.txt')]\n",
    "\n",
    "if archivos_maximos:\n",
    "    output_path_max = os.path.join(ruta_salida, \"MS_Unif_Maximo.txt\")\n",
    "    with open(output_path_max, 'w', encoding='latin1') as outfile:\n",
    "        \n",
    "        # Leer el primer archivo para escribir los encabezados una sola vez\n",
    "        with open(os.path.join(ruta_maximos_anual, archivos_maximos[0]), 'r', encoding='latin1') as infile:\n",
    "            header = infile.readline()\n",
    "            outfile.write(header)\n",
    "\n",
    "        # Recorrer todos los archivos para escribir los datos sin encabezados\n",
    "        for file_name in archivos_maximos:\n",
    "            file_path = os.path.join(ruta_maximos_anual, file_name)\n",
    "            with open(file_path, 'r', encoding='latin1') as infile:\n",
    "                next(infile) # Omitir el encabezado de este archivo\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "    print(\"\\nProceso de máximos completado.\")\n",
    "    print(f\"Archivo unificado guardado en: {output_path_max}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron archivos de maestros máximos para unificar.\")\n",
    "\n",
    "# --- Unificación de archivos mínimos ---\n",
    "print(\"---\")\n",
    "print(\"Iniciando la unificación de maestros mínimos de todos los años...\")\n",
    "\n",
    "archivos_minimos = [f for f in sorted(os.listdir(ruta_minimos_anual)) if f.endswith('.txt')]\n",
    "\n",
    "if archivos_minimos:\n",
    "    output_path_min = os.path.join(ruta_salida, \"MS_Unif_Minimo.txt\")\n",
    "    with open(output_path_min, 'w', encoding='latin1') as outfile:\n",
    "        \n",
    "        # Leer el primer archivo para escribir los encabezados una sola vez\n",
    "        with open(os.path.join(ruta_minimos_anual, archivos_minimos[0]), 'r', encoding='latin1') as infile:\n",
    "            header = infile.readline()\n",
    "            outfile.write(header)\n",
    "\n",
    "        # Recorrer todos los archivos para escribir los datos sin encabezados\n",
    "        for file_name in archivos_minimos:\n",
    "            file_path = os.path.join(ruta_minimos_anual, file_name)\n",
    "            with open(file_path, 'r', encoding='latin1') as infile:\n",
    "                next(infile) # Omitir el encabezado de este archivo\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "    print(\"\\nProceso de mínimos completado.\")\n",
    "    print(f\"Archivo unificado guardado en: {output_path_min}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron archivos de maestros mínimos para unificar.\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Proceso de unificación finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a28e06",
   "metadata": {},
   "source": [
    "# **5. Canvas de Proyectos de Data Science con Datos de Afiliados de EPS**\n",
    "\n",
    "---\n",
    "\n",
    "### **Sección 1: Análisis Descriptivo y de Calidad de Datos**\n",
    "#### **Objetivo:** Entender la población de afiliados y la evolución de los datos.\n",
    "\n",
    "* **Análisis de Afiliación en el SGSSS:**\n",
    "    * **Pregunta a responder:** ¿Cómo ha evolucionado la afiliación de la EPS a lo largo del tiempo (por año y mes)?\n",
    "    * **Métodos:** Contar registros únicos de afiliados por mes. Visualizar la tendencia en gráficos de líneas.\n",
    "    * **Resultados esperados:** Gráficos de crecimiento/decrecimiento de la base de afiliados, identificación de picos o caídas en la afiliación.\n",
    "\n",
    "* **Análisis de Calidad del Dato (BDUA):**\n",
    "    * **Pregunta a responder:** ¿Cuál es la calidad de los datos de identificación y demográficos a lo largo de los años?\n",
    "    * **Métodos:** Contar valores nulos o inconsistentes en campos clave como `HST_IDN_NUMERO_IDENTIFICACION_CF`, `AFL_FECHA_NACIMIENTO`, `DPR_ID`, `MNC_ID`.\n",
    "    * **Resultados esperados:** Gráficos que muestren la evolución del porcentaje de datos faltantes, reportes de calidad que ayuden a identificar mejoras en la recolección de información.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sección 2: Análisis Predictivo**\n",
    "#### **Objetivo:** Predecir tendencias y eventos futuros para optimizar la gestión.\n",
    "\n",
    "* **Modelo de Predicción de Desafiliación (Churn):**\n",
    "    * **Pregunta a responder:** ¿Qué afiliados tienen mayor probabilidad de desafiliarse de la EPS en los próximos 6 meses?\n",
    "    * **Métodos:** Usar algoritmos de clasificación (como Random Forest o Gradient Boosting) con variables como la antigüedad de la afiliación, cambios en el estado de cotización y características sociodemográficas.\n",
    "    * **Resultados esperados:** Un modelo que clasifique a los afiliados en \"alto riesgo\", \"riesgo moderado\" y \"bajo riesgo\" de desafiliación.\n",
    "\n",
    "* **Predicción de Pertenencia a Grupos Poblacionales:**\n",
    "    * **Pregunta a responder:** ¿Cómo podemos predecir si un afiliado pertenece a un grupo étnico o poblacional específico basándonos en sus datos históricos?\n",
    "    * **Métodos:** Aplicar modelos de clasificación para predecir la pertenencia a un `CNT_AFL_TPS_PRT_ETN_ID` o `TPS_GRP_PBL_ID`, utilizando variables como el lugar de residencia (`DPR_ID`, `MNC_ID`) y las características de su afiliación.\n",
    "    * **Resultados esperados:** Un modelo que ayude a la EPS a identificar y validar de forma automática la pertenencia de los afiliados a grupos especiales.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sección 3: Análisis Avanzado y de Tendencias**\n",
    "#### **Objetivo:** Descubrir patrones complejos y relaciones no evidentes en los datos.\n",
    "\n",
    "* **Detección de Anomalías:**\n",
    "    * **Pregunta a responder:** ¿Existen registros de afiliados o patrones de afiliación inusuales que puedan indicar posibles fraudes o errores de digitación?\n",
    "    * **Métodos:** Utilizar algoritmos de detección de anomalías (como Isolation Forest o DBSCAN) para identificar registros que no se ajustan al comportamiento de la mayoría.\n",
    "    * **Resultados esperados:** Un sistema que alerte sobre registros con datos de identificación atípicos, fechas de nacimiento inverosímiles, o patrones de afiliación/desafiliación sospechosos.\n",
    "\n",
    "* **Segmentación de la Población de Afiliados:**\n",
    "    * **Pregunta a responder:** ¿Cómo podemos segmentar a los afiliados en grupos homogéneos para ofrecer servicios de salud más personalizados y eficientes?\n",
    "    * **Métodos:** Aplicar algoritmos de clustering (como K-Means o jerárquico) usando variables demográficas (`AFL_FECHA_NACIMIENTO`, `TPS_GNR_ID`), de ubicación y de tipo de afiliación.\n",
    "    * **Resultados esperados:** Segmentos de afiliados bien definidos (ej. \"población joven urbana\", \"tercera edad rural\") que permitan una planeación estratégica de servicios de salud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
