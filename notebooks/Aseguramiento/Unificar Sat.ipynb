{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fceddd4",
   "metadata": {},
   "source": [
    "# 1. MODULOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27607fbf",
   "metadata": {},
   "source": [
    "# 2. RUTAS Y VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4525406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la ruta de la carpeta donde se encuentran los archivos SAT de todos los años\n",
    "R_Sat_base = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\"\n",
    "\n",
    "# Se define la ruta de la carpeta de salida para los consolidados anuales\n",
    "R_Salida_base = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48898628",
   "metadata": {},
   "source": [
    "# 3 UNIFICACION ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507d65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018\n",
      "Error al leer el archivo 'EPS025_2018-3-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-18.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-19.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-20.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-23.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-25.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-26.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-29.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-30.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-31.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-1.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-2.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-23.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-8.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-9.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2018 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2018.TXT'\n",
      "Procesando el año: 2019\n",
      "Error al leer el archivo 'EPS025-2019-3-4.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025-2019-11-5.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2019 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2019.TXT'\n",
      "Procesando el año: 2020\n",
      "Consolidado del año 2020 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2020.TXT'\n",
      "Procesando el año: 2021\n",
      "Error al leer el archivo 'EPS025-2021-6-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2021 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2021.TXT'\n",
      "Procesando el año: 2022\n",
      "Consolidado del año 2022 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2022.TXT'\n",
      "Procesando el año: 2023\n",
      "Consolidado del año 2023 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2023.TXT'\n",
      "Procesando el año: 2024\n",
      "Consolidado del año 2024 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2024.TXT'\n",
      "Procesando el año: 2025\n",
      "Consolidado del año 2025 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\consolidado año\\SAT_EPS025_2025.TXT'\n",
      "Proceso de consolidación anual completado.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# EJECUCIÓN DEL FLUJO DE TRABAJO\n",
    "# ----------------------------------\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "if not os.path.exists(R_Salida_base):\n",
    "    os.makedirs(R_Salida_base)\n",
    "\n",
    "# Obtener una lista de los subdirectorios que representan los años (ej. \"2018\", \"2019\", etc.)\n",
    "# Se filtran solo los directorios que contienen un número para evitar carpetas como 'consolidado año'\n",
    "años = [d for d in os.listdir(R_Sat_base) if os.path.isdir(os.path.join(R_Sat_base, d)) and d.isdigit()]\n",
    "\n",
    "# Iterar sobre cada año encontrado\n",
    "for año in años:\n",
    "    print(f\"Procesando el año: {año}\")\n",
    "    \n",
    "    # Se define la ruta de la carpeta para el año actual en el bucle\n",
    "    R_Sat_año = os.path.join(R_Sat_base, año)\n",
    "    \n",
    "    # Se define el nombre del archivo de salida para el año actual\n",
    "    SAT_SALIDA_año = f\"SAT_EPS025_{año}.TXT\"\n",
    "    \n",
    "    # Se construye la ruta completa del archivo de salida\n",
    "    R_Salida_año = os.path.join(R_Salida_base, SAT_SALIDA_año)\n",
    "\n",
    "    # Buscar todos los archivos .txt en la ruta del año y sus subcarpetas\n",
    "    archivos = glob.glob(os.path.join(R_Sat_año, '**', '*.txt'), recursive=True)\n",
    "\n",
    "    # Si no se encuentran archivos para el año, se salta al siguiente\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos .txt para el año {año}. Saltando.\")\n",
    "        continue\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Extraer la fecha del nombre del archivo\n",
    "        nombre = os.path.basename(archivo)\n",
    "        \n",
    "        # El formato de los nombres de archivo varía. Se ha modificado la lógica para\n",
    "        # manejar un formato más flexible, como 'EPS025_2018-3-16.txt' o 'SAT-2025-05-31.txt'\n",
    "        \n",
    "        partes = nombre.split('-')\n",
    "\n",
    "        # Se verifica si el nombre de archivo se ajusta a alguno de los formatos esperados\n",
    "        if len(partes) == 4:\n",
    "            # Formato 'SAT-2025-05-31.txt'\n",
    "            anio_archivo = partes[1]\n",
    "            mes_archivo = partes[2]\n",
    "            dia_archivo = partes[3].split('.')[0]\n",
    "        elif len(partes) >= 3:\n",
    "            # Formato 'EPS025_2018-3-16.txt'\n",
    "            # Extraemos el año de la primera parte (ej. '2018' de 'EPS025_2018')\n",
    "            parte_anio = partes[0]\n",
    "            try:\n",
    "                anio_archivo = parte_anio.split('_')[1]\n",
    "                mes_archivo = partes[1]\n",
    "                dia_archivo = partes[2].split('.')[0]\n",
    "            except IndexError:\n",
    "                # Si no se puede extraer el año, se ignora el archivo\n",
    "                print(f\"Alerta: El archivo '{nombre}' no tiene el formato esperado (no se pudo extraer el año). Este archivo será ignorado.\")\n",
    "                continue\n",
    "        else:\n",
    "            # Si el formato no coincide con ninguno, se ignora\n",
    "            print(f\"Alerta: El archivo '{nombre}' no tiene el formato esperado. Este archivo será ignorado.\")\n",
    "            continue  \n",
    "\n",
    "        fecha_archivo = f\"{dia_archivo.zfill(2)}-{mes_archivo.zfill(2)}-{anio_archivo}\"\n",
    "\n",
    "        try:\n",
    "            # Se usa header=None para no ignorar la primera fila\n",
    "            # Leer el archivo como texto, sin importar el tipo de dato\n",
    "            df = pd.read_csv(archivo, sep='|', dtype=str, encoding='ansi', on_bad_lines='skip', header=None)\n",
    "            \n",
    "            # Agregar la columna de fecha al inicio\n",
    "            df.insert(0, 'fecha_archivo', fecha_archivo)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo '{nombre}': {e}. Este archivo será ignorado.\")\n",
    "            continue\n",
    "\n",
    "    # Si no se pudo leer ningún dataframe, pasar al siguiente año\n",
    "    if not dataframes:\n",
    "        print(f\"No se pudieron leer archivos válidos para el año {año}. Saltando.\")\n",
    "        continue\n",
    "\n",
    "    # Unir todos los dataframes del año actual\n",
    "    df_total = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Eliminar los registros que son encabezados normativos\n",
    "    # Se crea una máscara booleana para identificar los registros que coinciden con el patrón\n",
    "    # Se usa .str.strip() para asegurar que no haya espacios en blanco en los campos\n",
    "    try:\n",
    "        mascara_encabezados_normativos = (df_total[1].str.strip() == '1') & \\\n",
    "                                         (df_total[2].str.strip() == '1') & \\\n",
    "                                         (df_total[3].str.strip() == 'NI') & \\\n",
    "                                         (df_total[4].str.strip() == '891856000')\n",
    "        # Se eliminan las filas que coinciden con la máscara\n",
    "        df_total = df_total[~mascara_encabezados_normativos]\n",
    "    except KeyError:\n",
    "        print(f\"No se pudo aplicar el filtro en el año {año}. Las columnas pueden tener un formato inesperado. Se continuará con el consolidado tal como está.\")\n",
    "\n",
    "\n",
    "    # Renombrar las columnas a COL1, COL2, ...\n",
    "    df_total.columns = [f'COL{i+1}' for i in range(df_total.shape[1])]\n",
    "\n",
    "    # Guardar el resultado en el archivo de salida para el año actual\n",
    "    df_total.to_csv(R_Salida_año, sep='|', index=False, header=True, encoding='ansi')\n",
    "    \n",
    "    print(f\"Consolidado del año {año} guardado exitosamente en '{R_Salida_año}'\")\n",
    "\n",
    "print(\"Proceso de consolidación anual completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
