{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fceddd4",
   "metadata": {},
   "source": [
    "# 1. MODULOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48898628",
   "metadata": {},
   "source": [
    "# 2 UNIFICACION ARCHIVOS AÑO A AÑO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507d65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando el año: 2018\n",
      "Error al leer el archivo 'EPS025_2018-3-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-18.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-19.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-20.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-23.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-25.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-26.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-29.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-30.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-3-31.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-1.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-2.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-23.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-8.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025_2018-4-9.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2018 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2018.TXT'\n",
      "Procesando el año: 2019\n",
      "Error al leer el archivo 'EPS025-2019-3-4.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Error al leer el archivo 'EPS025-2019-11-5.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2019 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2019.TXT'\n",
      "Procesando el año: 2020\n",
      "Consolidado del año 2020 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2020.TXT'\n",
      "Procesando el año: 2021\n",
      "Error al leer el archivo 'EPS025-2021-6-16.txt': No columns to parse from file. Este archivo será ignorado.\n",
      "Consolidado del año 2021 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2021.TXT'\n",
      "Procesando el año: 2022\n",
      "Consolidado del año 2022 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2022.TXT'\n",
      "Procesando el año: 2023\n",
      "Consolidado del año 2023 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2023.TXT'\n",
      "Procesando el año: 2024\n",
      "Consolidado del año 2024 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2024.TXT'\n",
      "Procesando el año: 2025\n",
      "Consolidado del año 2025 guardado exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\\SAT_EPS025_2025.TXT'\n",
      "Proceso de consolidación anual completado.\n"
     ]
    }
   ],
   "source": [
    "# Se define la ruta de la carpeta donde se encuentran los archivos SAT de todos los años\n",
    "R_Sat_base = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\"\n",
    "\n",
    "# Se define la ruta de la carpeta de salida para los consolidados anuales\n",
    "R_Salida_base = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\"\n",
    "\n",
    "# ----------------------------------\n",
    "# EJECUCIÓN DEL FLUJO DE TRABAJO\n",
    "# ----------------------------------\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "if not os.path.exists(R_Salida_base):\n",
    "    os.makedirs(R_Salida_base)\n",
    "\n",
    "# Obtener una lista de los subdirectorios que representan los años (ej. \"2018\", \"2019\", etc.)\n",
    "# Se filtran solo los directorios que contienen un número para evitar carpetas como 'All'\n",
    "años = [d for d in os.listdir(R_Sat_base) if os.path.isdir(os.path.join(R_Sat_base, d)) and d.isdigit()]\n",
    "\n",
    "# Iterar sobre cada año encontrado\n",
    "for año in años:\n",
    "    print(f\"Procesando el año: {año}\")\n",
    "    \n",
    "    # Se define la ruta de la carpeta para el año actual en el bucle\n",
    "    R_Sat_año = os.path.join(R_Sat_base, año)\n",
    "    \n",
    "    # Se define el nombre del archivo de salida para el año actual\n",
    "    SAT_SALIDA_año = f\"SAT_EPS025_{año}.TXT\"\n",
    "    \n",
    "    # Se construye la ruta completa del archivo de salida\n",
    "    R_Salida_año = os.path.join(R_Salida_base, SAT_SALIDA_año)\n",
    "\n",
    "    # Buscar todos los archivos .txt en la ruta del año y sus subcarpetas\n",
    "    archivos = glob.glob(os.path.join(R_Sat_año, '**', '*.txt'), recursive=True)\n",
    "\n",
    "    # Si no se encuentran archivos para el año, se salta al siguiente\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos .txt para el año {año}. Saltando.\")\n",
    "        continue\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Extraer la fecha del nombre del archivo\n",
    "        nombre = os.path.basename(archivo)\n",
    "        \n",
    "        # El formato de los nombres de archivo varía. Se ha modificado la lógica para\n",
    "        # manejar un formato más flexible, como 'EPS025_2018-3-16.txt' o 'SAT-2025-05-31.txt'\n",
    "        \n",
    "        partes = nombre.split('-')\n",
    "\n",
    "        # Se verifica si el nombre de archivo se ajusta a alguno de los formatos esperados\n",
    "        if len(partes) == 4:\n",
    "            # Formato 'SAT-2025-05-31.txt'\n",
    "            anio_archivo = partes[1]\n",
    "            mes_archivo = partes[2]\n",
    "            dia_archivo = partes[3].split('.')[0]\n",
    "        elif len(partes) >= 3:\n",
    "            # Formato 'EPS025_2018-3-16.txt'\n",
    "            # Extraemos el año de la primera parte (ej. '2018' de 'EPS025_2018')\n",
    "            parte_anio = partes[0]\n",
    "            try:\n",
    "                anio_archivo = parte_anio.split('_')[1]\n",
    "                mes_archivo = partes[1]\n",
    "                dia_archivo = partes[2].split('.')[0]\n",
    "            except IndexError:\n",
    "                # Si no se puede extraer el año, se ignora el archivo\n",
    "                print(f\"Alerta: El archivo '{nombre}' no tiene el formato esperado (no se pudo extraer el año). Este archivo será ignorado.\")\n",
    "                continue\n",
    "        else:\n",
    "            # Si el formato no coincide con ninguno, se ignora\n",
    "            print(f\"Alerta: El archivo '{nombre}' no tiene el formato esperado. Este archivo será ignorado.\")\n",
    "            continue  \n",
    "\n",
    "        fecha_archivo = f\"{dia_archivo.zfill(2)}-{mes_archivo.zfill(2)}-{anio_archivo}\"\n",
    "\n",
    "        try:\n",
    "            # Se usa header=None para no ignorar la primera fila\n",
    "            # Leer el archivo como texto, sin importar el tipo de dato\n",
    "            df = pd.read_csv(archivo, sep='|', dtype=str, encoding='ansi', on_bad_lines='skip', header=None)\n",
    "            \n",
    "            # Agregar la columna de fecha al inicio\n",
    "            df.insert(0, 'fecha_archivo', fecha_archivo)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo '{nombre}': {e}. Este archivo será ignorado.\")\n",
    "            continue\n",
    "\n",
    "    # Si no se pudo leer ningún dataframe, pasar al siguiente año\n",
    "    if not dataframes:\n",
    "        print(f\"No se pudieron leer archivos válidos para el año {año}. Saltando.\")\n",
    "        continue\n",
    "\n",
    "    # Unir todos los dataframes del año actual\n",
    "    df_total = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Eliminar los registros que son encabezados normativos\n",
    "    # Se crea una máscara booleana para identificar los registros que coinciden con el patrón\n",
    "    # Se usa .str.strip() para asegurar que no haya espacios en blanco en los campos\n",
    "    try:\n",
    "        mascara_encabezados_normativos = (df_total[0].str.strip() == '1') & \\\n",
    "                                         (df_total[1].str.strip() == '1') & \\\n",
    "                                         (df_total[2].str.strip() == 'NI') & \\\n",
    "                                         (df_total[3].str.strip() == '891856000')\n",
    "        # Se eliminan las filas que coinciden con la máscara\n",
    "        df_total = df_total[~mascara_encabezados_normativos]\n",
    "\n",
    "        mascara_encabezados_normativos = (df_total[0].str.strip() == '1') & \\\n",
    "                                         (df_total[1].str.strip() == 'NIT') & \\\n",
    "                                         (df_total[2].str.strip() == '891856000')\n",
    "        # Se eliminan las filas que coinciden con la máscara\n",
    "        df_total = df_total[~mascara_encabezados_normativos]\n",
    "\n",
    "        mascara_encabezados_normativos = (df_total[0].str.strip() == '1') & \\\n",
    "                                         (df_total[1].str.strip() == 'NI') & \\\n",
    "                                         (df_total[2].str.strip() == '891856000')\n",
    "        # Se eliminan las filas que coinciden con la máscara\n",
    "        df_total = df_total[~mascara_encabezados_normativos]\n",
    "\n",
    "        mascara_encabezados_normativos = (df_total[1].str.strip() == '1') & \\\n",
    "                                         (df_total[2].str.strip() == 'NI') & \\\n",
    "                                         (df_total[3].str.strip() == '891856000')\n",
    "        # Se eliminan las filas que coinciden con la máscara\n",
    "        df_total = df_total[~mascara_encabezados_normativos]\n",
    "    except KeyError:\n",
    "        print(f\"No se pudo aplicar el filtro en el año {año}. Las columnas pueden tener un formato inesperado. Se continuará con el consolidado tal como está.\")\n",
    "\n",
    "\n",
    "    # Renombrar las columnas a COL1, COL2, ...\n",
    "    df_total.columns = [f'COL{i+1}' for i in range(df_total.shape[1])]\n",
    "\n",
    "    # Guardar el resultado en el archivo de salida para el año actual\n",
    "    df_total.to_csv(R_Salida_año, sep='|', index=False, header=True, encoding='ansi')\n",
    "    \n",
    "    print(f\"Consolidado del año {año} guardado exitosamente en '{R_Salida_año}'\")\n",
    "\n",
    "print(\"Proceso de consolidación anual completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7d000",
   "metadata": {},
   "source": [
    "# 3. CONSOLIDADO SAT TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo: SAT_EPS025_2018.TXT\n",
      "Leyendo archivo: SAT_EPS025_2019.TXT\n",
      "Leyendo archivo: SAT_EPS025_2020.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2020.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "Leyendo archivo: SAT_EPS025_2021.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2021.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "Leyendo archivo: SAT_EPS025_2022.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2022.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "Leyendo archivo: SAT_EPS025_2023.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2023.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "Leyendo archivo: SAT_EPS025_2024.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2024.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "Leyendo archivo: SAT_EPS025_2025.TXT\n",
      "Alerta: El archivo 'SAT_EPS025_2025.TXT' tiene un número diferente de columnas (86). Esto podría causar inconsistencias.\n",
      "\n",
      "Todos los archivos consolidados por año han sido unificados exitosamente en 'C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\Consolidado_Sat_EPS025.txt'\n",
      "Se unificaron 8 archivos en total.\n",
      "\n",
      "--- Validación de Registros ---\n",
      "Total de registros consolidados (suma de todos los archivos): 49332\n",
      "Total de registros en el archivo unificado: 49332\n",
      "La validación de la cantidad de registros es correcta. El proceso fue coherente.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------\n",
    "# PARÁMETROS DEL FLUJO DE TRABAJO\n",
    "# ----------------------------------\n",
    "\n",
    "R_archivos_consolidados = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\All\"\n",
    "R_Salida_final = r\"C:\\Users\\osmarrincon\\OneDrive - uniminuto.edu\\Capresoca\\AlmostClear\\SAT\\SUBSIDIADO\\Consolidado_Sat_EPS025.txt\"\n",
    "\n",
    "# ----------------------------------\n",
    "# EJECUCIÓN DEL FLUJO DE TRABAJO DE UNIFICACIÓN\n",
    "# ----------------------------------\n",
    "\n",
    "archivos_anuales = glob.glob(os.path.join(R_archivos_consolidados, '*.TXT'))\n",
    "\n",
    "if not archivos_anuales:\n",
    "    print(f\"No se encontraron archivos .TXT para unificar en '{R_archivos_consolidados}'. El proceso ha finalizado.\")\n",
    "    exit()\n",
    "\n",
    "dataframes_final = []\n",
    "encabezado_df = None\n",
    "total_registros_brutos = 0\n",
    "\n",
    "for i, archivo in enumerate(archivos_anuales):\n",
    "    try:\n",
    "        print(f\"Leyendo archivo: {os.path.basename(archivo)}\")\n",
    "        \n",
    "        # Leer el encabezado y los datos por separado para mayor control\n",
    "        df_temp = pd.read_csv(archivo, sep='|', dtype=str, encoding='ansi', on_bad_lines='skip', header=0)\n",
    "\n",
    "        # Si es el primer archivo, se almacena el encabezado\n",
    "        if encabezado_df is None:\n",
    "            encabezado_df = df_temp.iloc[0:0] # Tomar la estructura del encabezado sin datos\n",
    "        \n",
    "        # Si las columnas de este archivo son diferentes al encabezado del primer archivo\n",
    "        # se usará la unión de las columnas de todos los archivos\n",
    "        if list(df_temp.columns) != list(encabezado_df.columns):\n",
    "            print(f\"Alerta: El archivo '{os.path.basename(archivo)}' tiene un número diferente de columnas ({len(df_temp.columns)}). Esto podría causar inconsistencias.\")\n",
    "\n",
    "        dataframes_final.append(df_temp)\n",
    "        total_registros_brutos += len(df_temp)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo '{os.path.basename(archivo)}': {e}. Este archivo será ignorado.\")\n",
    "        continue\n",
    "\n",
    "if not dataframes_final:\n",
    "    print(f\"No se pudieron leer archivos válidos para unificar. El proceso ha finalizado.\")\n",
    "    exit()\n",
    "\n",
    "# Unir todos los dataframes, llenando con NaN donde falten columnas\n",
    "# `pd.concat` se encargará de alinear los DataFrames por los nombres de las columnas\n",
    "df_total_unificado = pd.concat(dataframes_final, ignore_index=True)\n",
    "\n",
    "# Contar los registros finales para la validación\n",
    "registros_finales = len(df_total_unificado)\n",
    "\n",
    "# Guardar el resultado en el archivo de salida final\n",
    "# Los nombres de las columnas se toman del DataFrame unificado, que tendrá todas las columnas únicas\n",
    "df_total_unificado.to_csv(R_Salida_final, sep='|', index=False, header=True, encoding='ansi')\n",
    "\n",
    "print(f\"\\nTodos los archivos consolidados por año han sido unificados exitosamente en '{R_Salida_final}'\")\n",
    "print(f\"Se unificaron {len(archivos_anuales)} archivos en total.\")\n",
    "\n",
    "# Validación de la cantidad de registros\n",
    "print(\"\\n--- Validación de Registros ---\")\n",
    "print(f\"Total de registros consolidados (suma de todos los archivos): {total_registros_brutos}\")\n",
    "print(f\"Total de registros en el archivo unificado: {registros_finales}\")\n",
    "\n",
    "if registros_finales == total_registros_brutos:\n",
    "    print(\"La validación de la cantidad de registros es correcta. El proceso fue coherente.\")\n",
    "else:\n",
    "    print(\"Alerta: Se ha encontrado una discrepancia en el número de registros.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
